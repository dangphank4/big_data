# BigData Stock Analysis System

**Real-time Stock Market Data Pipeline - Production Ready**

## ğŸš€ Quick Start

See detailed step-by-step guides:

- **[Docker Deployment Guide](docs/DOCKER_DEPLOYMENT.md)** - Local testing with Docker Compose
- **[GKE Deployment Guide](docs/GKE_DEPLOYMENT.md)** - Production deployment on Google Kubernetes Engine

## ğŸ“ Project Structure

```
big_data/
â”œâ”€â”€ docs/                          # Complete deployment guides
â”‚   â”œâ”€â”€ DOCKER_DEPLOYMENT.md       # Step-by-step Docker guide (30 steps)
â”‚   â””â”€â”€ GKE_DEPLOYMENT.md          # Step-by-step GKE guide (50 steps)
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ producers/
â”‚   â”‚   â””â”€â”€ kafka_producer.py      # Real-time crawler (yfinance, 60s interval)
â”‚   â”œâ”€â”€ consumers/
â”‚   â”‚   â”œâ”€â”€ kafka_consumer_spark_streaming.py    # Kafka -> Spark bridge (topic fan-out)
â”‚   â”‚   â””â”€â”€ kafka_consumer_hdfs_archiver.py      # HDFS archiver (daily CronJob)
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ spark_streaming_simple.py           # Metrics stream -> ES: stock-realtime-1m
â”‚   â”‚   â””â”€â”€ spark_streaming_alert.py            # Alerts stream  -> ES: stock-alerts-1m
â”‚   â”œâ”€â”€ batch_jobs/
â”‚   â”‚   â””â”€â”€ run_all.py                          # Spark batch entrypoint (features)
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ crawl_data.py          # Yahoo Finance API wrapper
â”‚       â”œâ”€â”€ crawl_feed.py          # Historical backfill utility
â”‚       â””â”€â”€ standardization_local.py  # Schema definitions
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ docker-compose.yml         # Local deployment orchestration
â”‚   â”œâ”€â”€ Dockerfile                 # Application image
â”‚   â””â”€â”€ Dockerfile.production      # Production-optimized image
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ k8s/                       # Kubernetes manifests (15 files)
â”‚   â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”‚   â”œâ”€â”€ kafka-producer-crawl-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ spark-streaming-consumer-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ hdfs-archiver-cronjob.yaml
â”‚   â”‚   â”œâ”€â”€ kafka-statefulset.yaml
â”‚   â”‚   â”œâ”€â”€ hdfs-statefulset.yaml
â”‚   â”‚   â”œâ”€â”€ elasticsearch-statefulset.yaml
â”‚   â”‚   â””â”€â”€ ... (infrastructure manifests)
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ build-and-push.sh      # Build & push to GCR
â”‚       â”œâ”€â”€ create-cluster.sh      # Create GKE cluster
â”‚       â”œâ”€â”€ deploy.sh              # Deploy to K8s
â”‚       â””â”€â”€ cleanup.sh             # Cleanup resources
â””â”€â”€ requirements.txt               # Python dependencies
```

## ğŸ“Š Architecture

### Real-time Data Pipeline (Lambda Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        HOT PATH (Real-time)                          â”‚
â”‚                                                                       â”‚
â”‚  yfinance API                                                         â”‚
â”‚      â”‚                                                                â”‚
â”‚      â–¼                                                                â”‚
â”‚  kafka_producer.py â”€â”€â–º Kafka Topic: stocks-realtime                  â”‚
â”‚  (60s interval)              â”‚                                        â”‚
â”‚                              â”œâ”€â”€â–º kafka_consumer_spark_streaming.py  â”‚
â”‚                              â”‚    â€¢ bridge -> Kafka: stocks-realtime-spark â”‚
â”‚                              â”‚                                        â”‚
â”‚                              â”œâ”€â”€â–º spark_streaming_simple.py          â”‚
â”‚                              â”‚    â€¢ metrics aggregation (1m)         â”‚
â”‚                              â”‚    â–¼                                   â”‚
â”‚                              â”‚    Elasticsearch: stock-realtime-1m   â”‚
â”‚                              â”‚                                        â”‚
â”‚                              â”œâ”€â”€â–º spark_streaming_alert.py           â”‚
â”‚                              â”‚    â€¢ alerts detection (1m)            â”‚
â”‚                              â”‚    â–¼                                   â”‚
â”‚                              â”‚    Elasticsearch: stock-alerts-1m     â”‚
â”‚                              â”‚                                        â”‚
â”‚                              â””â”€â”€â–º [Kafka Buffer: 7 days retention]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        COLD PATH (Batch)                              â”‚
â”‚                                                                        â”‚
â”‚  kafka_consumer_hdfs_archiver.py                                      â”‚
â”‚  (CronJob: Daily 00:00 UTC)                                           â”‚
â”‚      â€¢ Reads last 24h from Kafka                                      â”‚
â”‚      â€¢ Deduplication                                                  â”‚
â”‚      â–¼                                                                 â”‚
â”‚  HDFS: /stock-data/YYYY-MM-DD/TICKER.json                            â”‚
â”‚                                                                        â”‚
â”‚  crawl_feed.py (Backfill Utility)                                     â”‚
â”‚      â€¢ Historical data (bypasses Kafka)                               â”‚
â”‚      â€¢ Direct to HDFS                                                 â”‚
â”‚      â€¢ Command: --days 30 or --start/--end                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SERVING LAYER                                  â”‚
â”‚                                                                        â”‚
â”‚  Elasticsearch â—„â”€â”€â”€ Query real-time data (last hours/days)           â”‚
â”‚  HDFS          â—„â”€â”€â”€ Query historical data (months/years)             â”‚
â”‚  Kibana        â—„â”€â”€â”€ Visualization & dashboards                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

- âœ… **Real-time Crawling**: yfinance API every 60 seconds (minute bars)
- âœ… **Hot Storage**: Elasticsearch for real-time queries (hours/days)
- âœ… **Cold Storage**: HDFS for long-term archival (months/years)
- âœ… **Kafka Buffer**: 7-day retention for replay and recovery
- âœ… **Deduplication**: Both consumers handle duplicates
- âœ… **Scalability**: Kubernetes-ready with HPA
- âœ… **Production Ready**: Comprehensive deployment guides

## ğŸ“¦ Services

| Service       | Port | Purpose                 |
| ------------- | ---- | ----------------------- |
| Zookeeper     | 2181 | Kafka coordination      |
| Kafka         | 9092 | Message broker          |
| HDFS NameNode | 9870 | HDFS management UI      |
| HDFS DataNode | 9866 | HDFS data storage       |
| Elasticsearch | 9200 | Real-time data indexing |
| Kibana        | 5601 | Data visualization      |

## ğŸ”§ Development

### Environment Variables

```bash
# Kafka Configuration
KAFKA_BROKER=kafka:9092
KAFKA_TOPIC=stocks-realtime
CRAWL_INTERVAL=60  # seconds

# Stock Tickers (comma-separated)
TICKERS=AAPL,NVDA,TSLA,MSFT,GOOGL

# HDFS Configuration
HDFS_HOST=hdfs-namenode
HDFS_PORT=9000
HDFS_BASE_PATH=/stock-data

# Elasticsearch Configuration
ELASTICSEARCH_HOST=elasticsearch
ELASTICSEARCH_PORT=9200
```

### Running Individual Components

```bash
# Run producer (real-time crawling)
docker run --rm --network bigdata_default \
  -e KAFKA_BROKER=kafka:9092 \
  -e TICKERS=AAPL,NVDA \
  bigdata-app:latest python -m src.producers.kafka_producer

# Run Kafka -> Spark bridge
docker run --rm --network bigdata_default \
  -e KAFKA_BROKER=kafka:9092 \
  -e INPUT_TOPIC=stocks-realtime \
  -e SPARK_TOPIC=stocks-realtime-spark \
  bigdata-app:latest python -m src.consumers.kafka_consumer_spark_streaming

# Run Spark Streaming metrics job
docker run --rm --network bigdata_default \
  -e KAFKA_BROKER=kafka:9092 \
  -e ELASTICSEARCH_HOST=elasticsearch \
  bigdata-app:latest \
  spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3 \

  # Metrics stream (stock-realtime-1m)
  /app/src/streaming/spark_streaming_simple.py

# Run Spark Streaming alerts job
docker run --rm --network bigdata_default \
  -e KAFKA_BROKER=kafka:9092 \
  -e ELASTICSEARCH_HOST=elasticsearch \
  bigdata-app:latest \
  spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3 \

  # Alerts stream (stock-alerts-1m)
  /app/src/streaming/spark_streaming_alert.py

# Run HDFS archiver manually
docker run --rm --network bigdata_default \
  -e KAFKA_BROKER=kafka:9092 \
  -e HDFS_HOST=hdfs-namenode \
  bigdata-app:latest python -m src.consumers.kafka_consumer_hdfs_archiver

# Backfill historical data
docker run --rm --network bigdata_default \
  -e HDFS_HOST=hdfs-namenode \
  bigdata-app:latest python -m src.utils.crawl_feed --days 30
```

## ğŸ“Š Data Schema

### Kafka Message Format (JSON)

```json
{
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "time": "2024-01-13T10:05:00",
  "Open": 185.23,
  "High": 185.45,
  "Low": 185.1,
  "Close": 185.32,
  "Adj Close": 185.32,
  "Volume": 12345678
}
```

### Elasticsearch Indices

- **stock-realtime-1m**: 1-minute metrics aggregation
- **stock-alerts-1m**: 1-minute alerts stream

### HDFS Structure

```
/stock-data/
â”œâ”€â”€ 2024-01-13/
â”‚   â”œâ”€â”€ AAPL.json     # All AAPL records for the day
â”‚   â”œâ”€â”€ NVDA.json     # All NVDA records for the day
â”‚   â””â”€â”€ TSLA.json     # All TSLA records for the day
â”œâ”€â”€ 2024-01-14/
â”‚   â””â”€â”€ ...
â””â”€â”€ 2024-01-15/
    â””â”€â”€ ...
```

## ğŸš€ Deployment

### Docker (Local Testing)

Follow [docs/DOCKER_DEPLOYMENT.md](docs/DOCKER_DEPLOYMENT.md) for complete step-by-step guide.

Quick start:

```bash
# Build image
docker build -f config/Dockerfile -t bigdata-app:latest .

# Start infrastructure
docker-compose -f config/docker-compose.yml up -d zookeeper kafka hadoop-namenode hadoop-datanode elasticsearch kibana

# Start application
docker-compose -f config/docker-compose.yml up -d stock-producer spark-kafka-bridge spark-streaming-metrics spark-streaming-alerts
```

### Kubernetes (Production)

Follow [docs/GKE_DEPLOYMENT.md](docs/GKE_DEPLOYMENT.md) for complete GKE deployment.

Quick start:

```bash
# Create GKE cluster
gcloud container clusters create bigdata-cluster \
  --zone=us-central1-a \
  --num-nodes=4 \
  --machine-type=n1-standard-4

# Build and push to GCR
docker build -f config/Dockerfile -t gcr.io/PROJECT_ID/bigdata-app:latest .
docker push gcr.io/PROJECT_ID/bigdata-app:latest

# Deploy
kubectl apply -f deployment/k8s/namespace.yaml
kubectl apply -f deployment/k8s/configmap.yaml
kubectl apply -f deployment/k8s/
```

## ğŸ§ª Testing

### Verify Data Flow

```bash
# 1. Check producer logs
kubectl logs -l app=kafka-producer -n bigdata --tail=50

# 2. Check Kafka messages
kubectl exec -it kafka-0 -n bigdata -- \
  kafka-console-consumer.sh \
    --bootstrap-server localhost:9092 \
    --topic stocks-realtime \
    --max-messages 5

# 3. Check Elasticsearch data
curl http://localhost:9200/stock-realtime-1m/_count

# 4. Check HDFS data
kubectl exec -it hdfs-namenode-0 -n bigdata -- \
  hdfs dfs -ls /stock-data/$(date +%Y-%m-%d)
```

## ğŸ“ˆ Monitoring

### Metrics

- **Producer**: Records sent per minute, crawl latency
- **Spark Streaming**: Processing rate, checkpoint age
- **Elasticsearch**: Index size, query latency
- **HDFS**: Storage usage, replication status

### Logs

```bash
# Docker
docker-compose -f config/docker-compose.yml logs -f [service-name]

# Kubernetes
kubectl logs -f -l app=[app-name] -n bigdata
kubectl logs -f [pod-name] -n bigdata --tail=100
```

## ğŸ› ï¸ Troubleshooting

Common issues and solutions are documented in:

- [docs/DOCKER_DEPLOYMENT.md#troubleshooting](docs/DOCKER_DEPLOYMENT.md#troubleshooting)
- [docs/GKE_DEPLOYMENT.md#troubleshooting](docs/GKE_DEPLOYMENT.md#troubleshooting)

## ğŸ“š Resources

- **Apache Kafka**: https://kafka.apache.org/
- **Apache Spark**: https://spark.apache.org/
- **Apache HDFS**: https://hadoop.apache.org/
- **Elasticsearch**: https://www.elastic.co/
- **yfinance API**: https://pypi.org/project/yfinance/
- **GKE Documentation**: https://cloud.google.com/kubernetes-engine/docs

## ğŸ“ License

This project is for educational and research purposes.

## ğŸ‘¥ Contributors

Big Data Stock Analysis Team

---

**Last Updated**: January 13, 2024  
**Version**: 2.0 (Real-time Crawling Architecture)

| Kafka | 9092 | Message broker |
| Zookeeper | 2181 | Kafka coordination |
| Elasticsearch | 9200 | Data storage & search |
| Kibana | 5601 | Visualization UI |
| HDFS NameNode | 9870 | Hadoop UI |
| HDFS DataNode | 9864 | Data storage |

## ğŸ” Monitoring

```bash
# Check Elasticsearch indices
curl "http://localhost:9200/_cat/indices?v"

# Query real-time data
curl "http://localhost:9200/stock_realtime/_search?size=5&pretty"

# Query anomalies
curl "http://localhost:9200/stock_anomalies/_search?size=5&pretty"

# Check Spark logs
docker logs spark-streaming-simple
docker logs spark-anomaly-detection
```

## ğŸ“– Documentation

- **Architecture**: [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)
- **Docker Guide**: [docs/HUONG_DAN_SU_DUNG_DOCKER.md](docs/HUONG_DAN_SU_DUNG_DOCKER.md)
- **GKE Deployment**: [docs/GKE_DEPLOYMENT_GUIDE.md](docs/GKE_DEPLOYMENT_GUIDE.md)

## ğŸ› ï¸ Technical Stack

- **Streaming**: Apache Kafka 7.9.1, Spark 3.4.3
- **Storage**: Hadoop HDFS 3.2.1, Elasticsearch 7.17.16
- **Visualization**: Kibana 7.17.16
- **Orchestration**: Docker Compose, Kubernetes
- **Language**: Python 3.12, PySpark

## ğŸ“ License

Internal project - All rights reserved

---

**Version**: 2.0.0 | **Last Updated**: January 2026
