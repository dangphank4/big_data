"""
SPARK STREAMING JOB: REAL-TIME ANOMALY DETECTION & RISK ALERTS
===============================================================
M·ª•c ƒë√≠ch: Ph√°t hi·ªán bi·∫øn ƒë·ªông b·∫•t th∆∞·ªùng v√† c·∫£nh b√°o r·ªßi ro real-time

C√°c lo·∫°i Anomaly Detection:
1. Price Spike/Drop Detection (Bi·∫øn ƒë·ªông gi√° ƒë·ªôt ng·ªôt)
2. Volume Spike Detection (Kh·ªëi l∆∞·ª£ng giao d·ªãch b·∫•t th∆∞·ªùng)
3. Volatility Breakout (V∆∞·ª£t ng∆∞·ª°ng bi·∫øn ƒë·ªông)
4. Circuit Breaker Alert (C·∫£nh b√°o tƒÉng/gi·∫£m s√†n)
5. Gap Detection (Gi√° m·ªü c·ª≠a kh√°c bi·ªát ƒë√°ng k·ªÉ v·ªõi gi√° ƒë√≥ng c·ª≠a h√¥m tr∆∞·ªõc)

G√≥c nh√¨n Risk Management:
- B·∫£o v·ªá v·ªën kh·ªèi bi·∫øn ƒë·ªông b·∫•t th∆∞·ªùng
- Ph√°t hi·ªán c∆° h·ªôi trading ng·∫Øn h·∫°n t·ª´ spike
- C·∫£nh b√°o r·ªßi ro thanh kho·∫£n
"""

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, window, avg, stddev, sum, count,
    lag, when, abs, max, min, lit, expr, unix_timestamp,
    to_timestamp, current_timestamp, sqrt, pow
)
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, 
    TimestampType, LongType
)
from pyspark.sql.window import Window

# ============================================================================
# SCHEMA ƒê·ªäNH NGHƒ®A
# ============================================================================

stock_schema = StructType([
    StructField("ticker", StringType(), True),
    StructField("company", StringType(), True),
    StructField("time", StringType(), True),
    StructField("Open", DoubleType(), True),
    StructField("High", DoubleType(), True),
    StructField("Low", DoubleType(), True),
    StructField("Close", DoubleType(), True),
    StructField("Volume", DoubleType(), True)
])

# ============================================================================
# H√ÄM PH√ÅT HI·ªÜN ANOMALY
# ============================================================================

def detect_price_anomaly(df, std_multiplier=3.0, lookback_period=50):
    """
    Ph√°t hi·ªán bi·∫øn ƒë·ªông gi√° b·∫•t th∆∞·ªùng b·∫±ng Z-Score Method
    
    √ù nghƒ©a Trading:
    - Z-Score > 3: Gi√° tƒÉng qu√° nhanh ‚Üí Overbought, c√≥ th·ªÉ pullback
    - Z-Score < -3: Gi√° gi·∫£m qu√° nhanh ‚Üí Oversold, c√≥ th·ªÉ rebound
    - |Z-Score| > 2: Bi·∫øn ƒë·ªông ƒë√°ng ch√∫ √Ω
    
    C√¥ng th·ª©c:
    Z-Score = (Current Price - Mean Price) / Std Dev
    """
    windowSpec = Window.partitionBy("ticker").orderBy("timestamp")
    window_period = windowSpec.rowsBetween(-(lookback_period-1), 0)
    
    df_with_stats = df.withColumn(
        "price_mean",
        avg("Close").over(window_period)
    ).withColumn(
        "price_std",
        stddev("Close").over(window_period)
    ).withColumn(
        "z_score",
        (col("Close") - col("price_mean")) / col("price_std")
    ).withColumn(
        "price_anomaly_type",
        when(col("z_score") > std_multiplier, "EXTREME_SPIKE")
        .when(col("z_score") > 2, "MODERATE_SPIKE")
        .when(col("z_score") < -std_multiplier, "EXTREME_DROP")
        .when(col("z_score") < -2, "MODERATE_DROP")
        .otherwise("NORMAL")
    ).withColumn(
        "is_price_anomaly",
        when(abs(col("z_score")) > 2, True).otherwise(False)
    )
    
    return df_with_stats

def detect_volume_anomaly(df, std_multiplier=3.0, lookback_period=50):
    """
    Ph√°t hi·ªán kh·ªëi l∆∞·ª£ng giao d·ªãch b·∫•t th∆∞·ªùng
    
    √ù nghƒ©a Trading:
    - Volume Spike + Price Up: Xu h∆∞·ªõng tƒÉng m·∫°nh, c√≥ confirmation
    - Volume Spike + Price Down: B√°n th√°o, c·∫£nh b√°o nguy hi·ªÉm
    - High Volume + Price flat: T√≠ch l≈©y ho·∫∑c ph√¢n ph·ªëi ‚Üí Breakout s·∫Øp x·∫£y ra
    - Low Volume: Thi·∫øu interest, xu h∆∞·ªõng y·∫øu
    """
    windowSpec = Window.partitionBy("ticker").orderBy("timestamp")
    window_period = windowSpec.rowsBetween(-(lookback_period-1), 0)
    
    df_with_volume_stats = df.withColumn(
        "volume_mean",
        avg("Volume").over(window_period)
    ).withColumn(
        "volume_std",
        stddev("Volume").over(window_period)
    ).withColumn(
        "volume_ratio",
        col("Volume") / col("volume_mean")
    ).withColumn(
        "volume_z_score",
        (col("Volume") - col("volume_mean")) / col("volume_std")
    ).withColumn(
        "volume_anomaly_type",
        when(col("volume_z_score") > std_multiplier, "EXTREME_VOLUME_SPIKE")
        .when(col("volume_z_score") > 2, "HIGH_VOLUME")
        .when(col("volume_z_score") < -2, "LOW_VOLUME")
        .otherwise("NORMAL_VOLUME")
    ).withColumn(
        "is_volume_anomaly",
        when(abs(col("volume_z_score")) > 2, True).otherwise(False)
    )
    
    # Ph√¢n t√≠ch Volume + Price Direction
    df_with_volume_stats = df_with_volume_stats.withColumn(
        "prev_close",
        lag("Close", 1).over(windowSpec)
    ).withColumn(
        "price_direction",
        when(col("Close") > col("prev_close"), "UP")
        .when(col("Close") < col("prev_close"), "DOWN")
        .otherwise("FLAT")
    ).withColumn(
        "volume_price_signal",
        when((col("volume_anomaly_type") == "EXTREME_VOLUME_SPIKE") & 
             (col("price_direction") == "UP"), "STRONG_BULLISH_VOLUME")
        .when((col("volume_anomaly_type") == "EXTREME_VOLUME_SPIKE") & 
              (col("price_direction") == "DOWN"), "STRONG_BEARISH_VOLUME")
        .when((col("volume_anomaly_type") == "HIGH_VOLUME") & 
              (col("price_direction") == "FLAT"), "ACCUMULATION_DISTRIBUTION")
        .otherwise("NORMAL_VOLUME_PATTERN")
    )
    
    return df_with_volume_stats

def detect_volatility_breakout(df, lookback_period=20):
    """
    Ph√°t hi·ªán breakout v·ªÅ volatility (bi·∫øn ƒë·ªông tƒÉng ƒë·ªôt ng·ªôt)
    
    √ù nghƒ©a Trading:
    - Volatility Expansion: Th·ªã tr∆∞·ªùng b·∫Øt ƒë·∫ßu xu h∆∞·ªõng m·ªõi
    - Volatility Contraction: Th·ªã tr∆∞·ªùng t√≠ch l≈©y, chu·∫©n b·ªã breakout
    - Bollinger Band Squeeze: Volatility th·∫•p nh·∫•t ‚Üí Breakout s·∫Øp x·∫£y ra
    """
    windowSpec = Window.partitionBy("ticker").orderBy("timestamp")
    window_period = windowSpec.rowsBetween(-(lookback_period-1), 0)
    
    # T√≠nh True Range
    df_with_tr = df.withColumn(
        "prev_close",
        lag("Close", 1).over(windowSpec)
    ).withColumn(
        "TR",
        expr("""
            GREATEST(
                High - Low,
                ABS(High - prev_close),
                ABS(Low - prev_close)
            )
        """)
    )
    
    # Average True Range v√† volatility metrics
    df_with_volatility = df_with_tr.withColumn(
        "ATR",
        avg("TR").over(window_period)
    ).withColumn(
        "ATR_mean",
        avg("TR").over(window_period)
    ).withColumn(
        "ATR_std",
        stddev("TR").over(window_period)
    ).withColumn(
        "volatility_ratio",
        col("TR") / col("ATR_mean")
    ).withColumn(
        "volatility_percentile",
        (col("TR") - min("TR").over(window_period)) / 
        (max("TR").over(window_period) - min("TR").over(window_period))
    ).withColumn(
        "volatility_state",
        when(col("volatility_ratio") > 2.0, "EXTREME_VOLATILITY")
        .when(col("volatility_ratio") > 1.5, "HIGH_VOLATILITY")
        .when(col("volatility_ratio") < 0.5, "LOW_VOLATILITY_SQUEEZE")
        .otherwise("NORMAL_VOLATILITY")
    ).withColumn(
        "is_volatility_breakout",
        when(col("volatility_state") == "EXTREME_VOLATILITY", True).otherwise(False)
    )
    
    return df_with_volatility

def detect_circuit_breaker(df, threshold_percent=10.0):
    """
    C·∫£nh b√°o Circuit Breaker (TƒÉng/Gi·∫£m s√†n)
    
    √ù nghƒ©a Trading:
    - Price change > +10%: Hitting upper limit ‚Üí C·ª±c k·ª≥ bullish
    - Price change < -10%: Hitting lower limit ‚Üí C·ª±c k·ª≥ bearish
    - Price change > +7%: Approaching limit ‚Üí Momentum m·∫°nh
    - Price change < -7%: Approaching limit ‚Üí Selling pressure m·∫°nh
    
    C·∫£nh b√°o Risk Management:
    - Kh√¥ng n√™n mua khi ƒëang tƒÉng s√†n (c√≥ th·ªÉ b·ªã stuck)
    - C√¢n nh·∫Øc cut loss n·∫øu gi·∫£m s√†n
    """
    windowSpec = Window.partitionBy("ticker").orderBy("timestamp")
    
    df_with_circuit = df.withColumn(
        "prev_close",
        lag("Close", 1).over(windowSpec)
    ).withColumn(
        "price_change_percent",
        ((col("Close") - col("prev_close")) / col("prev_close")) * 100
    ).withColumn(
        "intraday_range_percent",
        ((col("High") - col("Low")) / col("Open")) * 100
    ).withColumn(
        "circuit_breaker_status",
        when(col("price_change_percent") >= threshold_percent, "UPPER_LIMIT_HIT")
        .when(col("price_change_percent") <= -threshold_percent, "LOWER_LIMIT_HIT")
        .when(col("price_change_percent") >= (threshold_percent * 0.7), "APPROACHING_UPPER_LIMIT")
        .when(col("price_change_percent") <= -(threshold_percent * 0.7), "APPROACHING_LOWER_LIMIT")
        .when(col("price_change_percent") >= (threshold_percent * 0.5), "STRONG_UPWARD_MOVEMENT")
        .when(col("price_change_percent") <= -(threshold_percent * 0.5), "STRONG_DOWNWARD_MOVEMENT")
        .otherwise("NORMAL_MOVEMENT")
    ).withColumn(
        "is_circuit_breaker",
        when(col("circuit_breaker_status").isin([
            "UPPER_LIMIT_HIT", "LOWER_LIMIT_HIT",
            "APPROACHING_UPPER_LIMIT", "APPROACHING_LOWER_LIMIT"
        ]), True).otherwise(False)
    )
    
    return df_with_circuit

def detect_gap(df, gap_threshold_percent=2.0):
    """
    Ph√°t hi·ªán Gap (Gi√° m·ªü c·ª≠a kh√°c bi·ªát l·ªõn v·ªõi gi√° ƒë√≥ng c·ª≠a tr∆∞·ªõc)
    
    √ù nghƒ©a Trading:
    - Gap Up: Tin t·ª©c t·ªët overnight ‚Üí Bullish
    - Gap Down: Tin t·ª©c x·∫•u overnight ‚Üí Bearish
    - Gap Fill: Gi√° quay l·∫°i fill gap ‚Üí Reversal pattern
    - Breakaway Gap: Gap kh√¥ng fill ‚Üí Xu h∆∞·ªõng m·∫°nh ti·∫øp t·ª•c
    """
    windowSpec = Window.partitionBy("ticker").orderBy("timestamp")
    
    df_with_gap = df.withColumn(
        "prev_close",
        lag("Close", 1).over(windowSpec)
    ).withColumn(
        "gap_percent",
        ((col("Open") - col("prev_close")) / col("prev_close")) * 100
    ).withColumn(
        "gap_type",
        when(col("gap_percent") >= gap_threshold_percent, "GAP_UP")
        .when(col("gap_percent") <= -gap_threshold_percent, "GAP_DOWN")
        .otherwise("NO_GAP")
    ).withColumn(
        "gap_size",
        abs(col("gap_percent"))
    ).withColumn(
        "is_gap",
        when(abs(col("gap_percent")) >= gap_threshold_percent, True).otherwise(False)
    )
    
    # Check if gap is filled during the day
    df_with_gap = df_with_gap.withColumn(
        "gap_filled",
        when(
            (col("gap_type") == "GAP_UP") & (col("Low") <= col("prev_close")),
            True
        ).when(
            (col("gap_type") == "GAP_DOWN") & (col("High") >= col("prev_close")),
            True
        ).otherwise(False)
    ).withColumn(
        "gap_signal",
        when((col("gap_type") == "GAP_UP") & (col("gap_filled") == False), "STRONG_BULLISH_GAP")
        .when((col("gap_type") == "GAP_DOWN") & (col("gap_filled") == False), "STRONG_BEARISH_GAP")
        .when(col("gap_filled") == True, "GAP_FILL_REVERSAL")
        .otherwise("NO_GAP_SIGNAL")
    )
    
    return df_with_gap

def generate_risk_alert(df):
    """
    T·ªïng h·ª£p t·∫•t c·∫£ c√°c anomaly v√† t·∫°o risk alert level
    
    Risk Alert Levels:
    - CRITICAL: Multiple severe anomalies detected
    - HIGH: Severe anomaly in price or volume
    - MEDIUM: Moderate anomaly detected
    - LOW: Minor anomaly
    - NORMAL: No anomaly
    """
    df_with_risk = df.withColumn(
        "anomaly_count",
        (when(col("is_price_anomaly"), 1).otherwise(0) +
         when(col("is_volume_anomaly"), 1).otherwise(0) +
         when(col("is_volatility_breakout"), 1).otherwise(0) +
         when(col("is_circuit_breaker"), 1).otherwise(0) +
         when(col("is_gap"), 1).otherwise(0))
    ).withColumn(
        "risk_alert_level",
        when(col("anomaly_count") >= 3, "CRITICAL")
        .when((col("anomaly_count") == 2) | 
              col("price_anomaly_type").isin(["EXTREME_SPIKE", "EXTREME_DROP"]) |
              (col("circuit_breaker_status").isin(["UPPER_LIMIT_HIT", "LOWER_LIMIT_HIT"])),
              "HIGH")
        .when(col("anomaly_count") == 1, "MEDIUM")
        .otherwise("NORMAL")
    ).withColumn(
        "action_recommendation",
        when(
            (col("risk_alert_level") == "CRITICAL") & 
            (col("price_direction") == "DOWN"),
            "CONSIDER_STOP_LOSS_IMMEDIATELY"
        ).when(
            (col("risk_alert_level") == "CRITICAL") & 
            (col("price_direction") == "UP"),
            "CONSIDER_TAKE_PROFIT_OR_TRAILING_STOP"
        ).when(
            col("risk_alert_level") == "HIGH",
            "MONITOR_CLOSELY_REDUCE_POSITION_SIZE"
        ).when(
            col("gap_signal") == "STRONG_BULLISH_GAP",
            "POTENTIAL_ENTRY_ON_PULLBACK"
        ).when(
            col("gap_signal") == "STRONG_BEARISH_GAP",
            "AVOID_CATCHING_FALLING_KNIFE"
        ).otherwise("NORMAL_TRADING")
    )
    
    return df_with_risk

# ============================================================================
# MAIN STREAMING JOB
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("üö® B·∫ÆT ƒê·∫¶U SPARK STREAMING: ANOMALY DETECTION & RISK ALERTS")
    print("=" * 80)
    
    # Config
    KAFKA_BROKER = os.getenv("KAFKA_BROKER", "kafka:9092")
    KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "stock-realtime-topic")
    ES_NODES = os.getenv("ES_NODES", "elasticsearch")
    ES_PORT = os.getenv("ES_PORT", "9200")
    ES_INDEX = os.getenv("ES_INDEX", "stock_anomaly_alerts")
    CHECKPOINT_LOCATION = os.getenv(
        "CHECKPOINT_LOCATION",
        "hdfs://hadoop-namenode:8020/user/spark_checkpoints/stock_anomaly_alerts"
    )
    
    # Kh·ªüi t·∫°o Spark
    print("\nüìä Kh·ªüi t·∫°o SparkSession...")
    spark = SparkSession.builder \
        .appName("Stock Anomaly Detection Real-time") \
        .config("spark.es.nodes", ES_NODES) \
        .config("spark.es.port", ES_PORT) \
        .config("spark.es.nodes.wan.only", "true") \
        .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_LOCATION) \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    print("‚úÖ SparkSession ƒë√£ s·∫µn s√†ng")
    
    # ƒê·ªçc streaming t·ª´ Kafka
    print(f"\nüì° ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka - Topic: {KAFKA_TOPIC}")
    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BROKER) \
        .option("subscribe", KAFKA_TOPIC) \
        .option("startingOffsets", "latest") \
        .option("failOnDataLoss", "false") \
        .load()
    
    # Parse JSON
    print("\nüîç Parse d·ªØ li·ªáu JSON...")
    parsed_df = kafka_df \
        .select(
            col("timestamp").alias("kafka_timestamp"),
            col("value").cast(StringType())
        ) \
        .filter(col("value").isNotNull()) \
        .withColumn("data", from_json(col("value"), stock_schema)) \
        .select("kafka_timestamp", "data.*") \
        .withColumn("timestamp", to_timestamp(col("time"))) \
        .filter(col("Close").isNotNull())
    
    # Watermark
    print("\n‚è∞ √Åp d·ª•ng Watermark...")
    watermarked_df = parsed_df.withWatermark("timestamp", "10 minutes")
    
    # Detect anomalies
    print("\nüîç Ph√°t hi·ªán c√°c anomaly...")
    print("  - Price Anomaly Detection (Z-Score)")
    df_with_price_anomaly = detect_price_anomaly(watermarked_df, std_multiplier=3.0)
    
    print("  - Volume Anomaly Detection")
    df_with_volume_anomaly = detect_volume_anomaly(df_with_price_anomaly, std_multiplier=3.0)
    
    print("  - Volatility Breakout Detection")
    df_with_volatility = detect_volatility_breakout(df_with_volume_anomaly)
    
    print("  - Circuit Breaker Detection")
    df_with_circuit = detect_circuit_breaker(df_with_volatility, threshold_percent=10.0)
    
    print("  - Gap Detection")
    df_with_gap = detect_gap(df_with_circuit, gap_threshold_percent=2.0)
    
    print("  - Risk Alert Generation")
    df_final = generate_risk_alert(df_with_gap)
    
    # Select output columns
    output_df = df_final.select(
        "timestamp",
        "ticker",
        "company",
        "Open", "High", "Low", "Close", "Volume",
        # Price Anomaly
        "z_score", "price_anomaly_type", "is_price_anomaly",
        # Volume Anomaly
        "volume_ratio", "volume_anomaly_type", "volume_price_signal", "is_volume_anomaly",
        # Volatility
        "volatility_ratio", "volatility_state", "is_volatility_breakout",
        # Circuit Breaker
        "price_change_percent", "circuit_breaker_status", "is_circuit_breaker",
        # Gap
        "gap_percent", "gap_type", "gap_filled", "gap_signal", "is_gap",
        # Risk Alert
        "anomaly_count", "risk_alert_level", "action_recommendation"
    )
    
    # Ghi v√†o Elasticsearch
    print(f"\nüíæ Ghi k·∫øt qu·∫£ v√†o Elasticsearch - Index: {ES_INDEX}")
    
    query = output_df.writeStream \
        .outputMode("append") \
        .format("org.elasticsearch.spark.sql") \
        .option("es.resource", ES_INDEX) \
        .option("es.nodes", ES_NODES) \
        .option("es.port", ES_PORT) \
        .option("es.nodes.wan.only", "true") \
        .option("checkpointLocation", CHECKPOINT_LOCATION) \
        .start()
    
    # Console output for CRITICAL and HIGH alerts
    console_query = output_df \
        .filter(col("risk_alert_level").isin(["CRITICAL", "HIGH"])) \
        .writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .start()
    
    print("\n" + "=" * 80)
    print("üö® ANOMALY DETECTION ƒêANG HO·∫†T ƒê·ªòNG")
    print("=" * 80)
    print("\n‚ö†Ô∏è  CRITICAL v√† HIGH risk alerts s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã ·ªü console\n")
    
    query.awaitTermination()
    console_query.awaitTermination()
