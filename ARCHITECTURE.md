# ğŸ“š TÃ€I LIá»†U KIáº¾N TRÃšC Há»† THá»NG BIG DATA

## ğŸ¯ Tá»”NG QUAN Dá»° ÃN

### Má»¥c tiÃªu

XÃ¢y dá»±ng há»‡ thá»‘ng phÃ¢n tÃ­ch dá»¯ liá»‡u chá»©ng khoÃ¡n real-time káº¿t há»£p batch processing, triá»ƒn khai trÃªn Google Kubernetes Engine (GKE).

### CÃ´ng nghá»‡ sá»­ dá»¥ng

- **Stream Processing**: Apache Kafka, Spark Structured Streaming
- **Batch Processing**: Python, Pandas
- **Storage**: HDFS (Hadoop), Elasticsearch
- **Visualization**: Kibana
- **Orchestration**: Kubernetes (GKE)
- **Container**: Docker

---

## ğŸ—ï¸ KIáº¾N TRÃšC LAMBDA

### Lambda Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA SOURCES                                â”‚
â”‚                   (Stock Market APIs)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                  â”‚                  â”‚
                   â–¼                  â–¼                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  KAFKA PRODUCERâ”‚  â”‚ HISTORY.JSONâ”‚  â”‚  MARKET DATA   â”‚
          â”‚   (Real-time)  â”‚  â”‚   (Batch)   â”‚  â”‚   (External)   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                 â”‚                   â”‚
                   â–¼                 â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                      LAMBDA ARCHITECTURE                      â”‚
    â”‚                                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚  â”‚              BATCH LAYER (Cold Path)                    â”‚â”‚
    â”‚  â”‚  â€¢ Purpose: Historical data processing                  â”‚â”‚
    â”‚  â”‚  â€¢ Frequency: Daily (2 AM)                              â”‚â”‚
    â”‚  â”‚  â€¢ Processing: Python batch jobs                        â”‚â”‚
    â”‚  â”‚                                                          â”‚â”‚
    â”‚  â”‚  Batch Jobs:                                            â”‚â”‚
    â”‚  â”‚  â”œâ”€ batch_trend.py        (MA50, MA100, MA200)         â”‚â”‚
    â”‚  â”‚  â”œâ”€ drawdown.py           (Max drawdown)               â”‚â”‚
    â”‚  â”‚  â”œâ”€ cumulative_return.py  (Returns)                    â”‚â”‚
    â”‚  â”‚  â”œâ”€ volume_features.py    (Volume analysis)            â”‚â”‚
    â”‚  â”‚  â”œâ”€ market_regime.py      (Market state)               â”‚â”‚
    â”‚  â”‚  â””â”€ monthly.py            (Monthly metrics)            â”‚â”‚
    â”‚  â”‚                                                          â”‚â”‚
    â”‚  â”‚  Storage: HDFS + Elasticsearch                          â”‚â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â”‚                                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚  â”‚              SPEED LAYER (Hot Path)                     â”‚â”‚
    â”‚  â”‚  â€¢ Purpose: Real-time streaming                         â”‚â”‚
    â”‚  â”‚  â€¢ Latency: < 30 seconds                                â”‚â”‚
    â”‚  â”‚  â€¢ Processing: Spark Streaming                          â”‚â”‚
    â”‚  â”‚                                                          â”‚â”‚
    â”‚  â”‚  Pipeline:                                              â”‚â”‚
    â”‚  â”‚  Kafka â†’ Spark Streaming â†’ Elasticsearch               â”‚â”‚
    â”‚  â”‚    â”‚                                                    â”‚â”‚
    â”‚  â”‚    â””â”€â†’ HDFS (via Kafka Consumer)                       â”‚â”‚
    â”‚  â”‚                                                          â”‚â”‚
    â”‚  â”‚  Metrics:                                               â”‚â”‚
    â”‚  â”‚  â”œâ”€ Avg price (30s window)                             â”‚â”‚
    â”‚  â”‚  â”œâ”€ Min/Max price                                       â”‚â”‚
    â”‚  â”‚  â”œâ”€ Total volume                                        â”‚â”‚
    â”‚  â”‚  â”œâ”€ Trade count                                         â”‚â”‚
    â”‚  â”‚  â””â”€ Price volatility                                    â”‚â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â”‚                                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚  â”‚              SERVING LAYER                              â”‚â”‚
    â”‚  â”‚  â€¢ Storage: Elasticsearch (indexing & query)            â”‚â”‚
    â”‚  â”‚  â€¢ Visualization: Kibana dashboards                     â”‚â”‚
    â”‚  â”‚  â€¢ Data Lake: HDFS (long-term storage)                  â”‚â”‚
    â”‚  â”‚  â€¢ Query: Unified view of batch + streaming            â”‚â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    END USERS          â”‚
                    â”‚  â€¢ Analysts           â”‚
                    â”‚  â€¢ Traders            â”‚
                    â”‚  â€¢ Data Scientists    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Táº¡i sao Lambda Architecture?

#### 1. **Batch Layer** giáº£i quyáº¿t:

- **Äá»™ chÃ­nh xÃ¡c cao**: Xá»­ lÃ½ toÃ n bá»™ historical data vá»›i logic phá»©c táº¡p
- **TÃ­nh toÃ¡n náº·ng**: Moving averages 200 ngÃ y, cumulative returns
- **Reprocessing**: CÃ³ thá»ƒ tÃ­nh láº¡i khi cÃ³ bug hoáº·c thay Ä‘á»•i logic
- **Cost-effective**: Cháº¡y 1 láº§n/ngÃ y, táº­n dá»¥ng resources khi idle

#### 2. **Speed Layer** giáº£i quyáº¿t:

- **Äá»™ trá»… tháº¥p**: Cáº£nh bÃ¡o real-time, live monitoring
- **Event-driven**: Xá»­ lÃ½ tá»«ng message khi Ä‘áº¿n
- **Immediate insights**: PhÃ¡t hiá»‡n anomalies ngay láº­p tá»©c
- **High throughput**: Xá»­ lÃ½ hÃ ng nghÃ¬n messages/giÃ¢y

#### 3. **Serving Layer** merge cáº£ hai:

- Elasticsearch index cáº£ batch views vÃ  streaming views
- Query API thá»‘ng nháº¥t
- Balance giá»¯a accuracy (batch) vÃ  freshness (streaming)

---

## ğŸ“Š DATA FLOW CHI TIáº¾T

### 1. Real-time Flow (Speed Layer)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Stock API/Feedâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Simulated by kafka_producer.py
       â”‚ Fields: ticker, company, time, Open, High, Low, Close, Volume
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Topic      â”‚
â”‚ stocks-history   â”‚
â”‚ Retention: 7 daysâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                        â”‚                        â”‚
       â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Spark Stream  â”‚      â”‚Kafka Consumerâ”‚      â”‚Other Consumerâ”‚
â”‚              â”‚      â”‚(HDFS Writer) â”‚      â”‚  (Future)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚
       â”‚ Aggregate 30s       â”‚ Write raw to HDFS
       â”‚ window              â”‚ Path: /user/kafka_data/
       â”‚                     â”‚       stocks_history/
       â–¼                     â”‚       YYYY/MM/DD/HH/*.jsonl
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚Elasticsearch â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚Index:        â”‚
â”‚stock_realtimeâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Kibana    â”‚
â”‚  Dashboard   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Batch Flow (Batch Layer)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  history.json    â”‚ (Sample historical data)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ Loaded by run_all.py
          â”‚ Via standardization_local.load_history()
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Batch Processing Pipeline                   â”‚
â”‚                                                       â”‚
â”‚  1. batch_trend.py                                    â”‚
â”‚     â†’ MA50, MA100, MA200                              â”‚
â”‚     â†’ Trend classification (up/down/sideway)          â”‚
â”‚     â†’ Trend strength                                  â”‚
â”‚                                                       â”‚
â”‚  2. drawdown.py                                       â”‚
â”‚     â†’ Peak price tracking                             â”‚
â”‚     â†’ Max drawdown calculation                        â”‚
â”‚                                                       â”‚
â”‚  3. cumulative_return.py                              â”‚
â”‚     â†’ Returns from start                              â”‚
â”‚     â†’ Cumulative growth                               â”‚
â”‚                                                       â”‚
â”‚  4. volume_features.py                                â”‚
â”‚     â†’ Volume MA                                       â”‚
â”‚     â†’ Volume ratio                                    â”‚
â”‚     â†’ Volume spikes                                   â”‚
â”‚                                                       â”‚
â”‚  5. market_regime.py                                  â”‚
â”‚     â†’ Bull/Bear/Neutral classification                â”‚
â”‚     â†’ Based on trend + volatility                     â”‚
â”‚                                                       â”‚
â”‚  6. monthly.py                                        â”‚
â”‚     â†’ Monthly returns                                 â”‚
â”‚     â†’ Monthly volatility                              â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                 â”‚                 â”‚
                   â–¼                 â–¼                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     HDFS     â”‚  â”‚Elasticsearch â”‚  â”‚   Kibana     â”‚
         â”‚/tmp/serving/ â”‚  â”‚Index:        â”‚  â”‚ Dashboard    â”‚
         â”‚batch_        â”‚  â”‚batch_        â”‚  â”‚ (Long-term)  â”‚
         â”‚features.json â”‚  â”‚features      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Serving Layer Query

```
User Query â†’ Kibana â†’ Elasticsearch
                          â”‚
                          â”œâ”€ Index: stock_realtime*
                          â”‚  (Last 30 seconds - 1 hour)
                          â”‚
                          â””â”€ Index: batch_features*
                             (Historical analysis)
```

---

## ğŸ”§ THÃ€NH PHáº¦N Há»† THá»NG

### Infrastructure Layer (Kubernetes)

```
Namespace: bigdata
â”‚
â”œâ”€ StatefulSets (Stateful workloads)
â”‚  â”œâ”€ zookeeper (1 replica)
â”‚  â”‚  â””â”€ PVC: zookeeper-pvc (10Gi)
â”‚  â”œâ”€ kafka (1 replica)
â”‚  â”‚  â””â”€ PVC: kafka-pvc (50Gi)
â”‚  â”œâ”€ hadoop-namenode (1 replica)
â”‚  â”‚  â””â”€ PVC: hdfs-namenode-pvc (50Gi)
â”‚  â”œâ”€ hadoop-datanode (1 replica)
â”‚  â”‚  â””â”€ PVC: hdfs-datanode-pvc (100Gi)
â”‚  â””â”€ elasticsearch (1 replica)
â”‚     â””â”€ PVC: elasticsearch-pvc (50Gi)
â”‚
â”œâ”€ Deployments (Stateless workloads)
â”‚  â”œâ”€ kibana (1 replica, HPA-ready)
â”‚  â”œâ”€ kafka-producer (1 replica, HPA-enabled)
â”‚  â”œâ”€ kafka-consumer (1 replica, HPA-enabled)
â”‚  â””â”€ spark-streaming (1 replica)
â”‚
â”œâ”€ CronJobs (Scheduled jobs)
â”‚  â””â”€ batch-processing (Daily 2 AM)
â”‚
â”œâ”€ Services (ClusterIP, LoadBalancer)
â”‚  â”œâ”€ zookeeper:2181
â”‚  â”œâ”€ kafka:9092
â”‚  â”œâ”€ hadoop-namenode:9000,9870
â”‚  â”œâ”€ hadoop-datanode:9864
â”‚  â”œâ”€ elasticsearch:9200,9300
â”‚  â””â”€ kibana:5601 (LoadBalancer)
â”‚
â”œâ”€ ConfigMaps
â”‚  â””â”€ bigdata-config (Environment variables)
â”‚
â”œâ”€ HorizontalPodAutoscalers
â”‚  â”œâ”€ kafka-producer-hpa (CPU 70%, Mem 80%)
â”‚  â””â”€ kafka-consumer-hpa (CPU 70%, Mem 80%)
â”‚
â””â”€ NetworkPolicies
   â”œâ”€ kafka-network-policy
   â””â”€ elasticsearch-network-policy
```

### Application Layer

#### 1. **kafka_producer.py**

```python
Role: Simulate real-time stock data
Features:
  - Reads from history.json
  - Generates price movements with volatility
  - Publishes to Kafka topic: stocks-history
  - Update interval: 30 seconds (configurable)
Schema: ticker, company, time, Open, High, Low, Close, Adj Close, Volume
```

#### 2. **kafka_consumer.py**

```python
Role: Persist streaming data to HDFS
Features:
  - Consumes from Kafka
  - Batching: 100 records or 60 seconds
  - Writes to HDFS: /user/kafka_data/stocks_history/YYYY/MM/DD/HH/data_*.jsonl
  - Fault-tolerant with retries
Consumer Group: hdfs-writer-group-v1
```

#### 3. **spark_streaming_simple.py**

```python
Role: Real-time aggregation and indexing
Features:
  - Reads from Kafka (stocks-history topic)
  - 30-second tumbling window aggregations
  - Computes: avg_price, min_price, max_price, total_volume, trade_count, volatility
  - Writes to Elasticsearch index: stock_realtime
  - Checkpoint: HDFS /user/spark_checkpoints/stock_realtime_v1
Watermark: 1 minute late data tolerance
```

#### 4. **Batch Jobs** (run_all.py orchestrator)

```python
batch_trend.py:
  - MA50, MA100, MA200
  - Trend classification

drawdown.py:
  - Peak tracking
  - Max drawdown

cumulative_return.py:
  - Cumulative returns

volume_features.py:
  - Volume analysis

market_regime.py:
  - Bull/Bear/Neutral

monthly.py:
  - Monthly aggregations

Output:
  - HDFS: /tmp/serving/batch_features.json
  - Elasticsearch: batch_features index
```

#### 5. **standardization_local.py**

```python
Role: Unified schema definitions
Features:
  - Schema constants (field names)
  - PySpark schema for streaming
  - Pandas schema for batch
  - Ensures consistency across batch/streaming
```

---

## ğŸš€ DEPLOYMENT WORKFLOW

### Pre-deployment

```bash
1. Build Docker image
   ./scripts/build-and-push.sh

2. Create GKE cluster
   ./scripts/create-cluster.sh bigdata-cluster asia-southeast1

3. Update PROJECT_ID in manifests
   sed -i 's/PROJECT_ID/your-project-id/g' k8s/*.yaml
```

### Deployment Order

```bash
1. Namespace & ConfigMap
   kubectl apply -f k8s/namespace.yaml
   kubectl apply -f k8s/configmap.yaml

2. Storage
   kubectl apply -f k8s/persistent-volumes.yaml

3. Stateful Services (in order)
   kubectl apply -f k8s/zookeeper-statefulset.yaml
   kubectl apply -f k8s/kafka-statefulset.yaml
   kubectl apply -f k8s/hdfs-statefulset.yaml
   kubectl apply -f k8s/elasticsearch-statefulset.yaml

4. Application Services
   kubectl apply -f k8s/kibana-deployment-updated.yaml
   kubectl apply -f k8s/kafka-producer-deployment.yaml
   kubectl apply -f k8s/kafka-consumer-deployment.yaml
   kubectl apply -f k8s/spark-streaming-deployment.yaml

5. Batch Jobs
   kubectl apply -f k8s/batch-job-cronjob.yaml

6. Autoscaling & Policies
   kubectl apply -f k8s/hpa.yaml
   kubectl apply -f k8s/network-policy.yaml
```

### Automated Deployment

```bash
./scripts/deploy.sh
```

---

## ğŸ“ˆ PERFORMANCE & SCALABILITY

### Current Capacity

- **Kafka**: ~10,000 messages/second
- **Spark Streaming**: 4 partitions, local[*] mode
- **Elasticsearch**: Single node, ~50GB storage
- **HDFS**: 100GB total (50GB NameNode + 100GB DataNode)

### Scaling Strategies

#### Horizontal Scaling

```bash
# Scale Kafka consumers
kubectl scale deployment kafka-consumer --replicas=3 -n bigdata

# Scale producers
kubectl scale deployment kafka-producer --replicas=2 -n bigdata
```

#### Vertical Scaling

```yaml
# Increase resources
resources:
  requests:
    memory: "4Gi"
    cpu: "2000m"
  limits:
    memory: "8Gi"
    cpu: "4000m"
```

#### Cluster Scaling

```bash
# Add more nodes
gcloud container clusters resize bigdata-cluster --num-nodes=5 --zone=asia-southeast1-a
```

#### Kafka Partitioning

```bash
# Increase partitions for parallelism
kubectl exec -it kafka-0 -n bigdata -- \
  kafka-topics --alter --topic stocks-history --partitions 4 --bootstrap-server localhost:9092
```

#### Elasticsearch Scaling

```bash
# Add more ES nodes
kubectl scale statefulset elasticsearch --replicas=3 -n bigdata
```

### Performance Tuning

#### Spark

```yaml
spark.sql.shuffle.partitions: 8 # Increase for more parallelism
spark.executor.memory: 4g
spark.driver.memory: 2g
spark.streaming.kafka.maxRatePerPartition: 1000
```

#### Kafka

```yaml
KAFKA_NUM_PARTITIONS: 4
KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB
KAFKA_LOG_RETENTION_HOURS: 168 # 7 days
```

#### Elasticsearch

```yaml
ES_JAVA_OPTS: "-Xms2g -Xmx2g" # Increase heap
indices.memory.index_buffer_size: 20%
```

---

## ğŸ” SECURITY

### Network Policies

```yaml
# Kafka: Only accept from producer, consumer, spark
# Elasticsearch: Only accept from Kibana, Spark, batch jobs
# HDFS: Only accept from consumer, spark
```

### RBAC

```yaml
# Service accounts with minimal permissions
# Batch jobs: read ConfigMaps, write to ES/HDFS
# Apps: read Secrets, write to Kafka/ES
```

### Secrets Management

```bash
# Use Google Secret Manager
gcloud secrets create kafka-password --data-file=-
```

### Workload Identity

```bash
# GKE Workload Identity to access GCP services
# No need for service account keys
```

---

## ğŸ” MONITORING

### Metrics to Track

#### Application Metrics

- Kafka: Messages/sec, consumer lag
- Spark: Processing time, records/sec
- Elasticsearch: Index rate, query latency
- HDFS: Disk usage, block health

#### Infrastructure Metrics

- CPU/Memory usage per pod
- Network I/O
- Disk I/O
- PVC usage

#### Business Metrics

- Number of tickers tracked
- Data freshness (batch vs streaming)
- Query response time

### Monitoring Stack

```
Google Cloud Monitoring (default)
+ Prometheus (optional)
+ Grafana (optional)
+ Kibana (built-in)
```

### Alerts

- Pod crashes
- High CPU/Memory
- Kafka consumer lag > 1000
- HDFS disk > 80%
- Elasticsearch cluster status != green

---

## ğŸ“š REFERENCES

### Documentation

- [GKE Docs](https://cloud.google.com/kubernetes-engine/docs)
- [Kafka Docs](https://kafka.apache.org/documentation/)
- [Spark Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)

### Best Practices

- [Kubernetes Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)
- [Lambda Architecture](http://lambda-architecture.net/)
- [Data Engineering](https://www.databricks.com/glossary/lambda-architecture)

---

## ğŸ“ LEARNING PATH

### Beginner

1. Understand Kubernetes basics
2. Learn Docker containerization
3. Study Kafka fundamentals
4. Explore Pandas for data processing

### Intermediate

5. Master Spark Structured Streaming
6. Learn HDFS architecture
7. Elasticsearch indexing strategies
8. GKE cluster management

### Advanced

9. Lambda Architecture patterns
10. Stream processing optimizations
11. Production-grade monitoring
12. CI/CD for data pipelines

---

**Last Updated**: January 2026
**Version**: 1.0.0
**Maintainer**: Big Data Team
