# H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG - BIG DATA PIPELINE TH·ªêNG NH·∫§T

**Real-time Stock Data Processing: Batch + Streaming Pipeline**

---

## üìã T·ªîNG QUAN H·ªÜ TH·ªêNG

### Ki·∫øn tr√∫c t·ªïng quan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  history.json   ‚îÇ (D·ªØ li·ªáu l·ªãch s·ª≠)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KAFKA PRODUCER                             ‚îÇ
‚îÇ  - ƒê·ªçc history.json l√†m baseline                             ‚îÇ
‚îÇ  - S·ª≠ d·ª•ng price_simulator.py ƒë·ªÉ m√¥ ph·ªèng gi√° realtime      ‚îÇ
‚îÇ  - Simulate realtime prices v·ªõi volatility ƒë·ªông              ‚îÇ
‚îÇ  - G·ª≠i v√†o topic: stocks-history                             ‚îÇ
‚îÇ  - Schema th·ªëng nh·∫•t: ticker, company, time, OHLCV           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               v
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  KAFKA BROKER ‚îÇ
       ‚îÇ stocks-history‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ       ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                     ‚îÇ
    v                     v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HDFS    ‚îÇ      ‚îÇ SPARK STREAMING‚îÇ
‚îÇ Consumer ‚îÇ      ‚îÇ   (realtime)   ‚îÇ
‚îÇ  (raw)   ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
                          v
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇElasticsearch ‚îÇ
                  ‚îÇstock_realtime‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         v
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ  KIBANA  ‚îÇ
                   ‚îÇ Dashboard‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   BATCH PROCESSING (python-worker)   ‚îÇ
‚îÇ - ƒê·ªçc history.json                   ‚îÇ
‚îÇ - T√≠nh batch features:               ‚îÇ
‚îÇ   + Trend (MA50, MA100, MA200)       ‚îÇ
‚îÇ   + Cumulative Return                ‚îÇ
‚îÇ   + Drawdown                         ‚îÇ
‚îÇ   + Volume Features                  ‚îÇ
‚îÇ   + Monthly Volatility               ‚îÇ
‚îÇ   + Market Regime                    ‚îÇ
‚îÇ - Ghi HDFS: /serving/batch_features  ‚îÇ
‚îÇ - Ghi ES: batch-features index       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Lu·ªìng d·ªØ li·ªáu th·ªëng nh·∫•t

**1. SCHEMA CHUNG (T·∫•t c·∫£ services s·ª≠ d·ª•ng)**

```json
{
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "time": "2025-01-05T10:30:00Z",
  "Open": 280.5,
  "High": 282.1,
  "Low": 278.2,
  "Close": 281.0,
  "Adj Close": 281.0,
  "Volume": 45000000
}
```

**2. KAFKA TOPIC**: `stocks-history` (topic duy nh·∫•t cho c·∫£ batch v√† streaming)

**3. ELASTICSEARCH INDEXES**:

- `stock_realtime`: Streaming metrics (30s windows)
- `batch-features`: Batch engineered features

---

## üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG

## üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG

### B∆∞·ªõc 1: Start t·∫•t c·∫£ services

```bash
cd d:\HUST\2025_1\BIGDATA\big_data
docker compose up -d
```

**Ch·ªù 30 gi√¢y** ƒë·ªÉ c√°c services kh·ªüi ƒë·ªông ho√†n t·∫•t.

### B∆∞·ªõc 2: Ki·ªÉm tra services ƒëang ch·∫°y

```bash
docker compose ps
```

Ph·∫£i th·∫•y c√°c containers:

- ‚úÖ `zookeeper`
- ‚úÖ `kafka`
- ‚úÖ `hadoop-namenode`, `hadoop-datanode`
- ‚úÖ `elasticsearch`
- ‚úÖ `kibana`
- ‚úÖ `python-worker`
- ‚úÖ `spark-streaming-simple`
- ‚úÖ `spark-anomaly-detection` (Price Anomaly Detection)

### B∆∞·ªõc 3: Start Kafka Producer (g·ª≠i d·ªØ li·ªáu realtime)

```bash
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"
```

Ki·ªÉm tra Producer ƒëang ch·∫°y:

```bash
docker exec python-worker tail -20 /tmp/producer.log
```

Ph·∫£i th·∫•y:

```
[READY] 2025-01-05 10:30:00: Starting data stream...
[BATCH 1] 2025-01-05T10:30:00Z: Sent 2 messages
[BATCH 2] 2025-01-05T10:30:30Z: Sent 2 messages
```

### B∆∞·ªõc 4: Ki·ªÉm tra Spark Streaming ƒëang ch·∫°y

**Ki·ªÉm tra Spark Streaming (metrics aggregation):**

```bash
docker logs spark-streaming-simple --tail 30
```

Ph·∫£i th·∫•y:

```
[INFO] Starting Spark Streaming Job
[INFO] Kafka: kafka:9092, Topic: stocks-history
[INFO] Elasticsearch: elasticsearch:9200, Index: stock_realtime
[INFO] Streaming query started. Writing to stock_realtime
```

**Ki·ªÉm tra Spark Anomaly Detection (price anomalies):**

```bash
docker logs spark-anomaly-detection --tail 30
```

Ph·∫£i th·∫•y:

```
[INFO] Starting Anomaly Detection Job
[INFO] Thresholds:  ... docs.count > 0
yellow open stock_anomalies  ... docs.count >= 0
```

**L∆∞u √Ω:** Index `stock_anomalies` ch·ªâ c√≥ documents khi ph√°t hi·ªán b·∫•t th∆∞·ªùng. N·∫øu kh√¥ng c√≥ anomaly th√¨ index r·ªóng ho·∫∑c kh√¥ng t·ªìn t·∫°i. Price change: >5.0%

- Volume spike: >3.0x
- Volatility: >3.0%
- Price gap: >2.0%
  [INFO] Anomaly detection started. Writing to stock_anomalies

````

### B∆∞·ªõc 5: Ch·ªù d·ªØ li·ªáu ƒë∆∞·ª£c ghi v√†o Elasticsearch (2-3 ph√∫t)

```bash
sleep 120
curl -s "http://localhost:9200/_cat/indices?v" | grep stock
````

Ph·∫£i th·∫•y:

```
yellow open stock_realtime ... docs.count > 0
```

---

## üîÑ CH·∫†Y BATCH PROCESSING

### Ch·∫°y 1 l·∫ßn (manual)

```bash
docker exec python-worker python /app/unified_runner.py batch
```

Output:

```
============================================================
STARTING BATCH PROCESSING
============================================================
BAT ƒê·∫¶U ƒê·∫®Y D·ªÆ LI·ªÜU L√äN HDFS...
DONE: ƒê√£ l∆∞u v√†o HDFS t·∫°i /serving/batch_features.json
BAT ƒê·∫¶U ƒê·∫®Y D·ªÆ LI·ªÜU L√äN ELASTICSEARCH...
Indexed XXX documents into batch-features
DONE! H·ªÜ TH·ªêNG ƒê√É C·∫¨P NH·∫¨T C·∫¢ HDFS V√Ä ELASTICSEARCH.
‚úì Batch processing completed in XX.XXs
```

### Ch·∫°y ƒë·ªãnh k·ª≥ (daemon mode)

```bash
# Ch·∫°y batch m·ªói 24h + monitor system health m·ªói 30 ph√∫t
docker exec -d python-worker python /app/unified_runner.py

# Ho·∫∑c ch·ªâ monitor (kh√¥ng ch·∫°y batch)
docker exec -d python-worker bash -c "RUN_MODE=monitor python /app/unified_runner.py"

# Ho·∫∑c ch·ªâ ch·∫°y batch ƒë·ªãnh k·ª≥
docker exec -d python-worker bash -c "RUN_MODE=batch BATCH_INTERVAL_HOURS=12 python /app/unified_runner.py"
```

Ki·ªÉm tra:

```bash
docker exec python-worker python - <<'PY'
import os

def read_cmdline(pid: str) -> str:
  try:
    with open(f"/proc/{pid}/cmdline", "rb") as f:
      raw = f.read()
    return raw.replace(b"\x00", b" ").decode("utf-8", errors="replace").strip()
  except Exception:
    return ""

matches = []
for pid in os.listdir("/proc"):
  if pid.isdigit():
    cmd = read_cmdline(pid)
    if "unified_runner.py" in cmd:
      matches.append((pid, cmd))

if not matches:
  print("NOT_RUNNING")
else:
  for pid, cmd in sorted(matches, key=lambda x: int(x[0])):
    print(f"PID={pid} CMD={cmd}")
PY
```

---

## üìä XEM D·ªÆ LI·ªÜU TR√äN KIBANA

## üìä XEM D·ªÆ LI·ªÜU TR√äN KIBANA

### B∆∞·ªõc 1: M·ªü Kibana

```
http://localhost:5601
```

### B∆∞·ªõc 2: T·∫°o Index Patterns

#### Index Pattern 1: stock_realtime (Streaming Data)

1. V√†o **Management** ‚Üí **Stack Management** ‚Üí **Index Patterns**
2. Click **"Create index pattern"**
3. Nh·∫≠p: `stock_realtime`
4. Ch·ªçn Time field: `@timestamp` (khuy·∫øn ngh·ªã)
   - N·∫øu b·∫°n kh√¥ng th·∫•y `@timestamp` th√¨ c√≥ th·ªÉ ch·ªçn t·∫°m `window_start` (n·∫øu c√≥)
5. Click **"Create index pattern"**

N·∫øu **kh√¥ng ch·ªçn ƒë∆∞·ª£c Time field** (kh√¥ng th·∫•y `@timestamp`/`window_start` trong dropdown):

1. Ki·ªÉm tra Elasticsearch c√≥ nh·∫≠n ƒë√∫ng ki·ªÉu `date` ch∆∞a:

```bash
curl -s "http://localhost:9200/stock_realtime/_mapping?pretty" | head -200
```

2. N·∫øu b·∫°n th·∫•y `@timestamp` ƒëang b·ªã map sai (v√≠ d·ª• `long`), h√£y reset index `stock_realtime` v√† t·∫°o l·∫°i mapping chu·∫©n (sau ƒë√≥ restart Spark Streaming ƒë·ªÉ b·∫Øn d·ªØ li·ªáu l·∫°i):

```bash
# Stop Spark ƒë·ªÉ tr√°nh ghi mapping sai l·∫°i ngay l·∫≠p t·ª©c
docker compose stop spark-streaming-simple

# X√≥a index c≈©
curl -X DELETE "http://localhost:9200/stock_realtime"

# T·∫°o l·∫°i index v·ªõi mapping c√≥ Time field ki·ªÉu date
curl -X PUT "http://localhost:9200/stock_realtime" ^
 -H "Content-Type: application/json" ^
 -d "{\"mappings\":{\"properties\":{\"@timestamp\":{\"type\":\"date\"},\"window_start\":{\"type\":\"date\"},\"window_end\":{\"type\":\"date\"},\"processed_time\":{\"type\":\"date\"},\"ticker\":{\"type\":\"keyword\"},\"company\":{\"type\":\"keyword\"},\"avg_price\":{\"type\":\"double\"},\"min_price\":{\"type\":\"double\"},\"max_price\":{\"type\":\"double\"},\"price_volatility\":{\"type\":\"double\"},\"total_volume\":{\"type\":\"long\"},\"trade_count\":{\"type\":\"long\"}}}}"

# Start l·∫°i Spark Streaming
docker compose start spark-streaming-simple

# ƒê·ª£i 1-2 ph√∫t r·ªìi ki·ªÉm tra l·∫°i docs
sleep 90
curl -s "http://localhost:9200/stock_realtime/_count?pretty"
```

3. Quay l·∫°i Kibana:

- Refresh trang Kibana (F5)
- N·∫øu ƒë√£ t·∫°o index pattern r·ªìi: v√†o Index Pattern ‚Üí **Refresh field list** (ho·∫∑c x√≥a v√† t·∫°o l·∫°i)

#### Index Pattern 2: batch-features (Batch Data)

1. Click **"Create index pattern"**

#### Index Pattern 3: stock_anomalies (Price Anomaly Detection)

1. Click **"Create index pattern"** l·∫ßn n·ªØa
2. Nh·∫≠p: `stock_anomalies`
3. Ch·ªçn Time field: `@timestamp`
4. Click **"Create index pattern"**

**L∆∞u √Ω:** N·∫øu index `stock_anomalies` ch∆∞a t·ªìn t·∫°i (ch∆∞a c√≥ anomaly n√†o), h√£y ch·ªù v√†i ph√∫t ho·∫∑c quay l·∫°i sau khi h·ªá th·ªëng ph√°t hi·ªán b·∫•t th∆∞·ªùng ƒë·∫ßu ti√™n. l·∫ßn n·ªØa 2. Nh·∫≠p: `batch-features` 3. Ch·ªçn Time field: `@timestamp` (ho·∫∑c `time`) 4. Click **"Create index pattern"**

### B∆∞·ªõc 3: Xem d·ªØ li·ªáu Real-time

1. V√†o **Analytics** ‚Üí **Discover**
2. Ch·ªçn index pattern: `stock_realtime`
3. Ch·ªçn time range: **Last 15 minutes**
4. Refresh t·ª± ƒë·ªông: **10 seconds**

**C√°c tr∆∞·ªùng trong stock_realtime:**

- `window_start`, `window_end`: Th·ªùi gian window
- `ticker`, `company`: M√£ c·ªï phi·∫øu, t√™n c√¥ng ty
- `avg_price`: Gi√° trung b√¨nh trong window
- `min_price`, `max_price`: Gi√° th·∫•p/cao nh·∫•t
- `total_volume`: T·ªïng kh·ªëi l∆∞·ª£ng giao d·ªãch
- `trade_count`: S·ªë l∆∞·ª£ng trades
- `price_volatility`: ƒê·ªô bi·∫øn ƒë·ªông (stddev)

### B∆∞·ªõc 4: Xem d·ªØ li·ªáu Batch Features

1. Ch·ªçn index pattern: `batch-features`
2. Ch·ªçn time range: \*_Last 7 days_

### B∆∞·ªõc 5: Xem d·ªØ li·ªáu Price Anomalies

1. Ch·ªçn index pattern: `stock_anomalies`
2. Ch·ªçn time range: **Last 15 minutes** ho·∫∑c **Last 1 hour**
3. Refresh t·ª± ƒë·ªông: **10 seconds**

**C√°c tr∆∞·ªùng trong stock_anomalies:**

**Th√¥ng tin c∆° b·∫£n:**

- `window_start`, `window_end`: Th·ªùi gian window ph√°t hi·ªán
- `ticker`, `company`: M√£ c·ªï phi·∫øu, t√™n c√¥ng ty
- `avg_price`, `min_price`, `max_price`: Gi√° trong window
- `total_volume`, `trade_count`: Volume v√† s·ªë l∆∞·ª£ng trades

**D·ªØ li·ªáu so s√°nh:**

- `historical_avg_price`: Gi√° trung b√¨nh l·ªãch s·ª≠ (5 windows tr∆∞·ªõc)
- `hiAnomaly Alerts (stock_anomalies)\*\*

- **Alert Table**: Hi·ªÉn th·ªã anomalies g·∫ßn nh·∫•t
  - Columns: ticker, window_start, anomaly_types, anomaly_severity, price_change_pct
  - Sort: By @timestamp descending
  - Filter: Last 1 hour
- **Heat Map**: Anomaly severity by ticker and time
  - X-axis: window_start
  - Y-axis: ticker
  - Color: anomaly_severity
- **Bar Chart**: Anomaly types distribution
  - X-axis: anomaly_types
  - Y-axis: Count
  - Filter: Last 24 hours
- **Line Chart**: Price change % over time (only anomalies)
  - X-axis: window_start
  - Y-axis: price_change_pct
  - Split by: ticker
  - Threshold line: 5% (anomaly threshold)
- **Gauge**: Current anomalies count
  - Metric: Count of documents
  - Time range: Last 15 minutes
  - Color ranges: 0-green, 1-5 yellow, >5 red

\*\*3. storical_avg_volume`: Volume trung b√¨nh l·ªãch s·ª≠

- `historical_volatility`: Volatility trung b√¨nh l·ªãch s·ª≠

**Ch·ªâ s·ªë b·∫•t th∆∞·ªùng:**

- `price_change_p (metrics)
  docker logs spark-streaming-simple -f --tail 50

# Spark Anomaly Detection

docker logs spark-anomaly-detection volume (v√≠ d·ª•: 3.5 = tƒÉng 350%)

- `volatility_ratio`: T·ª∑ l·ªá volatility so v·ªõi l·ªãch s·ª≠
- `price_gap_pct`: % kho·∫£ng c√°ch gi·ªØa max v√† min

**C·ªù ph√°t hi·ªán:**

- `is_price_spike`: Gi√° tƒÉng/gi·∫£m ƒë·ªôt ng·ªôt >5%
- `is_volume_spike`: Volume tƒÉng v·ªçt >3x
- `is_high_volatility`: Volatility cao >3%
- `is_price_gap`: Kho·∫£ng c√°ch gi√° l·ªõn >2%
- `anomaly_severity`: ƒê·ªô nghi√™m tr·ªçng (0-4)
- `anomaly_types`: Lo·∫°i b·∫•t th∆∞·ªùng (PRICE_SPIKE, VOLUME_SPIKE, HIGH_VOLATILITY, PRICE_GAP)\* ho·∫∑c **Last 30 days**

**C√°c tr∆∞·ªùng trong batch-features:**

- `ticker`, `time`, `Open`, `High`, `Low`, `Close`, `Volume`: D·ªØ li·ªáu OHLCV
- `ma50`, `ma100`, `ma200`: Moving averages
- `trend`: up/down/sideway
- `trend_strength`: ƒê·ªô m·∫°nh xu h∆∞·ªõng
- `cumulative_return`: T·ª∑ su·∫•t sinh l·ª£i t√≠ch l≈©y
- `drawdown`, `max_drawdown`: S·ª•t gi·∫£m t·ª´ ƒë·ªânh
- `volume_ma20`, `volume_ratio`: Volume metrics
- `monthly_volatility`: Volatility theo th√°ng
- `market_regime`: normal/high_vol

### B∆∞·ªõc 5: T·∫°o Visualizations

V√†o **Analytics** ‚Üí **Visualize Library** ‚Üí **Create visualization**

**Dashboard ƒë·ªÅ xu·∫•t:**

**1. Real-time Monitoring (stock_realtime)**

- Line chart: `avg_price` theo `window_start` (split by ticker)
- Area chart: `total_volume` theo th·ªùi gian
- Metric: Current `price_volatility`
- Gauge: `trade_count` (last 5 minutes)

**2. Batch Analysis (batch-features)**

- Line chart: `Close` price v·ªõi `ma50`, `ma100`, `ma200`
- Line chart: `cumulative_return` theo ticker
- Area chart: `drawdown` (negative chart)
- Bar chart: `trend` distribution
- Heat map: `monthly_volatility` by ticker v√† month

---

## üîß QU·∫¢N L√ù V√Ä MONITORING

## üîß QU·∫¢N L√ù V√Ä MONITORING

### Ki·ªÉm tra logs c√°c services

Ki·ªÉm tra anomaly detection c√≥ ho·∫°t ƒë·ªông
curl "http://localhost:9200/stock_anomalies/\_count?pretty"

#

```bash
# T·∫•t c·∫£ services
docker compose logs -f

# Kafka Producer
docker exec python-worker tail -f /tmp/producer.log

# Spark Streaming
docker logs spark-streaming-simple -f --tail 50

# Batch Processing (n·∫øu ch·∫°y daemon)
docker exec python-worker tail -f /logs/unified_runner.log

# Elasticsearch
docker logs elasticsearch --tail 50

# Kafka
docker logs kafka --tail 50
```

### Ki·ªÉm tra health c·ªßa c√°c services

```bash
# System health check (t·ª± ƒë·ªông)
docker exec python-worker python /app/unified_runner.py monitor

# Elasticsearch
curl "http://localhost:9200/_cluster/health?pretty"
curl "http://localhost:9200/_cat/indices?v"

# HDFS
docker exec hadoop-namenode hdfs dfs -ls /
docker exec hadoop-namenode hdfs dfs -ls /serving
docker exec hadoop-namenode hdfs dfs -ls /user/kafka_data/stocks_history

# Kafka topics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic stocks-history

# Kibana
curl -s "http://localhost:5601/api/status" | grep -o '"state":"[^"]*"'
```

### Restart c√°c services

```bash
# Restart Producer
docker exeSpark Anomaly Detection
docker compose restart spark-anomaly-detection

# Restart c python-worker pkill -f kafka_producer.py
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"

# Restart Spark Streaming
docker compose restart spark-streaming-simple

# Restart Elasticsearch
docker compose restart elasticsearch

# Restart t·∫•t c·∫£
docker compose restart
```

---

## üõë D·ª™NG V√Ä RESET H·ªÜ TH·ªêNG

### D·ª´ng to√†n b·ªô h·ªá th·ªëng

```bash
docker compose down
```

### Reset ho√†n to√†n (x√≥a d·ªØ li·ªáu c≈©)

```bash
# Stop t·∫•t c·∫£
docker compose down

# X√≥a volumes
docker volume rm big_data_hdfs-namenode big_data_hdfs-datanode big_data_spark-ivy-cache 2>/dev/null || true

# X√≥a Spark checkpoints
docker compose up -d hadoop-namenode
sleep 15
docker exec hadoop-namenode hdfs dfs -rm -r /user/spark_checkpoints/* 2>/dev/null || true
docker compose stop hadoop-namenode

# Start l·∫°i t·ª´ ƒë·∫ßu
docker compose down
docker compose up -d
```

stock_anomalies"
curl -X DELETE "http://localhost:9200/batch-features"

# Ho·∫∑c x√≥a t·∫•t c·∫£ indexes

curl -X DELETE "http://localhost:9200/\*"

# Restart Spark Streaming ƒë·ªÉ t·∫°o l·∫°i indexes

docker compose restart spark-streaming-simple spark-anomaly-detectioneatures"

# Ho·∫∑c x√≥a t·∫•t c·∫£ indexes

curl -X DELETE "http://localhost:9200/\*"

# Restart Spark Streaming ƒë·ªÉ t·∫°o l·∫°i index

docker compose restart spark-streaming-simple

````

---

## üîç TROUBLESHOOTING

## üîç TROUBLESHOOTING

### 1. Index kh√¥ng xu·∫•t hi·ªán trong Elasticsearch

**Ki·ªÉm tra Producer:**

```bash
docker exec python-worker ps aux | grep kafka_producer
docker exec python-worker tail -30 /tmp/producer.log
````

**N·∫øu kh√¥ng ch·∫°y, restart:**

```bash
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"
```

**Ki·ªÉm tra Kafka c√≥ nh·∫≠n messages:**

```bash
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic stocks-history \
  --from-beginning \
  --max-messages 5
```

### 2. Spark Streaming c√≥ l·ªói

**Xem logs chi ti·∫øt:**

```bash
docker logs spark-streaming-simple --tail 100
```

**L·ªói th∆∞·ªùng g·∫∑p:**

**a) NoSuchMethodError / Compatibility issues**

```bash
# X√≥a checkpoint v√† restart
docker exec hadoop-namenode hdfs dfs -rm -r /user/spark_checkpoints/stock_realtime
docker compose restart spark-streaming-simple
```

**b) Elasticsearch connection refused**

```bash
# Ki·ªÉm tra ES ƒëang ch·∫°y
curl "http://localhost:9200/_cluster/health"

# Restart ES n·∫øu c·∫ßn
docker compose restart elasticsearch
sleep 30
docker compose restart spark-streaming-simple
```

**c) Kafka connection timeout**

```bash
# Ki·ªÉm tra Kafka
docker logs kafka --tail 50
 spark-anomaly-detection
# Restart Kafka
docker compose restart zookeeper kafka
sleep 30
docker compose restart spark-streaming-simple
```

### 3. Batch Processing th·∫•t b·∫°i

**Ki·ªÉm tra l·ªói:**

```bash
docker exec python-worker python /app/unified_runner.py batch
```

**L·ªói th∆∞·ªùng g·∫∑p:**

**a) HDFS connection failed**

```bash
# Ki·ªÉm tra HDFS
docker logs hadoop-namenode --tail 50
curl "http://localhost:9870"

# Restart HDFS
docker compose restart hadoop-namenode hadoop-datanode
```

**b) Elasticsearch indexing failed**

```bash
# Ki·ªÉm tra ES health
curl "http://localhost:9200/_cluster/health?pretty"

# Ki·ªÉm tra disk space
docker exec elasticsearch df -h
```

**c) Memory error (OOM)**

```bash
# Gi·∫£m batch size trong run_all.py
# Ho·∫∑c tƒÉng memory cho python-worker trong docker-compose.yml
```

### 4. Kibana kh√¥ng hi·ªÉn th·ªã d·ªØ li·ªáu

**Ki·ªÉm tra Elasticsearch c√≥ d·ªØ li·ªáu:**

```bash
curl "http://localhost:9200/stock_anomalies/_count?pretty"
curl "http://localhost:9200/batch-features/_count?pretty"
```

**N·∫øu count > 0 nh∆∞ng Kibana kh√¥ng th·∫•y:**

- Refresh trang Kibana (F5)
- Ki·ªÉm tra Time Range (ch·ªçn r·ªông h∆°n: Last 24 hours)
- X√≥a Index Pattern v√† t·∫°o l·∫°i
- Clear browser cache

**Ki·ªÉm tra Kibana logs:**

```bash
docker logs kibana --tail 50
```

**L∆∞u √Ω v·ªÅ stock_anomalies:**

- Index n√†y ch·ªâ c√≥ data khi ph√°t hi·ªán anomaly
- N·∫øu gi√° kh√¥ng bi·∫øn ƒë·ªông b·∫•t th∆∞·ªùng, index c√≥ th·ªÉ r·ªóng
- Th·ª≠ tƒÉng volatility trong price_simulator.py ƒë·ªÉ t·∫°o anomalies testker logs kibana --tail 50

````

### 5. Data kh√¥ng update trong Kibana

**Ki·ªÉm tra th·ªùi gian:**

```bash
# So s√°nh th·ªùi gian h·ªá th·ªëng v·ªõi d·ªØ li·ªáu
date
curl "http://localhost:9200/stock_realtime/_search?pretty" | grep window_start | head -5
````

**N·∫øu time mismatch:**

- Adjust time range trong Kibana
- Ho·∫∑c sync th·ªùi gian containers v·ªõi host

### 6. Performance issues

**a) Streaming lag:**

```bash
# Check Spark UI
# M·ªü http://localhost:18080 (n·∫øu history server ƒë∆∞·ª£c enable)
# Ho·∫∑c check logs
docker logs spark-streaming-simple | grep "Batch"
```

**b) Kafka lag:**

```bash
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe \
  --all-groups
```

**c) Elasticsearch slow:**

```bash
curl "http://localhost:9200/_cat/thread_pool?v"
curl "http://localhost:9200/_nodes/stats/indices?pretty"
```

---

## üìà C·∫§U TR√öC D·ªÆ LI·ªÜU CHI TI·∫æT

### Kafka Message Schema (stocks-history topic)

```json
{
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "time": "2025-01-05T10:30:00+00:00",
  "Open": 280.5,
  "High": 282.1,
  "Low": 278.2,
  "Close": 281.0,
  "Adj Close": 281.0,
  "Volume": 45123456
}
```

### Elasticsearch Index: stock_realtime

**Mapping:**

```json
{
  "window_start": "2025-01-05T10:00:00Z",
  "window_end": "2025-01-05T10:00:30Z",
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "avg_price": 280.75,
  "min_price": 278.2,
  "max_price": 282.1,
  "total_volume": 135370368,
  "trade_count": 3,
  "price_volatility": 1.42,
  "processed_time": "2025-01-05T10:01:05Z"
}
```

**Gi·∫£i th√≠ch:**

- `window_start/end`: 30s time window

### Elasticsearch Index: stock_anomalies

**Mapping:**

```json
{
  "@timestamp": "2025-01-05T10:00:00Z",
  "window_start": "2025-01-05T10:00:00Z",
  "window_end": "2025-01-05T10:00:30Z",
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "avg_price": 295.5,
  "min_price": 293.2,
  "max_price": 298.1,
  "total_volume": 180000000,
  "trade_count": 3,
  "price_volatility": 2.15,
  "historical_avg_price": 280.75,
  "historical_avg_volume": 45000000,
  "historical_volatility": 0.85,
  "price_change_pct": 0.0525,
  "price_change_abs": 0.0525,
  "volume_spike_ratio": 4.0,
  "volatility_ratio": 2.53,
  "price_gap_pct": 0.0165,
  "is_price_spike_up": true,
  "is_price_spike_down": false,
  "is_volume_spike": true,
  "is_high_volatility": false,
  "is_price_gap": false,
  "anomaly_severity": 2,
  "anomaly_types": "PRICE_SPIKE_UP,VOLUME_SPIKE",
  "detected_time": "2025-01-05T10:01:08Z"
}
```

**Gi·∫£i th√≠ch:**

- **Th√¥ng tin window**: window_start/end, ticker, company, gi√°/volume trong window
- **Historical baseline**: historical_avg_price/volume/volatility (5 windows tr∆∞·ªõc)
- **Anomaly metrics**:
  - `price_change_pct`: 0.0525 = tƒÉng **+5.25%** so v·ªõi l·ªãch s·ª≠ (c√≥ d·∫•u: +tƒÉng/-gi·∫£m)
  - `price_change_abs`: 0.0525 = absolute value ƒë·ªÉ so s√°nh threshold
  - `volume_spike_ratio`: 4.0 = volume g·∫•p 4 l·∫ßn trung b√¨nh
  - `volatility_ratio`: 2.53 = volatility g·∫•p 2.53 l·∫ßn th∆∞·ªùng
  - `price_gap_pct`: 0.0165 = ch√™nh l·ªách max-min l√† 1.65%
- **Detection flags**:
  - `is_price_spike_up: true` - Ph√°t hi·ªán tƒÉng gi√° >5%
  - `is_price_spike_down: false` - Kh√¥ng gi·∫£m >5%
  - `is_volume_spike: true` - Volume spike detected
  - `is_high_volatility: false` - Volatility b√¨nh th∆∞·ªùng
  - `is_price_gap: false` - Kh√¥ng c√≥ gap l·ªõn
- `anomaly_severity`: 0-5 (s·ªë lo·∫°i anomaly ph√°t hi·ªán, t·ªëi ƒëa 5)
- `anomaly_types`: "PRICE_SPIKE_UP,VOLUME_SPIKE" (danh s√°ch CSV)
- `avg_price`: Average Close price trong window
- `min_price`: Min Low price trong window
- `max_price`: Max High price trong window
- `total_volume`: Sum Volume trong window
- `trade_count`: S·ªë messages trong window
- `price_volatility`: Standard deviation c·ªßa Close price

### Elasticsearch Index: batch-features

**Mapping:**

```json
{
  "@timestamp": "2025-01-05T10:30:00Z",
  "ticker": "AAPL",
  "time": "2025-01-05T10:30:00Z",
  "Open": 280.5,
  "High": 282.1,
  "Low": 278.2,
  "Close": 281.0,
  "Volume": 45123456,

  "ma50": 275.3,
  "ma100": 270.45,
  "ma200": 265.8,
  "trend": "up",
  "trend_strength": 0.0234,

  "cumulative_return": 0.1523,
  "drawdown": -0.0234,
  "max_drawdown": -0.0812,

  "volume_ma20": 42000000,
  "volume_ratio": 1.074,

  "month": "2025-01",
  "monthly_volatility": 0.0245,
  "market_regime": "normal"
}
```

**Gi·∫£i th√≠ch:**

- `ma50/100/200`: Moving averages (50, 100, 200 days)
- `trend`: up (ma50 > ma200), down, sideway
- `trend_strength`: (ma50 - ma200) / Close
- `cumulative_return`: T·ª∑ su·∫•t sinh l·ª£i t√≠ch l≈©y
- `drawdown`: % s·ª•t gi·∫£m t·ª´ ƒë·ªânh g·∫ßn nh·∫•t
- `max_drawdown`: Drawdown t·ªëi ƒëa trong l·ªãch s·ª≠
- `volume_ma20`: Volume trung b√¨nh 20 ng√†y
- `volume_ratio`: Volume hi·ªán t·∫°i / volume_ma20
- `monthly_volatility`: Volatility theo th√°ng
- `market_regime`: normal ho·∫∑c high_vol

---

## ‚öôÔ∏è C·∫§U H√åNH H·ªÜ TH·ªêNG

### Kafka Producer (kafka_producer.py)

```python
KAFKA_TOPIC = "stocks-history"      # Topic duy nh·∫•t
UPDATE_INTERVAL = 30                # 30 gi√¢y/batch
TICKERS = ["AAPL", "NVDA"]          # Danh s√°ch c·ªï phi·∫øu
```

Thay ƒë·ªïi tickers:

```bash
# Trong docker-compose.yml
environment:
  - TICKERS=AAPL,NVDA,TSLA,MSFT
```

### Spark Streaming (spark_streaming_simple.py)

```python
WINDOW_DURATION = "30 seconds"      # Time window
WATERMARK_DELAY = "1 minute"        # Late data tolerance
TRIGGER_INTERVAL = "30 seconds"     # Processing trigger
```

### Batch Processing (run_all.py)

```python
# C√°c batch jobs ƒë∆∞·ª£c ch·∫°y:
- batch_long_term_trend()          # MA50, MA100, MA200
- batch_cumulative_return()         # Cumulative return
- batch_drawdown()                  # Drawdown, max drawdown
- batch_volume_features()           # Volume MA, ratio
- batch_monthly_volatility()        # Monthly volatility
- batch_market_regime()             # Market regime classification
```

### Unified Runner (unified_runner.py)

```bash
# Environment variables
RUN_MODE=all                        # all, batch, monitor
BATCH_INTERVAL_HOURS=24             # Batch interval

# Usage
python unified_runner.py            # Continuous mode
python unified_runner.py batch      # Run batch once
python unified_runner.py monitor    # Check health once
```

---

## üìû T·ªîNG K·∫æT V√Ä H·ªñ TR·ª¢

### Quick Commands Cheat Sheet

```bash
# Start h·ªá th·ªëng
docker compose up -d
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"

# Check status
docker compose ps
curl "http://localhost:9200/_cat/indices?v"
docker logs spark-streaming-simple --tail 20
docker logs spark-anomaly-detection --tail 20

# Run batch
docker exec python-worker python /app/unified_runner.py batch

# View logs
docker exec python-worker tail -f /tmp/producer.log
docker logs spark-streaming-simple -f
docker logs spark-anomaly-detection -f

# Health check
docker exec python-worker python /app/unified_runner.py monitor

# Check anomalies
curl "http://localhost:9200/stock_anomalies/_search?pretty&size=5&sort=@timestamp:desc"

# Reset
docker compose down
docker volume prune -f
docker compose up -d
```

### Architecture Summary

```
DATA FLOW:
1. history.json ‚Üí Kafka Producer ‚Üí stocks-history topic
2a. stocks-history ‚Üí Kafka Consumer ‚Üí HDFS (raw storage)
2b. stocks-history ‚Üí Spark Streaming ‚Üí Elasticsearch (stock_realtime)
2c. stocks-history ‚Üí Spark Anomaly Detection ‚Üí Elasticsearch (stock_anomalies)
3. history.json ‚Üí Batch Processing ‚Üí HDFS + Elasticsearch (batch-features)

UNIFIED SCHEMA: ticker, company, time, Open, High, Low, Close, Adj Close, Volume
SINGLE TOPIC: stocks-history
INDEXES:
  - stock_realtime (streaming metrics)
  - stock_anomalies (price anomaly alerts)
  - batch-features (batch features)
```

### Ports Reference

- **9200**: Elasticsearch REST API
- **5601**: Kibana UI
- **4040**: Spark UI (streaming jobs, n·∫øu expose)

### Monitoring URLs

- Kibana: http://localhost:5601
- Elasticsearch: http://localhost:9200
- HDFS: http://localhost:9870
- Spark Streaming UI: http://localhost:4040 (n·∫øu port-forward)

### Key Elasticsearch Queries

```bash
# Count documents per index
curl "http://localhost:9200/stock_realtime/_count?pretty"
curl "http://localhost:9200/stock_anomalies/_count?pretty"
curl "http://localhost:9200/batch-features/_count?pretty"

# Get latest anomalies
curl "http://localhost:9200/stock_anomalies/_search?pretty&size=5&sort=@timestamp:desc"

# Get anomalies for specific ticker
curl -X GET "http://localhost:9200/stock_anomalies/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {
    "bool": {
      "must": [
        {"term": {"ticker": "AAPL"}},
        {"range": {"@timestamp": {"gte": "now-1h"}}}
      ]
    }
  },
  "sort": [{"@timestamp": "desc"}],
  "size": 10
}
'

# Get high severity anomalies only
curl -X GET "http://localhost:9200/stock_anomalies/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {
    "range": {"anomaly_severity": {"gte": 2}}
  },
  "sort": [{"@timestamp": "desc"}],
  "size": 20
}
'
```

- Kibana: http://localhost:5601
- Elasticsearch: http://localhost:9200
- HDFS: http://localhost:9870

---

**üéâ H·ªÜ TH·ªêNG ƒê√É ƒê∆Ø·ª¢C TH·ªêNG NH·∫§T V√Ä S·∫¥N S√ÄNG HO·∫†T ƒê·ªòNG!**

_D·ª± √°n merge th√†nh c√¥ng batch processing v√† real-time streaming v·ªõi schema nh·∫•t qu√°n._

---

## üîî PRICE ANOMALY DETECTION - PH√ÅT HI·ªÜN B·∫§T TH∆Ø·ªúNG GI√Å

### T·ªïng quan

H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c b·ªï sung **Real-time Price Anomaly Detection** - m·ªôt Speed Layer job chuy√™n ph√°t hi·ªán c√°c b·∫•t th∆∞·ªùng v·ªÅ gi√° ch·ª©ng kho√°n trong th·ªùi gian th·ª±c.

### C√°c lo·∫°i anomaly ƒë∆∞·ª£c ph√°t hi·ªán

**1. PRICE_SPIKE_UP - TƒÉng gi√° ƒë·ªôt ng·ªôt**

- **Ng∆∞·ª°ng**: TƒÉng >5% trong 30 gi√¢y (so v·ªõi 5 windows tr∆∞·ªõc)
- **V√≠ d·ª•**: AAPL t·ª´ $280 ‚Üí $295 trong 30s (tƒÉng +5.4%)
- **Use case**: Tin t·ª©c t√≠ch c·ª±c, insider buying, pump schemes
- **Flag**: `is_price_spike_up: true`

**2. PRICE_SPIKE_DOWN - Gi·∫£m gi√° ƒë·ªôt ng·ªôt**

- **Ng∆∞·ª°ng**: Gi·∫£m >5% trong 30 gi√¢y (so v·ªõi 5 windows tr∆∞·ªõc)
- **V√≠ d·ª•**: AAPL t·ª´ $280 ‚Üí $265 trong 30s (gi·∫£m -5.4%)
- **Use case**: Tin t·ª©c ti√™u c·ª±c, insider selling, dump schemes
- **Flag**: `is_price_spike_down: true`

**3. VOLUME_SPIKE - Kh·ªëi l∆∞·ª£ng giao d·ªãch tƒÉng v·ªçt**

- **Ng∆∞·ª°ng**: Volume g·∫•p >3x trung b√¨nh (5 windows tr∆∞·ªõc)
- **V√≠ d·ª•**: Volume trung b√¨nh 45M ‚Üí ƒë·ªôt ng·ªôt 180M
- **Use case**: Institutional buying/selling, market events
- **Flag**: `is_volume_spike: true`

**4. HIGH_VOLATILITY - ƒê·ªô bi·∫øn ƒë·ªông cao**

- **Ng∆∞·ª°ng**: Standard deviation >3% (so v·ªõi 5 windows tr∆∞·ªõc)
- **V√≠ d·ª•**: Gi√° dao ƒë·ªông m·∫°nh trong c√πng m·ªôt window
- **Use case**: Market uncertainty, earnings announcements
- **Flag**: `is_high_volatility: true`

**5. PRICE_GAP - Kho·∫£ng c√°ch gi√° l·ªõn**

- **Ng∆∞·ª°ng**: (Max - Min) / Avg > 2% trong window
- **V√≠ d·ª•**: Trong 30s, gi√° t·ª´ $280 ‚Üí $286 ‚Üí $282 (gap 2.1%)
- **Use case**: Flash crashes, liquidity issues
- **Flag**: `is_price_gap: true`

### C√°ch ho·∫°t ƒë·ªông

```
Kafka Stream ‚Üí 30s Window Aggregation
    ‚Üì
Compare v·ªõi Historical Baseline (5 windows tr∆∞·ªõc)
    ‚Üì
T√≠nh to√°n c√°c ch·ªâ s·ªë:
  - price_change_pct (% thay ƒë·ªïi so v·ªõi l·ªãch s·ª≠)
  - volume_spike_ratio (t·ª∑ l·ªá volume/avg)
  - volatility_ratio (volatility/avg)
  - price_gap_pct (% ch√™nh l·ªách max-min)
    ‚Üì
So s√°nh v·ªõi Thresholds
    ‚Üì
N·∫øu v∆∞·ª£t ng∆∞·ª°ng ‚Üí Ghi v√†o Elasticsearch (stock_anomalies)
    ‚Üì
Alert tr√™n Kibana Dashboard
```

### Xem anomalies tr√™n Kibana

**T·∫°o Dashboard:**

1. **Data Table - Recent Anomalies**

   - Index: `stock_anomalies`
   - Columns: ticker, window_start, anomaly_types, anomaly_severity, price_change_pct
   - Time range: Last 1 hour
   - Sort: @timestamp descending

2. **Metric - Anomaly Count**

   - Count of anomalies in last 15 minutes
   - Color thresholds: 0 (green), 1-5 (yellow), >5 (red)

3. **Bar Chart - Anomaly Types**

   - X-axis: anomaly_types
   - Y-axis: Count
   - Shows which anomaly type is most common

4. **Line Chart - Price Change Over Time**
   - X-axis: @timestamp
   - Y-axis: price_change_pct
   - Split by: ticker
   - Add threshold line at 5%

### C·∫•u h√¨nh Thresholds

C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh trong `spark_anomaly_detection.py`:

```python
# Thresholds hi·ªán t·∫°i (cho 30s windows v·ªõi simulated data)
PRICE_CHANGE_THRESHOLD = 0.05  # 5%
VOLUME_SPIKE_THRESHOLD = 3.0    # 3x
VOLATILITY_THRESHOLD = 0.03     # 3%
PRICE_GAP_THRESHOLD = 0.02      # 2%
```

**Tuning tips cho c√°c lo·∫°i th·ªã tr∆∞·ªùng:**

- **Th·ªã tr∆∞·ªùng bi·∫øn ƒë·ªông cao** (crypto, penny stocks):
  - TƒÉng thresholds: 7-10%, 5x, 5%, 3%
  - Tr√°nh qu√° nhi·ªÅu false positives
- **Th·ªã tr∆∞·ªùng ·ªïn ƒë·ªãnh** (blue chips, bonds):
  - Gi·∫£m thresholds: 3%, 2x, 2%, 1%
  - Detect ƒë∆∞·ª£c c·∫£ bi·∫øn ƒë·ªông nh·ªè
- **Testing**:
  - Gi·∫£m thresholds ƒë·ªÉ th·∫•y nhi·ªÅu anomalies h∆°n
  - Monitor false positive rate

**‚ö†Ô∏è QUAN TR·ªåNG: Khi crawl d·ªØ li·ªáu theo ph√∫t**

Khi b·∫°n chuy·ªÉn sang crawl th·ª±c t·∫ø (1 data point/ph√∫t thay v√¨ m√¥ ph·ªèng 30s):

**B∆∞·ªõc 1: ƒêi·ªÅu ch·ªânh Window Duration**

```python
# Trong spark_anomaly_detection.py
WINDOW_DURATION = "1 minute"  # ho·∫∑c "2 minutes"
WATERMARK_DELAY = "2 minutes"
TRIGGER_INTERVAL = "1 minute"
```

**B∆∞·ªõc 2: ƒêi·ªÅu ch·ªânh Thresholds**

```python
# Cho 1-minute windows v·ªõi real crawl data
PRICE_CHANGE_THRESHOLD = 0.03  # 3% (ch·∫∑t h∆°n, data chi ti·∫øt h∆°n)
VOLUME_SPIKE_THRESHOLD = 2.5    # 2.5x (t√πy market)
VOLATILITY_THRESHOLD = 0.02     # 2% (ƒëi·ªÅu ch·ªânh theo reality)
PRICE_GAP_THRESHOLD = 0.015     # 1.5% (ch·∫∑t h∆°n)
```

**L√Ω do ƒëi·ªÅu ch·ªânh:**

- ‚úÖ **Data theo ph√∫t chi ti·∫øt h∆°n** ‚Üí C√≥ th·ªÉ detect anomaly nh·ªè h∆°n
- ‚úÖ **√çt aggregation** ‚Üí Price change th·ª±c t·∫ø h∆°n
- ‚úÖ **Tr√°nh false positives** ‚Üí Volatility intraday b√¨nh th∆∞·ªùng kh√¥ng trigger
- ‚úÖ **Baseline ch√≠nh x√°c h∆°n** ‚Üí 5 windows = 5 ph√∫t history (ƒë·ªß ƒë·ªÉ compare)

**B∆∞·ªõc 3: Test v√† Monitor**

```bash
# Sau khi deploy v·ªõi real crawl:
# 1. Monitor s·ªë l∆∞·ª£ng anomalies
curl "http://localhost:9200/stock_anomalies/_count?pretty"

# 2. Xem anomalies sample
curl "http://localhost:9200/stock_anomalies/_search?size=10&sort=@timestamp:desc&pretty"

# 3. Check false positive rate
# N·∫øu qu√° nhi·ªÅu alerts ‚Üí TƒÉng thresholds
# N·∫øu miss important events ‚Üí Gi·∫£m thresholds
```

**V√≠ d·ª• th·ª±c t·∫ø:**

Gi·∫£ s·ª≠ crawl AAPL m·ªói ph√∫t, gi√° dao ƒë·ªông b√¨nh th∆∞·ªùng 0.5-1%:

- Threshold 5% ‚Üí Ch·ªâ catch crash/pump th·∫≠t s·ª±
- Threshold 3% ‚Üí Catch c·∫£ unusual moves
- Threshold 1% ‚Üí Qu√° nhi·ªÅu false positives

**Khuy·∫øn ngh·ªã:**

- B·∫Øt ƒë·∫ßu v·ªõi 3% cho PRICE_CHANGE_THRESHOLD
- Monitor trong 1-2 ng√†y
- ƒêi·ªÅu ch·ªânh d·ª±a tr√™n k·∫øt qu·∫£ th·ª±c t·∫ø

### Queries h·ªØu √≠ch

**L·∫•y 10 anomalies m·ªõi nh·∫•t:**

```bash
curl "http://localhost:9200/stock_anomalies/_search?pretty&size=10&sort=@timestamp:desc"
```

**L·∫•y anomalies severity cao (‚â•3):**

```bash
curl -X GET "http://localhost:9200/stock_anomalies/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {"range": {"anomaly_severity": {"gte": 3}}},
  "sort": [{"@timestamp": "desc"}]
}
'
```

**L·∫•y price spikes c·ªßa AAPL trong 1 gi·ªù qua:**

```bash
curl -X GET "http://localhost:9200/stock_anomalies/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {
    "bool": {
      "must": [
        {"term": {"ticker": "AAPL"}},
        {"term": {"is_price_spike": true}},
        {"range": {"@timestamp": {"gte": "now-1h"}}}
      ]
    }
  }
}
'
```

### Alerting (Production)

Trong production, c√≥ th·ªÉ setup alerts v·ªõi:

**Elasticsearch Watcher (X-Pack):**

```json
{
  "trigger": {
    "schedule": { "interval": "1m" }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["stock_anomalies"],
        "body": {
          "query": {
            "bool": {
              "must": [
                { "range": { "@timestamp": { "gte": "now-1m" } } },
                { "range": { "anomaly_severity": { "gte": 2 } } }
              ]
            }
          }
        }
      }
    }
  },
  "actions": {
    "email_admin": {
      "email": {
        "to": "admin@example.com",
        "subject": "Price Anomaly Alert",
        "body": "Detected {{ctx.payload.hits.total}} anomalies"
      }
    }
  }
}
```

**Ho·∫∑c Slack/Discord webhook:**

- ƒê·ªçc anomalies t·ª´ ES m·ªói ph√∫t
- N·∫øu severity ‚â• 2 ‚Üí POST ƒë·∫øn webhook
- Hi·ªÉn th·ªã alert tr√™n Slack channel

---

**üéâ H·ªÜ TH·ªêNG HO√ÄN CH·ªàNH V·ªöI ANOMALY DETECTION!**
