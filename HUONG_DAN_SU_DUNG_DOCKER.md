# H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG - BIG DATA PIPELINE TH·ªêNG NH·∫§T

**Real-time Stock Data Processing: Batch + Streaming Pipeline**

---

## üìã T·ªîNG QUAN H·ªÜ TH·ªêNG

### Ki·∫øn tr√∫c t·ªïng quan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  history.json   ‚îÇ (D·ªØ li·ªáu l·ªãch s·ª≠)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KAFKA PRODUCER                             ‚îÇ
‚îÇ  - ƒê·ªçc history.json l√†m baseline                             ‚îÇ
‚îÇ  - S·ª≠ d·ª•ng price_simulator.py ƒë·ªÉ m√¥ ph·ªèng gi√° realtime      ‚îÇ
‚îÇ  - Simulate realtime prices v·ªõi volatility ƒë·ªông              ‚îÇ
‚îÇ  - G·ª≠i v√†o topic: stocks-history                             ‚îÇ
‚îÇ  - Schema th·ªëng nh·∫•t: ticker, company, time, OHLCV           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               v
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  KAFKA BROKER ‚îÇ
       ‚îÇ stocks-history‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ       ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                     ‚îÇ
    v                     v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HDFS    ‚îÇ      ‚îÇ SPARK STREAMING‚îÇ
‚îÇ Consumer ‚îÇ      ‚îÇ   (realtime)   ‚îÇ
‚îÇ  (raw)   ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
                          v
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇElasticsearch ‚îÇ
                  ‚îÇstock_realtime‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         v
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ  KIBANA  ‚îÇ
                   ‚îÇ Dashboard‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   BATCH PROCESSING (python-worker)   ‚îÇ
‚îÇ - ƒê·ªçc history.json                   ‚îÇ
‚îÇ - T√≠nh batch features:               ‚îÇ
‚îÇ   + Trend (MA50, MA100, MA200)       ‚îÇ
‚îÇ   + Cumulative Return                ‚îÇ
‚îÇ   + Drawdown                         ‚îÇ
‚îÇ   + Volume Features                  ‚îÇ
‚îÇ   + Monthly Volatility               ‚îÇ
‚îÇ   + Market Regime                    ‚îÇ
‚îÇ - Ghi HDFS: /serving/batch_features  ‚îÇ
‚îÇ - Ghi ES: batch-features index       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Lu·ªìng d·ªØ li·ªáu th·ªëng nh·∫•t

**1. SCHEMA CHUNG (T·∫•t c·∫£ services s·ª≠ d·ª•ng)**

```json
{
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "time": "2025-01-05T10:30:00Z",
  "Open": 280.5,
  "High": 282.1,
  "Low": 278.2,
  "Close": 281.0,
  "Adj Close": 281.0,
  "Volume": 45000000
}
```

**2. KAFKA TOPIC**: `stocks-history` (topic duy nh·∫•t cho c·∫£ batch v√† streaming)

**3. ELASTICSEARCH INDEXES**:

- `stock_realtime`: Streaming metrics (30s windows)
- `batch-features`: Batch engineered features

---

## üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG

## üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG

### B∆∞·ªõc 1: Start t·∫•t c·∫£ services

```bash
cd d:\HUST\2025_1\BIGDATA\big_data
docker compose up -d
```

**Ch·ªù 30 gi√¢y** ƒë·ªÉ c√°c services kh·ªüi ƒë·ªông ho√†n t·∫•t.

### B∆∞·ªõc 2: Ki·ªÉm tra services ƒëang ch·∫°y

```bash
docker compose ps
```

Ph·∫£i th·∫•y c√°c containers:

- ‚úÖ `zookeeper`
- ‚úÖ `kafka`
- ‚úÖ `hadoop-namenode`, `hadoop-datanode`
- ‚úÖ `elasticsearch`
- ‚úÖ `kibana`
- ‚úÖ `python-worker`
- ‚úÖ `spark-streaming-simple`

### B∆∞·ªõc 3: Start Kafka Producer (g·ª≠i d·ªØ li·ªáu realtime)

```bash
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"
```

Ki·ªÉm tra Producer ƒëang ch·∫°y:

```bash
docker exec python-worker tail -20 /tmp/producer.log
```

Ph·∫£i th·∫•y:

```
[READY] 2025-01-05 10:30:00: Starting data stream...
[BATCH 1] 2025-01-05T10:30:00Z: Sent 2 messages
[BATCH 2] 2025-01-05T10:30:30Z: Sent 2 messages
```

### B∆∞·ªõc 4: Ki·ªÉm tra Spark Streaming ƒëang ch·∫°y

```bash
docker logs spark-streaming-simple --tail 30
```

Ph·∫£i th·∫•y:

```
[INFO] Starting Spark Streaming Job
[INFO] Kafka: kafka:9092, Topic: stocks-history
[INFO] Elasticsearch: elasticsearch:9200, Index: stock_realtime
[INFO] Streaming query started. Writing to stock_realtime
```

### B∆∞·ªõc 5: Ch·ªù d·ªØ li·ªáu ƒë∆∞·ª£c ghi v√†o Elasticsearch (2-3 ph√∫t)

```bash
sleep 120
curl -s "http://localhost:9200/_cat/indices?v" | grep stock
```

Ph·∫£i th·∫•y:

```
yellow open stock_realtime ... docs.count > 0
```

---

## üîÑ CH·∫†Y BATCH PROCESSING

### Ch·∫°y 1 l·∫ßn (manual)

```bash
docker exec python-worker python /app/unified_runner.py batch
```

Output:

```
============================================================
STARTING BATCH PROCESSING
============================================================
BAT ƒê·∫¶U ƒê·∫®Y D·ªÆ LI·ªÜU L√äN HDFS...
DONE: ƒê√£ l∆∞u v√†o HDFS t·∫°i /serving/batch_features.json
BAT ƒê·∫¶U ƒê·∫®Y D·ªÆ LI·ªÜU L√äN ELASTICSEARCH...
Indexed XXX documents into batch-features
DONE! H·ªÜ TH·ªêNG ƒê√É C·∫¨P NH·∫¨T C·∫¢ HDFS V√Ä ELASTICSEARCH.
‚úì Batch processing completed in XX.XXs
```

### Ch·∫°y ƒë·ªãnh k·ª≥ (daemon mode)

```bash
# Ch·∫°y batch m·ªói 24h + monitor system health m·ªói 30 ph√∫t
docker exec -d python-worker python /app/unified_runner.py

# Ho·∫∑c ch·ªâ monitor (kh√¥ng ch·∫°y batch)
docker exec -d python-worker bash -c "RUN_MODE=monitor python /app/unified_runner.py"

# Ho·∫∑c ch·ªâ ch·∫°y batch ƒë·ªãnh k·ª≥
docker exec -d python-worker bash -c "RUN_MODE=batch BATCH_INTERVAL_HOURS=12 python /app/unified_runner.py"
```

Ki·ªÉm tra:

```bash
docker exec python-worker python - <<'PY'
import os

def read_cmdline(pid: str) -> str:
  try:
    with open(f"/proc/{pid}/cmdline", "rb") as f:
      raw = f.read()
    return raw.replace(b"\x00", b" ").decode("utf-8", errors="replace").strip()
  except Exception:
    return ""

matches = []
for pid in os.listdir("/proc"):
  if pid.isdigit():
    cmd = read_cmdline(pid)
    if "unified_runner.py" in cmd:
      matches.append((pid, cmd))

if not matches:
  print("NOT_RUNNING")
else:
  for pid, cmd in sorted(matches, key=lambda x: int(x[0])):
    print(f"PID={pid} CMD={cmd}")
PY
```

---

## üìä XEM D·ªÆ LI·ªÜU TR√äN KIBANA

## üìä XEM D·ªÆ LI·ªÜU TR√äN KIBANA

### B∆∞·ªõc 1: M·ªü Kibana

```
http://localhost:5601
```

### B∆∞·ªõc 2: T·∫°o Index Patterns

#### Index Pattern 1: stock_realtime (Streaming Data)

1. V√†o **Management** ‚Üí **Stack Management** ‚Üí **Index Patterns**
2. Click **"Create index pattern"**
3. Nh·∫≠p: `stock_realtime`
4. Ch·ªçn Time field: `@timestamp` (khuy·∫øn ngh·ªã)
   - N·∫øu b·∫°n kh√¥ng th·∫•y `@timestamp` th√¨ c√≥ th·ªÉ ch·ªçn t·∫°m `window_start` (n·∫øu c√≥)
5. Click **"Create index pattern"**

N·∫øu **kh√¥ng ch·ªçn ƒë∆∞·ª£c Time field** (kh√¥ng th·∫•y `@timestamp`/`window_start` trong dropdown):

1. Ki·ªÉm tra Elasticsearch c√≥ nh·∫≠n ƒë√∫ng ki·ªÉu `date` ch∆∞a:

```bash
curl -s "http://localhost:9200/stock_realtime/_mapping?pretty" | head -200
```

2. N·∫øu b·∫°n th·∫•y `@timestamp` ƒëang b·ªã map sai (v√≠ d·ª• `long`), h√£y reset index `stock_realtime` v√† t·∫°o l·∫°i mapping chu·∫©n (sau ƒë√≥ restart Spark Streaming ƒë·ªÉ b·∫Øn d·ªØ li·ªáu l·∫°i):

```bash
# Stop Spark ƒë·ªÉ tr√°nh ghi mapping sai l·∫°i ngay l·∫≠p t·ª©c
docker compose stop spark-streaming-simple

# X√≥a index c≈©
curl -X DELETE "http://localhost:9200/stock_realtime"

# T·∫°o l·∫°i index v·ªõi mapping c√≥ Time field ki·ªÉu date
curl -X PUT "http://localhost:9200/stock_realtime" ^
 -H "Content-Type: application/json" ^
 -d "{\"mappings\":{\"properties\":{\"@timestamp\":{\"type\":\"date\"},\"window_start\":{\"type\":\"date\"},\"window_end\":{\"type\":\"date\"},\"processed_time\":{\"type\":\"date\"},\"ticker\":{\"type\":\"keyword\"},\"company\":{\"type\":\"keyword\"},\"avg_price\":{\"type\":\"double\"},\"min_price\":{\"type\":\"double\"},\"max_price\":{\"type\":\"double\"},\"price_volatility\":{\"type\":\"double\"},\"total_volume\":{\"type\":\"long\"},\"trade_count\":{\"type\":\"long\"}}}}"

# Start l·∫°i Spark Streaming
docker compose start spark-streaming-simple

# ƒê·ª£i 1-2 ph√∫t r·ªìi ki·ªÉm tra l·∫°i docs
sleep 90
curl -s "http://localhost:9200/stock_realtime/_count?pretty"
```

3. Quay l·∫°i Kibana:

- Refresh trang Kibana (F5)
- N·∫øu ƒë√£ t·∫°o index pattern r·ªìi: v√†o Index Pattern ‚Üí **Refresh field list** (ho·∫∑c x√≥a v√† t·∫°o l·∫°i)

#### Index Pattern 2: batch-features (Batch Data)

1. Click **"Create index pattern"** l·∫ßn n·ªØa
2. Nh·∫≠p: `batch-features`
3. Ch·ªçn Time field: `@timestamp` (ho·∫∑c `time`)
4. Click **"Create index pattern"**

### B∆∞·ªõc 3: Xem d·ªØ li·ªáu Real-time

1. V√†o **Analytics** ‚Üí **Discover**
2. Ch·ªçn index pattern: `stock_realtime`
3. Ch·ªçn time range: **Last 15 minutes**
4. Refresh t·ª± ƒë·ªông: **10 seconds**

**C√°c tr∆∞·ªùng trong stock_realtime:**

- `window_start`, `window_end`: Th·ªùi gian window
- `ticker`, `company`: M√£ c·ªï phi·∫øu, t√™n c√¥ng ty
- `avg_price`: Gi√° trung b√¨nh trong window
- `min_price`, `max_price`: Gi√° th·∫•p/cao nh·∫•t
- `total_volume`: T·ªïng kh·ªëi l∆∞·ª£ng giao d·ªãch
- `trade_count`: S·ªë l∆∞·ª£ng trades
- `price_volatility`: ƒê·ªô bi·∫øn ƒë·ªông (stddev)

### B∆∞·ªõc 4: Xem d·ªØ li·ªáu Batch Features

1. Ch·ªçn index pattern: `batch-features`
2. Ch·ªçn time range: **Last 7 days** ho·∫∑c **Last 30 days**

**C√°c tr∆∞·ªùng trong batch-features:**

- `ticker`, `time`, `Open`, `High`, `Low`, `Close`, `Volume`: D·ªØ li·ªáu OHLCV
- `ma50`, `ma100`, `ma200`: Moving averages
- `trend`: up/down/sideway
- `trend_strength`: ƒê·ªô m·∫°nh xu h∆∞·ªõng
- `cumulative_return`: T·ª∑ su·∫•t sinh l·ª£i t√≠ch l≈©y
- `drawdown`, `max_drawdown`: S·ª•t gi·∫£m t·ª´ ƒë·ªânh
- `volume_ma20`, `volume_ratio`: Volume metrics
- `monthly_volatility`: Volatility theo th√°ng
- `market_regime`: normal/high_vol

### B∆∞·ªõc 5: T·∫°o Visualizations

V√†o **Analytics** ‚Üí **Visualize Library** ‚Üí **Create visualization**

**Dashboard ƒë·ªÅ xu·∫•t:**

**1. Real-time Monitoring (stock_realtime)**

- Line chart: `avg_price` theo `window_start` (split by ticker)
- Area chart: `total_volume` theo th·ªùi gian
- Metric: Current `price_volatility`
- Gauge: `trade_count` (last 5 minutes)

**2. Batch Analysis (batch-features)**

- Line chart: `Close` price v·ªõi `ma50`, `ma100`, `ma200`
- Line chart: `cumulative_return` theo ticker
- Area chart: `drawdown` (negative chart)
- Bar chart: `trend` distribution
- Heat map: `monthly_volatility` by ticker v√† month

---

## üîß QU·∫¢N L√ù V√Ä MONITORING

## üîß QU·∫¢N L√ù V√Ä MONITORING

### Ki·ªÉm tra logs c√°c services

```bash
# T·∫•t c·∫£ services
docker compose logs -f

# Kafka Producer
docker exec python-worker tail -f /tmp/producer.log

# Spark Streaming
docker logs spark-streaming-simple -f --tail 50

# Batch Processing (n·∫øu ch·∫°y daemon)
docker exec python-worker tail -f /logs/unified_runner.log

# Elasticsearch
docker logs elasticsearch --tail 50

# Kafka
docker logs kafka --tail 50
```

### Ki·ªÉm tra health c·ªßa c√°c services

```bash
# System health check (t·ª± ƒë·ªông)
docker exec python-worker python /app/unified_runner.py monitor

# Elasticsearch
curl "http://localhost:9200/_cluster/health?pretty"
curl "http://localhost:9200/_cat/indices?v"

# HDFS
docker exec hadoop-namenode hdfs dfs -ls /
docker exec hadoop-namenode hdfs dfs -ls /serving
docker exec hadoop-namenode hdfs dfs -ls /user/kafka_data/stocks_history

# Kafka topics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic stocks-history

# Kibana
curl -s "http://localhost:5601/api/status" | grep -o '"state":"[^"]*"'
```

### Restart c√°c services

```bash
# Restart Producer
docker exec python-worker pkill -f kafka_producer.py
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"

# Restart Spark Streaming
docker compose restart spark-streaming-simple

# Restart Elasticsearch
docker compose restart elasticsearch

# Restart t·∫•t c·∫£
docker compose restart
```

---

## üõë D·ª™NG V√Ä RESET H·ªÜ TH·ªêNG

### D·ª´ng to√†n b·ªô h·ªá th·ªëng

```bash
docker compose down
```

### Reset ho√†n to√†n (x√≥a d·ªØ li·ªáu c≈©)

```bash
# Stop t·∫•t c·∫£
docker compose down

# X√≥a volumes
docker volume rm big_data_hdfs-namenode big_data_hdfs-datanode big_data_spark-ivy-cache 2>/dev/null || true

# X√≥a Spark checkpoints
docker compose up -d hadoop-namenode
sleep 15
docker exec hadoop-namenode hdfs dfs -rm -r /user/spark_checkpoints/* 2>/dev/null || true
docker compose stop hadoop-namenode

# Start l·∫°i t·ª´ ƒë·∫ßu
docker compose down
docker compose up -d
```

### Reset ch·ªâ Elasticsearch data

```bash
# X√≥a indexes
curl -X DELETE "http://localhost:9200/stock_realtime"
curl -X DELETE "http://localhost:9200/batch-features"

# Ho·∫∑c x√≥a t·∫•t c·∫£ indexes
curl -X DELETE "http://localhost:9200/*"

# Restart Spark Streaming ƒë·ªÉ t·∫°o l·∫°i index
docker compose restart spark-streaming-simple
```

---

## üîç TROUBLESHOOTING

## üîç TROUBLESHOOTING

### 1. Index kh√¥ng xu·∫•t hi·ªán trong Elasticsearch

**Ki·ªÉm tra Producer:**

```bash
docker exec python-worker ps aux | grep kafka_producer
docker exec python-worker tail -30 /tmp/producer.log
```

**N·∫øu kh√¥ng ch·∫°y, restart:**

```bash
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"
```

**Ki·ªÉm tra Kafka c√≥ nh·∫≠n messages:**

```bash
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic stocks-history \
  --from-beginning \
  --max-messages 5
```

### 2. Spark Streaming c√≥ l·ªói

**Xem logs chi ti·∫øt:**

```bash
docker logs spark-streaming-simple --tail 100
```

**L·ªói th∆∞·ªùng g·∫∑p:**

**a) NoSuchMethodError / Compatibility issues**

```bash
# X√≥a checkpoint v√† restart
docker exec hadoop-namenode hdfs dfs -rm -r /user/spark_checkpoints/stock_realtime
docker compose restart spark-streaming-simple
```

**b) Elasticsearch connection refused**

```bash
# Ki·ªÉm tra ES ƒëang ch·∫°y
curl "http://localhost:9200/_cluster/health"

# Restart ES n·∫øu c·∫ßn
docker compose restart elasticsearch
sleep 30
docker compose restart spark-streaming-simple
```

**c) Kafka connection timeout**

```bash
# Ki·ªÉm tra Kafka
docker logs kafka --tail 50

# Restart Kafka
docker compose restart zookeeper kafka
sleep 30
docker compose restart spark-streaming-simple
```

### 3. Batch Processing th·∫•t b·∫°i

**Ki·ªÉm tra l·ªói:**

```bash
docker exec python-worker python /app/unified_runner.py batch
```

**L·ªói th∆∞·ªùng g·∫∑p:**

**a) HDFS connection failed**

```bash
# Ki·ªÉm tra HDFS
docker logs hadoop-namenode --tail 50
curl "http://localhost:9870"

# Restart HDFS
docker compose restart hadoop-namenode hadoop-datanode
```

**b) Elasticsearch indexing failed**

```bash
# Ki·ªÉm tra ES health
curl "http://localhost:9200/_cluster/health?pretty"

# Ki·ªÉm tra disk space
docker exec elasticsearch df -h
```

**c) Memory error (OOM)**

```bash
# Gi·∫£m batch size trong run_all.py
# Ho·∫∑c tƒÉng memory cho python-worker trong docker-compose.yml
```

### 4. Kibana kh√¥ng hi·ªÉn th·ªã d·ªØ li·ªáu

**Ki·ªÉm tra Elasticsearch c√≥ d·ªØ li·ªáu:**

```bash
curl "http://localhost:9200/stock_realtime/_count?pretty"
curl "http://localhost:9200/batch-features/_count?pretty"
```

**N·∫øu count > 0 nh∆∞ng Kibana kh√¥ng th·∫•y:**

- Refresh trang Kibana (F5)
- Ki·ªÉm tra Time Range (ch·ªçn r·ªông h∆°n: Last 24 hours)
- X√≥a Index Pattern v√† t·∫°o l·∫°i
- Clear browser cache

**Ki·ªÉm tra Kibana logs:**

```bash
docker logs kibana --tail 50
```

### 5. Data kh√¥ng update trong Kibana

**Ki·ªÉm tra th·ªùi gian:**

```bash
# So s√°nh th·ªùi gian h·ªá th·ªëng v·ªõi d·ªØ li·ªáu
date
curl "http://localhost:9200/stock_realtime/_search?pretty" | grep window_start | head -5
```

**N·∫øu time mismatch:**

- Adjust time range trong Kibana
- Ho·∫∑c sync th·ªùi gian containers v·ªõi host

### 6. Performance issues

**a) Streaming lag:**

```bash
# Check Spark UI
# M·ªü http://localhost:18080 (n·∫øu history server ƒë∆∞·ª£c enable)
# Ho·∫∑c check logs
docker logs spark-streaming-simple | grep "Batch"
```

**b) Kafka lag:**

```bash
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe \
  --all-groups
```

**c) Elasticsearch slow:**

```bash
curl "http://localhost:9200/_cat/thread_pool?v"
curl "http://localhost:9200/_nodes/stats/indices?pretty"
```

---

## üìà C·∫§U TR√öC D·ªÆ LI·ªÜU CHI TI·∫æT

### Kafka Message Schema (stocks-history topic)

```json
{
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "time": "2025-01-05T10:30:00+00:00",
  "Open": 280.5,
  "High": 282.1,
  "Low": 278.2,
  "Close": 281.0,
  "Adj Close": 281.0,
  "Volume": 45123456
}
```

### Elasticsearch Index: stock_realtime

**Mapping:**

```json
{
  "window_start": "2025-01-05T10:00:00Z",
  "window_end": "2025-01-05T10:00:30Z",
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "avg_price": 280.75,
  "min_price": 278.2,
  "max_price": 282.1,
  "total_volume": 135370368,
  "trade_count": 3,
  "price_volatility": 1.42,
  "processed_time": "2025-01-05T10:01:05Z"
}
```

**Gi·∫£i th√≠ch:**

- `window_start/end`: 30s time window
- `avg_price`: Average Close price trong window
- `min_price`: Min Low price trong window
- `max_price`: Max High price trong window
- `total_volume`: Sum Volume trong window
- `trade_count`: S·ªë messages trong window
- `price_volatility`: Standard deviation c·ªßa Close price

### Elasticsearch Index: batch-features

**Mapping:**

```json
{
  "@timestamp": "2025-01-05T10:30:00Z",
  "ticker": "AAPL",
  "time": "2025-01-05T10:30:00Z",
  "Open": 280.5,
  "High": 282.1,
  "Low": 278.2,
  "Close": 281.0,
  "Volume": 45123456,

  "ma50": 275.3,
  "ma100": 270.45,
  "ma200": 265.8,
  "trend": "up",
  "trend_strength": 0.0234,

  "cumulative_return": 0.1523,
  "drawdown": -0.0234,
  "max_drawdown": -0.0812,

  "volume_ma20": 42000000,
  "volume_ratio": 1.074,

  "month": "2025-01",
  "monthly_volatility": 0.0245,
  "market_regime": "normal"
}
```

**Gi·∫£i th√≠ch:**

- `ma50/100/200`: Moving averages (50, 100, 200 days)
- `trend`: up (ma50 > ma200), down, sideway
- `trend_strength`: (ma50 - ma200) / Close
- `cumulative_return`: T·ª∑ su·∫•t sinh l·ª£i t√≠ch l≈©y
- `drawdown`: % s·ª•t gi·∫£m t·ª´ ƒë·ªânh g·∫ßn nh·∫•t
- `max_drawdown`: Drawdown t·ªëi ƒëa trong l·ªãch s·ª≠
- `volume_ma20`: Volume trung b√¨nh 20 ng√†y
- `volume_ratio`: Volume hi·ªán t·∫°i / volume_ma20
- `monthly_volatility`: Volatility theo th√°ng
- `market_regime`: normal ho·∫∑c high_vol

---

## ‚öôÔ∏è C·∫§U H√åNH H·ªÜ TH·ªêNG

### Kafka Producer (kafka_producer.py)

```python
KAFKA_TOPIC = "stocks-history"      # Topic duy nh·∫•t
UPDATE_INTERVAL = 30                # 30 gi√¢y/batch
TICKERS = ["AAPL", "NVDA"]          # Danh s√°ch c·ªï phi·∫øu
```

Thay ƒë·ªïi tickers:

```bash
# Trong docker-compose.yml
environment:
  - TICKERS=AAPL,NVDA,TSLA,MSFT
```

### Spark Streaming (spark_streaming_simple.py)

```python
WINDOW_DURATION = "30 seconds"      # Time window
WATERMARK_DELAY = "1 minute"        # Late data tolerance
TRIGGER_INTERVAL = "30 seconds"     # Processing trigger
```

### Batch Processing (run_all.py)

```python
# C√°c batch jobs ƒë∆∞·ª£c ch·∫°y:
- batch_long_term_trend()          # MA50, MA100, MA200
- batch_cumulative_return()         # Cumulative return
- batch_drawdown()                  # Drawdown, max drawdown
- batch_volume_features()           # Volume MA, ratio
- batch_monthly_volatility()        # Monthly volatility
- batch_market_regime()             # Market regime classification
```

### Unified Runner (unified_runner.py)

```bash
# Environment variables
RUN_MODE=all                        # all, batch, monitor
BATCH_INTERVAL_HOURS=24             # Batch interval

# Usage
python unified_runner.py            # Continuous mode
python unified_runner.py batch      # Run batch once
python unified_runner.py monitor    # Check health once
```

---

## üìû T·ªîNG K·∫æT V√Ä H·ªñ TR·ª¢

### Quick Commands Cheat Sheet

```bash
# Start h·ªá th·ªëng
docker compose up -d
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"

# Check status
docker compose ps
curl "http://localhost:9200/_cat/indices?v"
docker logs spark-streaming-simple --tail 20

# Run batch
docker exec python-worker python /app/unified_runner.py batch

# View logs
docker exec python-worker tail -f /tmp/producer.log
docker logs spark-streaming-simple -f

# Health check
docker exec python-worker python /app/unified_runner.py monitor

# Reset
docker compose down
docker volume prune -f
docker compose up -d
```

### Architecture Summary

```
DATA FLOW:
1. history.json ‚Üí Kafka Producer ‚Üí stocks-history topic
2a. stocks-history ‚Üí Kafka Consumer ‚Üí HDFS (raw storage)
2b. stocks-history ‚Üí Spark Streaming ‚Üí Elasticsearch (stock_realtime)
3. history.json ‚Üí Batch Processing ‚Üí HDFS + Elasticsearch (batch-features)

UNIFIED SCHEMA: ticker, company, time, Open, High, Low, Close, Adj Close, Volume
SINGLE TOPIC: stocks-history
INDEXES: stock_realtime (streaming), batch-features (batch)
```

### Ports Reference

- **9200**: Elasticsearch REST API
- **5601**: Kibana UI
- **9092**: Kafka broker
- **2181**: Zookeeper
- **9870**: HDFS NameNode UI
- **9864**: HDFS DataNode UI

### Monitoring URLs

- Kibana: http://localhost:5601
- Elasticsearch: http://localhost:9200
- HDFS: http://localhost:9870

---

**üéâ H·ªÜ TH·ªêNG ƒê√É ƒê∆Ø·ª¢C TH·ªêNG NH·∫§T V√Ä S·∫¥N S√ÄNG HO·∫†T ƒê·ªòNG!**

_D·ª± √°n merge th√†nh c√¥ng batch processing v√† real-time streaming v·ªõi schema nh·∫•t qu√°n._
