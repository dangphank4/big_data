---
# Spark Anomaly Detection - Real-time Price Anomaly Detection
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-anomaly-detection
  namespace: bigdata
  labels:
    app: spark-anomaly-detection
    component: speed-layer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-anomaly-detection
  template:
    metadata:
      labels:
        app: spark-anomaly-detection
        component: speed-layer
    spec:
      containers:
      - name: spark-anomaly-detection
        image: apache/spark:3.4.3
        command:
        - /opt/spark/bin/spark-submit
        - --master
        - local[*]
        - --conf
        - spark.sql.shuffle.partitions=4
        - --conf
        - spark.sql.streaming.minBatchesToRetain=2
        - --packages
        - org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3,org.elasticsearch:elasticsearch-spark-30_2.12:7.17.16
        - /app/spark_streaming/spark_anomaly_detection.py
        envFrom:
        - configMapRef:
            name: bigdata-config
        env:
        - name: ES_INDEX_ANOMALY
          value: "stock_anomalies"
        - name: CHECKPOINT_LOCATION_ANOMALY
          value: "hdfs://hadoop-namenode:9000/user/spark_checkpoints/stock_anomaly_v1"
        volumeMounts:
        - name: app-code
          mountPath: /app
        - name: logs
          mountPath: /logs
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command:
            - pgrep
            - -f
            - spark_anomaly_detection
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
      volumes:
      - name: app-code
        hostPath:
          path: /path/to/your/project  # Update this
          type: Directory
      - name: logs
        emptyDir: {}
      restartPolicy: Always

---
# Service for anomaly detection (optional, for monitoring)
apiVersion: v1
kind: Service
metadata:
  name: spark-anomaly-detection
  namespace: bigdata
  labels:
    app: spark-anomaly-detection
spec:
  selector:
    app: spark-anomaly-detection
  ports:
  - name: ui
    port: 4040
    targetPort: 4040
    protocol: TCP
  type: ClusterIP
