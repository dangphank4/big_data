# üê≥ DOCKER DEPLOYMENT GUIDE

**Complete step-by-step guide for deploying Big Data Stock Analysis System using Docker Compose**

---

## ÔøΩ FIX THIS FIRST - Docker Permission Error

**N·∫øu b·∫°n th·∫•y l·ªói n√†y:**

```
permission denied while trying to connect to the Docker daemon socket
```

**‚úÖ Gi·∫£i ph√°p (ch·ªçn 1 trong 2):**

### Option 1: Fix Permissions (RECOMMENDED - ch·ªâ l√†m 1 l·∫ßn)

```bash
# Add user v√†o docker group
sudo usermod -aG docker $USER

# Logout v√† login l·∫°i HO·∫∂C ch·∫°y:
newgrp docker

# Ki·ªÉm tra:
docker ps
# N·∫øu kh√¥ng c√≤n l·ªói "permission denied" = OK!
```

### Option 2: Temporary Fix (m·ªói l·∫ßn ph·∫£i th√™m sudo)

```bash
# Th√™m sudo v√†o m·ªçi l·ªánh docker:
sudo docker build -f config/Dockerfile -t bigdata-app:latest .
sudo docker compose -f config/docker-compose.yml up -d
```

---

## üìã TABLE OF CONTENTS

1. [Quick Start Summary](#-quick-start-summary) ‚≠ê **B·∫ÆT ƒê·∫¶U T·ª™ ƒê√ÇY**
2. [Prerequisites](#-prerequisites)
3. [Environment Setup](#-environment-setup)
4. [Build Docker Images](#-build-docker-images)
5. [Deploy Infrastructure](#-deploy-infrastructure)
6. [Deploy Application Services](#-deploy-application-services)
7. [Verify Deployment](#-verify-deployment)
8. [Monitor System](#-monitor-system)
9. [Access Services](#-access-services)
10. [Testing Data Flow](#-testing-data-flow)
11. [Troubleshooting](#-troubleshooting)
12. [Cleanup & Teardown](#-cleanup--teardown)

---

## ‚≠ê QUICK START SUMMARY

**Ch·∫°y theo th·ª© t·ª± n√†y (sau khi fix Docker permissions ·ªü tr√™n):**

```bash
# 1. Fix Docker permissions (n·∫øu ch∆∞a l√†m)
sudo usermod -aG docker $USER
newgrp docker

# 2. C√†i Docker Compose v2 (n·∫øu ch∆∞a c√≥)
# Add Docker repository
sudo apt-get update
sudo apt-get install -y ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt-get update
sudo apt-get install -y docker-compose-plugin docker-buildx-plugin

# 3. Navigate to project
cd /home/danz/Downloads/big_data

# 4. Build image
docker build -f config/Dockerfile -t bigdata-app:latest .

# 5. Start infrastructure (Kafka, HDFS, Elasticsearch)
docker compose -f config/docker-compose.yml up -d zookeeper
sleep 30
docker compose -f config/docker-compose.yml up -d kafka
sleep 60
docker compose -f config/docker-compose.yml up -d hadoop-namenode hadoop-datanode
sleep 45
docker compose -f config/docker-compose.yml up -d elasticsearch kibana
sleep 60

# 6. Start application services
docker compose -f config/docker-compose.yml up -d stock-producer
docker compose -f config/docker-compose.yml up -d spark-kafka-bridge
docker compose -f config/docker-compose.yml up -d spark-streaming-metrics spark-streaming-alerts

# 7. Check everything is running
docker compose -f config/docker-compose.yml ps

# 8. Wait 2-3 minutes, then check data
curl -X GET "http://localhost:9200/_cat/indices?v"
# Should see: stock-realtime-1m, stock-alerts-1m

# 9. Open Kibana
echo "Open browser: http://localhost:5601"
```

**Chi ti·∫øt t·ª´ng b∆∞·ªõc ·ªü c√°c section ph√≠a d∆∞·ªõi ‚Üì**

---

## ‚úÖ PREREQUISITES

### 1. Fix Docker Access (REQUIRED - l√†m tr∆∞·ªõc ti√™n!)

```bash
# Test xem Docker c√≥ ho·∫°t ƒë·ªông kh√¥ng:
docker ps

# N·∫øu th·∫•y "permission denied":
sudo usermod -aG docker $USER
newgrp docker
docker ps  # Test l·∫°i
```

### 2. Install Docker Compose v2 & Buildx

```bash
# Check hi·ªán t·∫°i:
docker compose version
docker buildx version

# N·∫øu kh√¥ng c√≥, c·∫ßn add Docker repository tr∆∞·ªõc:
# 1. Add Docker's GPG key
sudo apt-get update
sudo apt-get install -y ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# 2. Add Docker repository
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# 3. Install plugins
sudo apt-get update
sudo apt-get install -y docker-compose-plugin docker-buildx-plugin

# Verify:
docker compose version  # Should show v2.x.x
docker buildx version   # Should show buildx version
```

### 3. System Requirements Check

```bash
# Check Docker version
docker --version
# Expected: Docker version 20.10.x or higher

# Check available disk space (need at least 20GB)
df -h /var/lib/docker
```

### System Requirements

- **RAM**: Minimum 8GB (16GB recommended)
- **Disk**: Minimum 20GB free space
- **OS**: Linux/macOS/Windows with WSL2
- **Network**: Internet connection for pulling images and crawling data

---

## üîß ENVIRONMENT SETUP

### Step 1: Clone/Navigate to Project

```bash
cd /home/danz/Downloads/big_data

# Verify project structure
ls -la
# Should see: src/, deployment/, config/, requirements.txt, README.md
```

**üì∏ Screenshot Checkpoint 2**: Project directory structure

---

### Step 2: Review Environment Variables

```bash
# Check docker-compose configuration
cat config/docker-compose.yml | grep -A 20 "environment:"
```

**üì∏ Screenshot Checkpoint 3**: Environment variables displayed

---

### Step 3: Update Configuration (Optional)

Edit `config/docker-compose.yml` if needed:

```yaml
environment:
  # Kafka Configuration
  KAFKA_BROKER: "kafka:9092"
  KAFKA_TOPIC: "stocks-realtime"
  CRAWL_INTERVAL: "60" # seconds (1 minute)

  # Tickers to crawl (modify as needed)
  TICKERS: "AAPL,NVDA,TSLA,MSFT,GOOGL"

  # HDFS Configuration
  # NOTE:
  # - WebHDFS (HTTP) is on port 9870 (used by Python hdfs.InsecureClient)
  # - HDFS RPC is on port 9000 (used by Spark hdfs:// URIs)
  HDFS_HOST: "hadoop-namenode"
  HDFS_PORT: "9870"
  HDFS_BASE_PATH: "/stock-data"

  # Elasticsearch
  ELASTICSEARCH_HOST: "elasticsearch"
  ELASTICSEARCH_PORT: "9200"
```

---

## üèóÔ∏è BUILD DOCKER IMAGES

### Step 4: Build Application Image

```bash
cd /home/danz/Downloads/big_data

# Build the main application image
docker build -f config/Dockerfile -t bigdata-app:latest .

# N·∫øu build th√†nh c√¥ng, b·∫°n s·∫Ω th·∫•y:
# => exporting to image
# => naming to docker.io/library/bigdata-app:latest
```

**N·∫øu g·∫∑p l·ªói:**

- `permission denied` ‚Üí Quay l·∫°i [FIX THIS FIRST](#-fix-this-first---docker-permission-error)
- `docker: unknown command` ‚Üí Install Compose v2: `sudo apt-get install -y docker-compose-plugin`

**Expected output:**

```
[+] Building 45.2s (12/12) FINISHED
 => [internal] load build definition
 => [internal] load .dockerignore
 => [internal] transferring context
 => CACHED [1/7] FROM docker.io/library/python:3.12-slim
 => [2/7] RUN apt-get update && apt-get install -y...
 ...
 => exporting to image
 => naming to docker.io/library/bigdata-app:latest
```

**üì∏ Screenshot Checkpoint 4**: Successful build output

---

### Step 5: Verify Image Created

```bash
# List Docker images
docker images | grep bigdata

# Check image size
docker images bigdata-app:latest --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}"
```

**üì∏ Screenshot Checkpoint 5**: Docker image listed

---

## üöÄ DEPLOY INFRASTRUCTURE

### One-command option (recommended)

This starts the full infrastructure stack (Kafka + HDFS + Elasticsearch + Kibana) in exactly one command.

```bash
docker compose -f config/docker-compose.yml up -d \
  zookeeper kafka \
  hadoop-namenode hadoop-datanode \
  elasticsearch kibana
```

Quick verification:

```bash
# Show container status
docker compose -f config/docker-compose.yml ps

# Kibana should respond once Elasticsearch is ready
curl -I http://localhost:5601
```

### One-command shutdown/cleanup

```bash
# Stop and remove containers + network (keeps named volumes: HDFS/Elasticsearch data)
docker compose -f config/docker-compose.yml down

# Full cleanup (also deletes named volumes: ALL persisted data will be lost)
docker compose -f config/docker-compose.yml down -v
```

Optional (aggressive) cleanup if your disk fills up:

```bash
# Removes unused images/build cache across Docker (global)
docker system prune -af
```

### Step 6: Start Zookeeper & Kafka

```bash
# Start Zookeeper first
docker compose -f config/docker-compose.yml up -d zookeeper

# Wait 30 seconds for Zookeeper to be ready
sleep 30

# Check Zookeeper logs
docker compose -f config/docker-compose.yml logs zookeeper | tail -20
```

**Expected output:**

```
zookeeper_1  | [2024-01-13 10:00:00,123] INFO binding to port 0.0.0.0/0.0.0.0:2181
zookeeper_1  | [2024-01-13 10:00:00,456] INFO Server environment:zookeeper.version=3.8.0
```

**üì∏ Screenshot Checkpoint 6**: Zookeeper running successfully

---

### Step 7: Start Kafka Broker

```bash
# Start Kafka
docker compose -f config/docker-compose.yml up -d kafka

# Wait 60 seconds for Kafka to be ready
sleep 60

# Check Kafka logs
docker compose -f config/docker-compose.yml logs kafka | tail -30
```

**Expected output:**

```
kafka_1  | [2024-01-13 10:01:00] INFO Kafka Server started
kafka_1  | [2024-01-13 10:01:01] INFO [KafkaServer id=1] started
```

**üì∏ Screenshot Checkpoint 7**: Kafka broker running

---

### Step 8: Verify Kafka Topic Creation

```bash
# List Kafka topics
docker exec -it kafka kafka-topics.sh \
  --list \
  --bootstrap-server localhost:9092
```

**Expected output:**

```
stocks-realtime
stocks-realtime-spark
```

**üì∏ Screenshot Checkpoint 8**: Kafka topic exists

---

### Step 9: Start HDFS

```bash
# Start HDFS namenode and datanode
docker compose -f config/docker-compose.yml up -d hadoop-namenode hadoop-datanode

# Wait 45 seconds
sleep 45

# Check HDFS status
docker exec -it hadoop-namenode hdfs dfsadmin -report
```

**Expected output:**

```
Live datanodes (1):
Name: 172.18.0.x:9866 (hdfs-datanode)
...
```

**üì∏ Screenshot Checkpoint 9**: HDFS cluster healthy

---

### Step 10: Start Elasticsearch

```bash
# Start Elasticsearch
docker compose -f config/docker-compose.yml up -d elasticsearch

# Wait 60 seconds
sleep 60

# Check Elasticsearch health
curl -X GET "http://localhost:9200/_cluster/health?pretty"
```

**Expected output:**

```json
{
  "cluster_name" : "docker-cluster",
  "status" : "yellow",
  "number_of_nodes" : 1,
  ...
}
```

**üì∏ Screenshot Checkpoint 10**: Elasticsearch running

---

### Step 11: Start Kibana (Optional)

```bash
# Start Kibana
docker compose -f config/docker-compose.yml up -d kibana

# Wait 30 seconds
sleep 30

# Check if Kibana is accessible
curl -I http://localhost:5601
```

**Expected output:**

```
HTTP/1.1 200 OK
```

**üì∏ Screenshot Checkpoint 11**: Kibana accessible

---

## üì° DEPLOY APPLICATION SERVICES

### One-command option (recommended)

```bash
docker compose -f config/docker-compose.yml up -d \
  stock-producer \
  spark-kafka-bridge \
  spark-streaming-metrics \
  spark-streaming-alerts \
  hdfs-archiver
```

**Note**: L·ªánh n√†y s·∫Ω build images n·∫øu ch∆∞a c√≥ v√† start t·∫•t c·∫£ application services c√πng l√∫c.

**Expected build time**:

- L·∫ßn ƒë·∫ßu: ~3-5 ph√∫t (build Spark image v·ªõi dependencies)
- L·∫ßn sau: ~10 gi√¢y (d√πng cached image)

**Verify deployment**:

```bash
# Check all containers are running
docker compose -f config/docker-compose.yml ps

# Check Spark Streaming logs (should see "Streaming query started")
docker compose -f config/docker-compose.yml logs --tail=30 spark-streaming-metrics
docker compose -f config/docker-compose.yml logs --tail=30 spark-streaming-alerts

# Check producer is crawling
docker compose -f config/docker-compose.yml logs --tail=20 stock-producer
```

### Alternative: Deploy t·ª´ng service m·ªôt (Step by Step)

N·∫øu mu·ªën monitor t·ª´ng service khi start:

### Step 12: Start Kafka Producer (Real-time Crawling)

```bash
# Start the producer
docker compose -f config/docker-compose.yml up -d stock-producer

# Watch producer logs (Ctrl+C to exit)
docker compose -f config/docker-compose.yml logs -f stock-producer
```

**Expected output:**

```
stock-producer  | [START] Real-time crawling for 5 tickers every 60s
stock-producer  | [CRAWL] AAPL
stock-producer  | [CRAWL] NVDA
stock-producer  | [BATCH 1] 2024-01-13T10:05:00 | 5 records | 3.45s
```

**üì∏ Screenshot Checkpoint 12**: Producer crawling successfully

---

### Step 12B (Recommended): Start Kafka -> HDFS Archiver (near-real-time archive)

Service n√†y s·∫Ω ƒë·ªçc t·ª´ `stocks-realtime` v√† ghi NDJSON v√†o HDFS theo c·∫•u tr√∫c `/stock-data/YYYY-MM-DD/TICKER.json`.

```bash
docker compose -f config/docker-compose.yml up -d hdfs-archiver

# Check archiver logs
docker compose -f config/docker-compose.yml logs --tail=50 hdfs-archiver
```

---

### Step 13: Verify Data in Kafka

```bash
# Consume messages from Kafka (Ctrl+C after seeing data)
docker exec -it kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic stocks-realtime \
  --from-beginning \
  --max-messages 5
```

**Expected output:**

```json
{"ticker":"AAPL","company":"Apple Inc.","time":"2024-01-13T10:05:00","Open":185.23,"High":185.45,"Low":185.10,"Close":185.32,"Adj Close":185.32,"Volume":12345678}
...
```

**üì∏ Screenshot Checkpoint 13**: Kafka messages visible

---

### Step 14: Start Kafka Bridge + Spark Streaming

```bash

# Start Kafka -> Spark bridge
docker compose -f config/docker-compose.yml up -d spark-kafka-bridge

# Start Spark Streaming jobs
docker compose -f config/docker-compose.yml up -d spark-streaming-metrics spark-streaming-alerts

# Wait 90 seconds for Spark to initialize
sleep 90

# Check Spark logs
docker compose -f config/docker-compose.yml logs spark-streaming-metrics | tail -50
docker compose -f config/docker-compose.yml logs spark-streaming-alerts | tail -50
```

**Expected output:**

```
spark-streaming-metrics  | [INFO] Kafka: kafka:9092, Topic: stocks-realtime-spark
spark-streaming-alerts   | [READY] Alerts streaming started -> ES index: stock-alerts-1m
```

**üì∏ Screenshot Checkpoint 14**: Spark Streaming processing data

---

## ‚úîÔ∏è VERIFY DEPLOYMENT

### Step 15: Check All Running Containers

```bash
# List all running containers
docker compose -f config/docker-compose.yml ps

```

**Expected output:**

```
         Name                        State     Ports
-----------------------------------------------------------
zookeeper               Up      2181/tcp
kafka                   Up      9092/tcp
hadoop-namenode         Up      9870/tcp
hadoop-datanode         Up      9864/tcp
elasticsearch           Up      9200/tcp, 9300/tcp
kibana                  Up      5601/tcp
stock-producer          Up
hdfs-archiver            Up
spark-kafka-bridge      Up
spark-streaming-metrics Up
spark-streaming-alerts  Up
```

**üì∏ Screenshot Checkpoint 15**: All services running

---

### Step 16: Check Elasticsearch Indices

```bash
# List Elasticsearch indices
curl -X GET "http://localhost:9200/_cat/indices?v"
```

**Expected output:**

```
health status index                uuid   pri rep docs.count docs.deleted store.size
yellow open   stock-realtime-1m    xyz123   1   1         45            0      12.5kb
yellow open   stock-alerts-1m      abc456   1   1         12            0       8.2kb
```

**üì∏ Screenshot Checkpoint 16**: Elasticsearch indices created

---

### Step 17: Query Real-time Data

```bash
# Query 1-minute aggregations
curl -X GET "http://localhost:9200/stock-realtime-1m/_search?pretty" \
  -H 'Content-Type: application/json' \
  -d '{
    "size": 5,
    "sort": [{"window_start": {"order": "desc"}}]
  }'
```

**Expected output:**

```json
{
  "hits": {
    "total": { "value": 45 },
    "hits": [
      {
        "_source": {
          "ticker": "AAPL",
          "window_start": "2024-01-13T10:05:00",
          "high_1m": 185.45,
          "low_1m": 185.1,
          "close_avg_1m": 185.32
        }
      }
    ]
  }
}
```

**üì∏ Screenshot Checkpoint 17**: Real-time data in Elasticsearch

---

## üìä MONITOR SYSTEM

### Step 18: Monitor Resource Usage

```bash
# Check Docker stats
docker stats --no-stream
```

**Expected output:**

```
CONTAINER           CPU %     MEM USAGE / LIMIT     MEM %
stock-producer      2.5%      512MiB / 1GiB        50%
spark-streaming-metrics  15.3%     2.1GiB / 4GiB        52.5%
elasticsearch       8.2%      1.5GiB / 2GiB        75%
kafka               5.1%      1GiB / 2GiB          50%
```

**üì∏ Screenshot Checkpoint 18**: Resource usage

---

### Step 19: Check Disk Usage

```bash
# Check HDFS usage
docker exec -it hadoop-namenode hdfs dfs -df -h
docker exec -it hadoop-namenode hdfs dfs -ls /stock-data
```

**Expected output:**

```
Filesystem                Size    Used   Available  Use%
hdfs://namenode:9000     50.0G   1.2G      48.8G    2%

Found 1 items
drwxr-xr-x   - root supergroup          0 2024-01-13 10:00 /stock-data/2024-01-13
```

**üì∏ Screenshot Checkpoint 19**: HDFS data directory

---

## üåê ACCESS SERVICES

### Step 20: Access Web UIs

Open these URLs in your browser:

1. **Kibana Dashboard**: http://localhost:5601
2. **Elasticsearch**: http://localhost:9200
3. **HDFS NameNode UI**: http://localhost:9870

**üì∏ Screenshot Checkpoint 20**:

- Kibana main page
- HDFS web UI showing cluster overview

---

### Step 21: Configure Kibana Index Pattern

1. Open Kibana: http://localhost:5601
2. Go to **Management** ‚Üí **Stack Management** ‚Üí **Index Patterns**
3. Click **Create index pattern**
4. Enter pattern: `stock-realtime-*`
5. Select time field: `window_start`
6. Click **Create**

**üì∏ Screenshot Checkpoint 21**: Kibana index pattern created

---

### Step 22: Create Kibana Visualization

1. Go to **Discover** tab
2. Select `stock-realtime-*` index
3. View real-time data streaming in

**üì∏ Screenshot Checkpoint 22**: Kibana showing real-time stock data

---

## üß™ TESTING DATA FLOW

### Step 23: Test End-to-End Data Flow

```bash
# 1. Check producer is sending data
docker compose -f config/docker-compose.yml logs --tail=20 stock-producer

# 2. Verify Kafka has messages
docker exec -it kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic stocks-realtime \
  --max-messages 2

# 3. Check Spark is processing
docker compose -f config/docker-compose.yml logs --tail=30 spark-streaming-metrics
docker compose -f config/docker-compose.yml logs --tail=30 spark-streaming-alerts

# 4. Verify data in Elasticsearch
curl -X GET "http://localhost:9200/stock-realtime-1m/_count"
```

**üì∏ Screenshot Checkpoint 23**: Complete data flow verified

---


---

### Step 24: Feed D·ªØ Li·ªáu L·ªãch S·ª≠ v√†o HDFS (Ph·ª•c v·ª• Batch Features)

**üéØ M·ª•c ƒë√≠ch**: Batch job c·∫ßn d·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ HDFS ƒë·ªÉ t√≠nh to√°n features (MA, trend, volatility, drawdown, etc.). Archiver t·ª´ Kafka ch·ªâ l∆∞u d·ªØ li·ªáu realtime, v√¨ v·∫≠y c·∫ßn backfill d·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ Yahoo Finance.

**Y√™u c·∫ßu**: `hadoop-namenode` v√† `hadoop-datanode` ƒëang ch·∫°y v√† healthy.

#### Option 1: Backfill d·ªØ li·ªáu 1 nƒÉm (RECOMMENDED)

```bash
# Crawl d·ªØ li·ªáu daily nƒÉm 2024 cho 3 m√£ ch√≠nh
docker compose -f config/docker-compose.yml run --rm hdfs-archiver \
  python /app/src/utils/crawl_feed.py \
  --tickers AAPL,NVDA,TSLA \
  --start 2024-01-01 \
  --end 2024-12-31 \
  --interval 1d
```

**Expected output:**

```
======================================================================
BACKFILL TASK
  Tickers: AAPL, NVDA, TSLA
  Date Range: 2024-01-01 to 2024-12-31
======================================================================

[CRAWL] AAPL
  ‚úì /stock-data/2024-01-02/AAPL.json | +1 records (1 total)
  ‚úì /stock-data/2024-01-03/AAPL.json | +1 records (1 total)
  ...
  ‚úì /stock-data/2024-12-30/AAPL.json | +1 records (1 total)

[CRAWL] NVDA
  ‚úì /stock-data/2024-01-02/NVDA.json | +1 records (1 total)
  ...

[CRAWL] TSLA
  ‚úì /stock-data/2024-01-02/TSLA.json | +1 records (1 total)
  ...

======================================================================
BACKFILL COMPLETE
  Files Written: 753
  Files Skipped: 0 (no new data)
  Total New Records: 753
======================================================================
```

#### Option 2: Backfill nhi·ªÅu nƒÉm (cho ph√¢n t√≠ch d√†i h·∫°n)

```bash
# Crawl d·ªØ li·ªáu 5 nƒÉm (s·∫Ω m·∫•t ~5-10 ph√∫t)
docker compose -f config/docker-compose.yml run --rm hdfs-archiver \
  python /app/src/utils/crawl_feed.py \
  --tickers AAPL,NVDA,TSLA,MSFT,GOOGL \
  --days 1825 \
  --interval 1d
```

#### Option 3: Backfill ch·ªçn kho·∫£ng th·ªùi gian c·ª• th·ªÉ

```bash
# Crawl d·ªØ li·ªáu t·ª´ th√°ng 1 ƒë·∫øn th√°ng 6 nƒÉm 2024
docker compose -f config/docker-compose.yml run --rm hdfs-archiver \
  python /app/src/utils/crawl_feed.py \
  --tickers AAPL,NVDA \
  --start 2024-01-01 \
  --end 2024-06-30 \
  --interval 1d
```

**üí° G·ª£i √Ω quan tr·ªçng**:

- **Daily interval (`--interval 1d`)**: Recommended cho backfill d√†i h·∫°n. Yahoo Finance h·ªó tr·ª£ d·ªØ li·ªáu daily cho nhi·ªÅu nƒÉm.
- **Minute interval (`--interval 1m`)**: Ch·ªâ c√≥ d·ªØ li·ªáu ~30 ng√†y g·∫ßn ƒë√¢y t·ª´ Yahoo Finance.
- **Deduplication**: Script t·ª± ƒë·ªông deduplicate n√™n c√≥ th·ªÉ ch·∫°y nhi·ªÅu l·∫ßn an to√†n.

#### Verify d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c crawl

```bash
# Ki·ªÉm tra s·ªë ng√†y c√≥ d·ªØ li·ªáu
docker exec -it hadoop-namenode hdfs dfs -ls /stock-data | wc -l

# Xem chi ti·∫øt c√°c ng√†y
docker exec -it hadoop-namenode hdfs dfs -ls /stock-data | tail -20

# Xem n·ªôi dung file m·∫´u
docker exec -it hadoop-namenode hdfs dfs -cat /stock-data/2024-01-02/AAPL.json
```

**Expected output:**

```json
{"ticker":"AAPL","company":"Apple Inc.","time":"2024-01-02T00:00:00","Open":187.15,"High":188.44,"Low":183.89,"Close":185.64,"Adj Close":184.58,"Volume":82488300}
```

**üì∏ Screenshot Checkpoint 24a**: D·ªØ li·ªáu l·ªãch s·ª≠ ƒë√£ ƒë∆∞·ª£c crawl v√†o HDFS

---

### Step 24b: Ch·∫°y Batch Features Job

Sau khi ƒë√£ c√≥ d·ªØ li·ªáu trong HDFS, ch·∫°y batch job ƒë·ªÉ t√≠nh to√°n features:

```bash
# Run batch features computation
docker compose -f config/docker-compose.yml run --rm spark-batch-features
```

**Expected output:**

```
=== [1] ƒêANG L·∫¨P K·∫æ HO·∫†CH ƒê·ªåC D·ªÆ LI·ªÜU ===
=== [2] X√ÇY D·ª∞NG CHU·ªñI T√çNH TO√ÅN (TRANSFORMATIONS) ===
=== [3] TH·ª∞C THI V√Ä ƒê·∫®Y D·ªÆ LI·ªÜU (ACTIONS) ===
DONE: ƒê√£ l∆∞u k·∫øt qu·∫£ ph√¢n t√°n v√†o HDFS: hdfs://hadoop-namenode:9000/tmp/serving/batch_features
ƒêang ƒë·∫©y d·ªØ li·ªáu l√™n Elasticsearch (Serving Layer)...
DONE: ƒê√£ ƒë·∫©y d·ªØ li·ªáu l√™n Elasticsearch index: batch-features.
```

#### Verify batch features trong Elasticsearch

```bash
# Ki·ªÉm tra s·ªë l∆∞·ª£ng documents
curl -s "http://localhost:9200/batch-features/_count?pretty"

# Xem sample document
curl -s "http://localhost:9200/batch-features/_search?size=1&pretty"
```

**Expected output:**

```json
{
  "count": 501,
  ...
}
```

Sample document s·∫Ω c√≥ c√°c features:

- **Moving Averages**: ma50, ma100, ma200
- **Trend Analysis**: trend, trend_strength
- **Returns**: daily_return, cumulative_return, return_30d, return_90d
- **Risk Metrics**: drawdown, max_drawdown
- **Volume Features**: volume_ma20, volume_ratio
- **Volatility**: monthly_volatility
- **Market Regime**: market_regime (normal/high_vol)

#### T·∫°o Kibana Index Pattern cho Batch Features

1. M·ªü Kibana: http://localhost:5601
2. Go to **Stack Management** ‚Üí **Index Patterns**
3. Click **Create index pattern**
4. Enter: `batch-features*`
5. Select time field: `time`
6. Click **Create index pattern**
7. Go to **Discover** ƒë·ªÉ xem d·ªØ li·ªáu batch features

**üì∏ Screenshot Checkpoint 24b**: Batch features trong Kibana

**‚ö†Ô∏è L∆∞u √Ω**: Batch job c·∫ßn √≠t nh·∫•t 20-30 records m·ªói ticker ƒë·ªÉ t√≠nh to√°n ƒë·∫ßy ƒë·ªß c√°c features (MA200 c·∫ßn 200 data points). V·ªõi √≠t h∆°n, m·ªôt s·ªë features s·∫Ω c√≥ gi√° tr·ªã null v√† b·ªã dropna() lo·∫°i b·ªè.

---

### Step 25: (Optional) Schedule Batch Features CronJob

N·∫øu mu·ªën batch features t·ª± ƒë·ªông ch·∫°y ƒë·ªãnh k·ª≥ (vd: m·ªói ng√†y 1 l·∫ßn), start batch cronjob:

```bash
docker compose -f config/docker-compose.yml up -d spark-batch-features-cron
```

Default schedule trong `docker-compose.yml` l√†:

```yaml
# M·ªói ng√†y l√∫c 2 gi·ªù s√°ng
CRON_SCHEDULE: "0 2 * * *"
```

Verify cronjob ƒëang ch·∫°y:

```bash
docker ps | grep batch-features-cron
```

Xem logs c·ªßa cronjob:

```bash
docker logs -f spark-batch-features-cron
```

**üí° L∆∞u √Ω**: Cronjob ch·ªâ c√≥ √≠ch khi HDFS li√™n t·ª•c c√≥ d·ªØ li·ªáu m·ªõi (t·ª´ archiver ho·∫∑c crawl feed ƒë·ªãnh k·ª≥). N·∫øu kh√¥ng, batch job s·∫Ω ch·ªâ process l·∫°i d·ªØ li·ªáu c≈©.

**üì∏ Screenshot Checkpoint 25**: Batch features cronjob running

---

### Step 26: Test HDFS Archiver (Manual Run)

```bash
# Run HDFS archiver manually (takes recent Kafka data and writes to HDFS)
docker compose -f config/docker-compose.yml run --rm \
  -e KAFKA_BROKER=kafka:9092 \
  -e KAFKA_TOPIC=stocks-realtime \
  -e HDFS_HOST=hadoop-namenode \
  -e HDFS_PORT=9870 \
  -e LOOKBACK_HOURS=1 \
  python-worker \
  python -m src.consumers.kafka_consumer_hdfs_archiver
```

**Expected output:**

```
[START] Archiving data from 2024-01-13 09:00:00 to 2024-01-13 10:00:00
[CONSUME] Reading from Kafka...
  Read 120 messages...
[DONE] Read 120 messages from Kafka
[HDFS] Writing 1 dates to HDFS...
  ‚úì /stock-data/2024-01-13/AAPL.json | +24 records
[COMPLETE] Wrote 5 files, 120 new records to HDFS
```

**üì∏ Screenshot Checkpoint 24**: HDFS archiver completed successfully

---

### Step 27: Verify HDFS Data

```bash
# List HDFS files
docker exec -it hadoop-namenode hdfs dfs -ls -R /stock-data

# Check file content
docker exec -it hadoop-namenode hdfs dfs -cat /stock-data/2024-01-13/AAPL.json | head -5
```

**Expected output:**

```
drwxr-xr-x   - root supergroup          0 2024-01-13 10:00 /stock-data/2024-01-13
-rw-r--r--   1 root supergroup       2456 2024-01-13 10:00 /stock-data/2024-01-13/AAPL.json
-rw-r--r--   1 root supergroup       2391 2024-01-13 10:00 /stock-data/2024-01-13/NVDA.json
```

**üì∏ Screenshot Checkpoint 25**: HDFS files created

---

## üîß TROUBLESHOOTING

### Common Issues & Solutions

#### Issue 1: Kafka Not Starting

```bash
# Check Zookeeper is running
docker compose -f config/docker-compose.yml logs zookeeper | grep -i error

# Restart Kafka
docker compose -f config/docker-compose.yml restart kafka

# Wait and check
sleep 30
docker compose -f config/docker-compose.yml logs kafka | tail -20
```

---

#### Issue 2: Producer Not Sending Data

```bash
# Check producer logs for errors
docker compose -f config/docker-compose.yml logs stock-producer | grep -i error

# Verify network connectivity
docker exec -it stock-producer nc -zv kafka 9092

# Restart producer
docker compose -f config/docker-compose.yml restart stock-producer
```

---

#### Issue 3: Elasticsearch Yellow Status

```bash
# This is normal for single-node cluster
# Check cluster health
curl -X GET "http://localhost:9200/_cluster/health?pretty"

# Reduce replica count (optional)
curl -X PUT "http://localhost:9200/_settings" \
  -H 'Content-Type: application/json' \
  -d '{"index": {"number_of_replicas": 0}}'
```

---

#### Issue 4: Spark Streaming Errors

```bash
# Check Spark logs
docker compose -f config/docker-compose.yml logs spark-streaming-metrics | grep -i error
docker compose -f config/docker-compose.yml logs spark-streaming-alerts | grep -i error

# Check checkpoint directories
docker exec -it spark-streaming-metrics ls -la /tmp/spark-checkpoints || true
docker exec -it spark-streaming-alerts ls -la /tmp/spark-checkpoints || true

# If a checkpoint gets corrupted, remove it and restart the affected service
docker compose -f config/docker-compose.yml stop spark-streaming-metrics
docker exec -it spark-streaming-metrics rm -rf /tmp/spark-checkpoints
docker compose -f config/docker-compose.yml start spark-streaming-metrics
```

---

#### Issue 5: HDFS SafeMode

```bash
# Check HDFS status
docker exec -it hadoop-namenode hdfs dfsadmin -safemode get

# If in safe mode, leave it
docker exec -it hadoop-namenode hdfs dfsadmin -safemode leave

# Verify
docker exec -it hadoop-namenode hdfs dfsadmin -report
```

---

## üóëÔ∏è CLEANUP & TEARDOWN

### Step 28: View Current Resources

```bash
# List all containers
docker compose -f config/docker-compose.yml ps

# List volumes
docker volume ls | grep big_data

# Check disk usage
docker system df
```

**üì∏ Screenshot Checkpoint 26**: Resources before cleanup

---

### Step 29: Stop All Services

```bash
# Stop all containers (keeps data)
docker compose -f config/docker-compose.yml stop

# Verify all stopped
docker compose -f config/docker-compose.yml ps
```

**Expected output:**

```
         Name                        State
-------------------------------------------
big_data_zookeeper_1         Exit 0
big_data_kafka_1             Exit 0

...
```

**üì∏ Screenshot Checkpoint 27**: All services stopped

---

### Step 30: Remove Containers (Keep Data)

```bash
# Remove containers but keep volumes
docker compose -f config/docker-compose.yml down

# Verify containers removed
docker ps -a | grep big_data
```

**üì∏ Screenshot Checkpoint 28**: Containers removed

---

### Step 31: Complete Cleanup (Remove All Data)

```bash
# ‚ö†Ô∏è WARNING: This will DELETE ALL DATA!

# Remove containers and volumes
docker compose -f config/docker-compose.yml down -v

# Remove images (optional)
docker rmi bigdata-app:latest

# Clean up unused resources
docker system prune -a --volumes -f

# Verify cleanup
docker ps -a
docker volume ls
docker images
```

**Expected output:**

```
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
(empty)

VOLUME NAME
(empty)
```

**üì∏ Screenshot Checkpoint 29**: Complete cleanup done

---

### Step 32: Verify Disk Space Reclaimed

```bash
# Check disk usage after cleanup
df -h /var/lib/docker

# Check Docker system info
docker system df
```

**üì∏ Screenshot Checkpoint 30**: Disk space reclaimed

---

## üìù DEPLOYMENT SUMMARY

### ‚úÖ Completed Steps Checklist

- [ ] Prerequisites verified
- [ ] Environment configured
- [ ] Docker images built
- [ ] Zookeeper deployed
- [ ] Kafka deployed
- [ ] HDFS deployed
- [ ] Elasticsearch deployed
- [ ] Kibana deployed
- [ ] Kafka Producer running
- [ ] Spark Streaming running
- [ ] Spark Alerts (1m) running
- [ ] Spark Batch (features) executed
- [ ] Data flow verified
- [ ] Elasticsearch indices created
- [ ] Kibana configured
- [ ] HDFS archiver tested
- [ ] All services monitored
- [ ] Screenshots captured at each checkpoint
- [ ] System tested end-to-end
- [ ] Cleanup performed

---

## üéØ Next Steps

After successful deployment:

1. **Configure Alerts**:

- Infra alerts: pod/container restarts, lag, disk full
- Data alerts: Elasticsearch index `stock-alerts-1m` receiving documents

2. **Optimize Resources**: Adjust memory/CPU based on load
3. **Schedule HDFS Archiver**: Add cron job for daily runs
4. **Schedule Spark Batch**: Run `src/batch_jobs/run_all.py` daily to build batch features and index `batch-features`
5. **Backup HDFS**: Implement backup strategy
6. **Scale Up**: Add more Kafka/Spark containers if needed

---

## üìû Support

If you encounter issues:

1. Check logs: `docker compose -f config/docker-compose.yml logs [service-name]`
2. Review this guide's troubleshooting section
3. Check container health: `docker ps`
4. Verify network: `docker network ls` then `docker network inspect <project>_bigdata-network`

---

**Last Updated**: January 13, 2026  
**Version**: 1.0  
**Environment**: Docker Compose
