# ğŸ³ DOCKER DEPLOYMENT GUIDE

**Complete step-by-step guide for deploying Big Data Stock Analysis System using Docker Compose**

---

## ï¿½ FIX THIS FIRST - Docker Permission Error

**Náº¿u báº¡n tháº¥y lá»—i nÃ y:**

```
permission denied while trying to connect to the Docker daemon socket
```

**âœ… Giáº£i phÃ¡p (chá»n 1 trong 2):**

### Option 1: Fix Permissions (RECOMMENDED - chá»‰ lÃ m 1 láº§n)

```bash
# Add user vÃ o docker group
sudo usermod -aG docker $USER

# Logout vÃ  login láº¡i HOáº¶C cháº¡y:
newgrp docker

# Kiá»ƒm tra:
docker ps
# Náº¿u khÃ´ng cÃ²n lá»—i "permission denied" = OK!
```

### Option 2: Temporary Fix (má»—i láº§n pháº£i thÃªm sudo)

```bash
# ThÃªm sudo vÃ o má»i lá»‡nh docker:
sudo docker build -f config/Dockerfile -t bigdata-app:latest .
sudo docker compose -f config/docker-compose.yml up -d
```

---

## ğŸ“‹ TABLE OF CONTENTS

1. [Quick Start Summary](#-quick-start-summary) â­ **Báº®T Äáº¦U Tá»ª ÄÃ‚Y**
2. [Prerequisites](#-prerequisites)
3. [Environment Setup](#-environment-setup)
4. [Build Docker Images](#-build-docker-images)
5. [Deploy Infrastructure](#-deploy-infrastructure)
6. [Deploy Application Services](#-deploy-application-services)
7. [Verify Deployment](#-verify-deployment)
8. [Monitor System](#-monitor-system)
9. [Access Services](#-access-services)
10. [Testing Data Flow](#-testing-data-flow)
11. [Troubleshooting](#-troubleshooting)
12. [Cleanup & Teardown](#-cleanup--teardown)

---

## â­ QUICK START SUMMARY

**Cháº¡y theo thá»© tá»± nÃ y (sau khi fix Docker permissions á»Ÿ trÃªn):**

```bash
# 1. Fix Docker permissions (náº¿u chÆ°a lÃ m)
sudo usermod -aG docker $USER
newgrp docker

# 2. CÃ i Docker Compose v2 (náº¿u chÆ°a cÃ³)
# Add Docker repository
sudo apt-get update
sudo apt-get install -y ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt-get update
sudo apt-get install -y docker-compose-plugin docker-buildx-plugin

# 3. Navigate to project
cd /home/danz/Downloads/big_data

# 4. Build image
docker build -f config/Dockerfile -t bigdata-app:latest .

# 5. Start infrastructure (Kafka, HDFS, Elasticsearch)
docker compose -f config/docker-compose.yml up -d zookeeper
sleep 30
docker compose -f config/docker-compose.yml up -d kafka
sleep 60
docker compose -f config/docker-compose.yml up -d hadoop-namenode hadoop-datanode
sleep 45
docker compose -f config/docker-compose.yml up -d elasticsearch kibana
sleep 60

# 6. Start application services
docker compose -f config/docker-compose.yml up -d stock-producer
docker compose -f config/docker-compose.yml up -d spark-kafka-bridge
docker compose -f config/docker-compose.yml up -d spark-streaming-metrics spark-streaming-alerts

# 7. Check everything is running
docker compose -f config/docker-compose.yml ps

# 8. Wait 2-3 minutes, then check data
curl -X GET "http://localhost:9200/_cat/indices?v"
# Should see: stock-realtime-1m, stock-alerts-1m

# 9. Open Kibana
echo "Open browser: http://localhost:5601"
```

**Chi tiáº¿t tá»«ng bÆ°á»›c á»Ÿ cÃ¡c section phÃ­a dÆ°á»›i â†“**

---

## âœ… PREREQUISITES

### 1. Fix Docker Access (REQUIRED - lÃ m trÆ°á»›c tiÃªn!)

```bash
# Test xem Docker cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng:
docker ps

# Náº¿u tháº¥y "permission denied":
sudo usermod -aG docker $USER
newgrp docker
docker ps  # Test láº¡i
```

### 2. Install Docker Compose v2 & Buildx

```bash
# Check hiá»‡n táº¡i:
docker compose version
docker buildx version

# Náº¿u khÃ´ng cÃ³, cáº§n add Docker repository trÆ°á»›c:
# 1. Add Docker's GPG key
sudo apt-get update
sudo apt-get install -y ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# 2. Add Docker repository
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# 3. Install plugins
sudo apt-get update
sudo apt-get install -y docker-compose-plugin docker-buildx-plugin

# Verify:
docker compose version  # Should show v2.x.x
docker buildx version   # Should show buildx version
```

### 3. System Requirements Check

```bash
# Check Docker version
docker --version
# Expected: Docker version 20.10.x or higher

# Check available disk space (need at least 20GB)
df -h /var/lib/docker
```

### System Requirements

- **RAM**: Minimum 8GB (16GB recommended)
- **Disk**: Minimum 20GB free space
- **OS**: Linux/macOS/Windows with WSL2
- **Network**: Internet connection for pulling images and crawling data

---

## ğŸ”§ ENVIRONMENT SETUP

### Step 1: Clone/Navigate to Project

```bash
cd /home/danz/Downloads/big_data

# Verify project structure
ls -la
# Should see: src/, deployment/, config/, requirements.txt, README.md
```

**ğŸ“¸ Screenshot Checkpoint 2**: Project directory structure

---

### Step 2: Review Environment Variables

```bash
# Check docker-compose configuration
cat config/docker-compose.yml | grep -A 20 "environment:"
```

**ğŸ“¸ Screenshot Checkpoint 3**: Environment variables displayed

---

### Step 3: Update Configuration (Optional)

Edit `config/docker-compose.yml` if needed:

```yaml
environment:
  # Kafka Configuration
  KAFKA_BROKER: "kafka:9092"
  KAFKA_TOPIC: "stocks-realtime"
  CRAWL_INTERVAL: "60" # seconds (1 minute)

  # Tickers to crawl (modify as needed)
  TICKERS: "AAPL,NVDA,TSLA,MSFT,GOOGL"

  # HDFS Configuration
  # NOTE:
  # - WebHDFS (HTTP) is on port 9870 (used by Python hdfs.InsecureClient)
  # - HDFS RPC is on port 9000 (used by Spark hdfs:// URIs)
  HDFS_HOST: "hadoop-namenode"
  HDFS_PORT: "9870"
  HDFS_BASE_PATH: "/stock-data"

  # Elasticsearch
  ELASTICSEARCH_HOST: "elasticsearch"
  ELASTICSEARCH_PORT: "9200"
```

---

## ğŸ—ï¸ BUILD DOCKER IMAGES

### Step 4: Build Application Image

```bash
cd /home/danz/Downloads/big_data

# Build the main application image
docker build -f config/Dockerfile -t bigdata-app:latest .

# Náº¿u build thÃ nh cÃ´ng, báº¡n sáº½ tháº¥y:
# => exporting to image
# => naming to docker.io/library/bigdata-app:latest
```

**Náº¿u gáº·p lá»—i:**

- `permission denied` â†’ Quay láº¡i [FIX THIS FIRST](#-fix-this-first---docker-permission-error)
- `docker: unknown command` â†’ Install Compose v2: `sudo apt-get install -y docker-compose-plugin`

**Expected output:**

```
[+] Building 45.2s (12/12) FINISHED
 => [internal] load build definition
 => [internal] load .dockerignore
 => [internal] transferring context
 => CACHED [1/7] FROM docker.io/library/python:3.12-slim
 => [2/7] RUN apt-get update && apt-get install -y...
 ...
 => exporting to image
 => naming to docker.io/library/bigdata-app:latest
```

**ğŸ“¸ Screenshot Checkpoint 4**: Successful build output

---

### Step 5: Verify Image Created

```bash
# List Docker images
docker images | grep bigdata

# Check image size
docker images bigdata-app:latest --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}"
```

**ğŸ“¸ Screenshot Checkpoint 5**: Docker image listed

---

## ğŸš€ DEPLOY INFRASTRUCTURE

### One-command option (recommended)

This starts the full infrastructure stack (Kafka + HDFS + Elasticsearch + Kibana) in exactly one command.

```bash
docker compose -f config/docker-compose.yml up -d \
  zookeeper kafka \
  hadoop-namenode hadoop-datanode \
  elasticsearch kibana
```

Quick verification:

```bash
# Show container status
docker compose -f config/docker-compose.yml ps

# Kibana should respond once Elasticsearch is ready
curl -I http://localhost:5601
```

### One-command shutdown/cleanup

```bash
# Stop and remove containers + network (keeps named volumes: HDFS/Elasticsearch data)
docker compose -f config/docker-compose.yml down

# Full cleanup (also deletes named volumes: ALL persisted data will be lost)
docker compose -f config/docker-compose.yml down -v
```

Optional (aggressive) cleanup if your disk fills up:

```bash
# Removes unused images/build cache across Docker (global)
docker system prune -af
```

### Step 6: Start Zookeeper & Kafka

```bash
# Start Zookeeper first
docker compose -f config/docker-compose.yml up -d zookeeper

# Wait 30 seconds for Zookeeper to be ready
sleep 30

# Check Zookeeper logs
docker compose -f config/docker-compose.yml logs zookeeper | tail -20
```

**Expected output:**

```
zookeeper_1  | [2024-01-13 10:00:00,123] INFO binding to port 0.0.0.0/0.0.0.0:2181
zookeeper_1  | [2024-01-13 10:00:00,456] INFO Server environment:zookeeper.version=3.8.0
```

**ğŸ“¸ Screenshot Checkpoint 6**: Zookeeper running successfully

---

### Step 7: Start Kafka Broker

```bash
# Start Kafka
docker compose -f config/docker-compose.yml up -d kafka

# Wait 60 seconds for Kafka to be ready
sleep 60

# Check Kafka logs
docker compose -f config/docker-compose.yml logs kafka | tail -30
```

**Expected output:**

```
kafka_1  | [2024-01-13 10:01:00] INFO Kafka Server started
kafka_1  | [2024-01-13 10:01:01] INFO [KafkaServer id=1] started
```

**ğŸ“¸ Screenshot Checkpoint 7**: Kafka broker running

---

### Step 8: Verify Kafka Topic Creation

```bash
# List Kafka topics
docker exec -it kafka kafka-topics.sh \
  --list \
  --bootstrap-server localhost:9092
```

**Expected output:**

```
stocks-realtime
stocks-realtime-spark
```

**ğŸ“¸ Screenshot Checkpoint 8**: Kafka topic exists

---

### Step 9: Start HDFS

```bash
# Start HDFS namenode and datanode
docker compose -f config/docker-compose.yml up -d hadoop-namenode hadoop-datanode

# Wait 45 seconds
sleep 45

# Check HDFS status
docker exec -it hadoop-namenode hdfs dfsadmin -report
```

**Expected output:**

```
Live datanodes (1):
Name: 172.18.0.x:9866 (hdfs-datanode)
...
```

**ğŸ“¸ Screenshot Checkpoint 9**: HDFS cluster healthy

---

### Step 10: Start Elasticsearch

```bash
# Start Elasticsearch
docker compose -f config/docker-compose.yml up -d elasticsearch

# Wait 60 seconds
sleep 60

# Check Elasticsearch health
curl -X GET "http://localhost:9200/_cluster/health?pretty"
```

**Expected output:**

```json
{
  "cluster_name" : "docker-cluster",
  "status" : "yellow",
  "number_of_nodes" : 1,
  ...
}
```

**ğŸ“¸ Screenshot Checkpoint 10**: Elasticsearch running

---

### Step 11: Start Kibana (Optional)

```bash
# Start Kibana
docker compose -f config/docker-compose.yml up -d kibana

# Wait 30 seconds
sleep 30

# Check if Kibana is accessible
curl -I http://localhost:5601
```

**Expected output:**

```
HTTP/1.1 200 OK
```

**ğŸ“¸ Screenshot Checkpoint 11**: Kibana accessible

---

## ğŸ“¡ DEPLOY APPLICATION SERVICES

### One-command option (recommended)

```bash
docker compose -f config/docker-compose.yml up -d \
  stock-producer \
  spark-kafka-bridge \
  spark-streaming-metrics \
  spark-streaming-alerts \
  hdfs-archiver
```

**Note**: Lá»‡nh nÃ y sáº½ build images náº¿u chÆ°a cÃ³ vÃ  start táº¥t cáº£ application services cÃ¹ng lÃºc.

**Expected build time**:

- Láº§n Ä‘áº§u: ~3-5 phÃºt (build Spark image vá»›i dependencies)
- Láº§n sau: ~10 giÃ¢y (dÃ¹ng cached image)

**Verify deployment**:

```bash
# Check all containers are running
docker compose -f config/docker-compose.yml ps

# Check Spark Streaming logs (should see "Streaming query started")
docker compose -f config/docker-compose.yml logs --tail=30 spark-streaming-metrics
docker compose -f config/docker-compose.yml logs --tail=30 spark-streaming-alerts

# Check producer is crawling
docker compose -f config/docker-compose.yml logs --tail=20 stock-producer
```

### Alternative: Deploy tá»«ng service má»™t (Step by Step)

Náº¿u muá»‘n monitor tá»«ng service khi start:

### Step 12: Start Kafka Producer (Real-time Crawling)

```bash
# Start the producer
docker compose -f config/docker-compose.yml up -d stock-producer

# Watch producer logs (Ctrl+C to exit)
docker compose -f config/docker-compose.yml logs -f stock-producer
```

**Expected output:**

```
stock-producer  | [START] Real-time crawling for 5 tickers every 60s
stock-producer  | [CRAWL] AAPL
stock-producer  | [CRAWL] NVDA
stock-producer  | [BATCH 1] 2024-01-13T10:05:00 | 5 records | 3.45s
```

**ğŸ“¸ Screenshot Checkpoint 12**: Producer crawling successfully

---

### Step 12B (Recommended): Start Kafka -> HDFS Archiver (near-real-time archive)

Service nÃ y sáº½ Ä‘á»c tá»« `stocks-realtime` vÃ  ghi NDJSON vÃ o HDFS theo cáº¥u trÃºc `/stock-data/YYYY-MM-DD/TICKER.json`.

```bash
docker compose -f config/docker-compose.yml up -d hdfs-archiver

# Check archiver logs
docker compose -f config/docker-compose.yml logs --tail=50 hdfs-archiver
```

---

### Step 13: Verify Data in Kafka

```bash
# Consume messages from Kafka (Ctrl+C after seeing data)
docker exec -it kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic stocks-realtime \
  --from-beginning \
  --max-messages 5
```

**Expected output:**

```json
{"ticker":"AAPL","company":"Apple Inc.","time":"2024-01-13T10:05:00","Open":185.23,"High":185.45,"Low":185.10,"Close":185.32,"Adj Close":185.32,"Volume":12345678}
...
```

**ğŸ“¸ Screenshot Checkpoint 13**: Kafka messages visible

---

### Step 14: Start Kafka Bridge + Spark Streaming

```bash

# Start Kafka -> Spark bridge
docker compose -f config/docker-compose.yml up -d spark-kafka-bridge

# Start Spark Streaming jobs
docker compose -f config/docker-compose.yml up -d spark-streaming-metrics spark-streaming-alerts

# Wait 90 seconds for Spark to initialize
sleep 90

# Check Spark logs
docker compose -f config/docker-compose.yml logs spark-streaming-metrics | tail -50
docker compose -f config/docker-compose.yml logs spark-streaming-alerts | tail -50
```

**Expected output:**

```
spark-streaming-metrics  | [INFO] Kafka: kafka:9092, Topic: stocks-realtime-spark
spark-streaming-alerts   | [READY] Alerts streaming started -> ES index: stock-alerts-1m
```

**ğŸ“¸ Screenshot Checkpoint 14**: Spark Streaming processing data

---

## âœ”ï¸ VERIFY DEPLOYMENT

### Step 15: Check All Running Containers

```bash
# List all running containers
docker compose -f config/docker-compose.yml ps

```

**Expected output:**

```
         Name                        State     Ports
-----------------------------------------------------------
zookeeper               Up      2181/tcp
kafka                   Up      9092/tcp
hadoop-namenode         Up      9870/tcp
hadoop-datanode         Up      9864/tcp
elasticsearch           Up      9200/tcp, 9300/tcp
kibana                  Up      5601/tcp
stock-producer          Up
hdfs-archiver            Up
spark-kafka-bridge      Up
spark-streaming-metrics Up
spark-streaming-alerts  Up
```

**ğŸ“¸ Screenshot Checkpoint 15**: All services running

---

### Step 16: Check Elasticsearch Indices

```bash
# List Elasticsearch indices
curl -X GET "http://localhost:9200/_cat/indices?v"
```

**Expected output:**

```
health status index                uuid   pri rep docs.count docs.deleted store.size
yellow open   stock-realtime-1m    xyz123   1   1         45            0      12.5kb
yellow open   stock-alerts-1m      abc456   1   1         12            0       8.2kb
```

**ğŸ“¸ Screenshot Checkpoint 16**: Elasticsearch indices created

---

### Step 17: Query Real-time Data

```bash
# Query 1-minute aggregations
curl -X GET "http://localhost:9200/stock-realtime-1m/_search?pretty" \
  -H 'Content-Type: application/json' \
  -d '{
    "size": 5,
    "sort": [{"window_start": {"order": "desc"}}]
  }'
```

**Expected output:**

```json
{
  "hits": {
    "total": { "value": 45 },
    "hits": [
      {
        "_source": {
          "ticker": "AAPL",
          "window_start": "2024-01-13T10:05:00",
          "high_1m": 185.45,
          "low_1m": 185.1,
          "close_avg_1m": 185.32
        }
      }
    ]
  }
}
```

**ğŸ“¸ Screenshot Checkpoint 17**: Real-time data in Elasticsearch

---

## ğŸ“Š MONITOR SYSTEM

### Step 18: Monitor Resource Usage

```bash
# Check Docker stats
docker stats --no-stream
```

**Expected output:**

```
CONTAINER           CPU %     MEM USAGE / LIMIT     MEM %
stock-producer      2.5%      512MiB / 1GiB        50%
spark-streaming-metrics  15.3%     2.1GiB / 4GiB        52.5%
elasticsearch       8.2%      1.5GiB / 2GiB        75%
kafka               5.1%      1GiB / 2GiB          50%
```

**ğŸ“¸ Screenshot Checkpoint 18**: Resource usage

---

### Step 19: Check Disk Usage

```bash
# Check HDFS usage
docker exec -it hadoop-namenode hdfs dfs -df -h
docker exec -it hadoop-namenode hdfs dfs -ls /stock-data
```

**Expected output:**

```
Filesystem                Size    Used   Available  Use%
hdfs://namenode:9000     50.0G   1.2G      48.8G    2%

Found 1 items
drwxr-xr-x   - root supergroup          0 2024-01-13 10:00 /stock-data/2024-01-13
```

**ğŸ“¸ Screenshot Checkpoint 19**: HDFS data directory

---

## ğŸŒ ACCESS SERVICES

### Step 20: Access Web UIs

Open these URLs in your browser:

1. **Kibana Dashboard**: http://localhost:5601
2. **Elasticsearch**: http://localhost:9200
3. **HDFS NameNode UI**: http://localhost:9870

**ğŸ“¸ Screenshot Checkpoint 20**:

- Kibana main page
- HDFS web UI showing cluster overview

---

### Step 21: Configure Kibana Index Pattern

1. Open Kibana: http://localhost:5601
2. Go to **Management** â†’ **Stack Management** â†’ **Index Patterns**
3. Click **Create index pattern**
4. Enter pattern: `stock-realtime-*`
5. Select time field: `window_start`
6. Click **Create**

**ğŸ“¸ Screenshot Checkpoint 21**: Kibana index pattern created

---

### Step 22: Create Kibana Visualization

1. Go to **Discover** tab
2. Select `stock-realtime-*` index
3. View real-time data streaming in

**ğŸ“¸ Screenshot Checkpoint 22**: Kibana showing real-time stock data

---

## ğŸ§ª TESTING DATA FLOW

### Step 23: Test End-to-End Data Flow

```bash
# 1. Check producer is sending data
docker compose -f config/docker-compose.yml logs --tail=20 stock-producer

# 2. Verify Kafka has messages
docker exec -it kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic stocks-realtime \
  --max-messages 2

# 3. Check Spark is processing
docker compose -f config/docker-compose.yml logs --tail=30 spark-streaming-metrics
docker compose -f config/docker-compose.yml logs --tail=30 spark-streaming-alerts

# 4. Verify data in Elasticsearch
curl -X GET "http://localhost:9200/stock-realtime-1m/_count"
```

**ğŸ“¸ Screenshot Checkpoint 23**: Complete data flow verified

---

---

### Step 24: Feed Dá»¯ Liá»‡u Lá»‹ch Sá»­ vÃ o HDFS (Phá»¥c vá»¥ Batch Features)

**ğŸ¯ Má»¥c Ä‘Ã­ch**: Batch job cáº§n dá»¯ liá»‡u lá»‹ch sá»­ tá»« HDFS Ä‘á»ƒ tÃ­nh toÃ¡n features (MA, trend, volatility, drawdown, etc.). Archiver tá»« Kafka chá»‰ lÆ°u dá»¯ liá»‡u realtime, vÃ¬ váº­y cáº§n backfill dá»¯ liá»‡u lá»‹ch sá»­ tá»« Yahoo Finance.

**YÃªu cáº§u**: `hadoop-namenode` vÃ  `hadoop-datanode` Ä‘ang cháº¡y vÃ  healthy.

#### Option 1: Backfill dá»¯ liá»‡u 1 nÄƒm (RECOMMENDED)

```bash
# Crawl dá»¯ liá»‡u daily nÄƒm 2024 cho 3 mÃ£ chÃ­nh
docker compose -f config/docker-compose.yml run --rm hdfs-archiver \
  python /app/src/utils/crawl_feed.py \
  --tickers AAPL,NVDA,TSLA \
  --start 2024-01-01 \
  --end 2024-12-31 \
  --interval 1d
```

**Expected output:**

```
======================================================================
BACKFILL TASK
  Tickers: AAPL, NVDA, TSLA
  Date Range: 2024-01-01 to 2024-12-31
======================================================================

[CRAWL] AAPL
  âœ“ /stock-data/2024-01-02/AAPL.json | +1 records (1 total)
  âœ“ /stock-data/2024-01-03/AAPL.json | +1 records (1 total)
  ...
  âœ“ /stock-data/2024-12-30/AAPL.json | +1 records (1 total)

[CRAWL] NVDA
  âœ“ /stock-data/2024-01-02/NVDA.json | +1 records (1 total)
  ...

[CRAWL] TSLA
  âœ“ /stock-data/2024-01-02/TSLA.json | +1 records (1 total)
  ...

======================================================================
BACKFILL COMPLETE
  Files Written: 753
  Files Skipped: 0 (no new data)
  Total New Records: 753
======================================================================
```

#### Option 2: Backfill nhiá»u nÄƒm (cho phÃ¢n tÃ­ch dÃ i háº¡n)

```bash
# Crawl dá»¯ liá»‡u 5 nÄƒm (sáº½ máº¥t ~5-10 phÃºt)
docker compose -f config/docker-compose.yml run --rm hdfs-archiver \
  python /app/src/utils/crawl_feed.py \
  --tickers AAPL,NVDA,TSLA,MSFT,GOOGL \
  --days 1825 \
  --interval 1d
```

#### Option 3: Backfill chá»n khoáº£ng thá»i gian cá»¥ thá»ƒ

```bash
# Crawl dá»¯ liá»‡u tá»« thÃ¡ng 1 Ä‘áº¿n thÃ¡ng 6 nÄƒm 2024
docker compose -f config/docker-compose.yml run --rm hdfs-archiver \
  python /app/src/utils/crawl_feed.py \
  --tickers AAPL,NVDA \
  --start 2024-01-01 \
  --end 2024-06-30 \
  --interval 1d
```

**ğŸ’¡ Gá»£i Ã½ quan trá»ng**:

- **Daily interval (`--interval 1d`)**: Recommended cho backfill dÃ i háº¡n. Yahoo Finance há»— trá»£ dá»¯ liá»‡u daily cho nhiá»u nÄƒm.
- **Minute interval (`--interval 1m`)**: Chá»‰ cÃ³ dá»¯ liá»‡u ~30 ngÃ y gáº§n Ä‘Ã¢y tá»« Yahoo Finance.
- **Deduplication**: Script tá»± Ä‘á»™ng deduplicate nÃªn cÃ³ thá»ƒ cháº¡y nhiá»u láº§n an toÃ n.

#### Verify dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c crawl

```bash
# Kiá»ƒm tra sá»‘ ngÃ y cÃ³ dá»¯ liá»‡u
docker exec -it hadoop-namenode hdfs dfs -ls /stock-data | wc -l

# Xem chi tiáº¿t cÃ¡c ngÃ y
docker exec -it hadoop-namenode hdfs dfs -ls /stock-data | tail -20

# Xem ná»™i dung file máº«u
docker exec -it hadoop-namenode hdfs dfs -cat /stock-data/2024-01-02/AAPL.json
```

**Expected output:**

```json
{
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "time": "2024-01-02T00:00:00",
  "Open": 187.15,
  "High": 188.44,
  "Low": 183.89,
  "Close": 185.64,
  "Adj Close": 184.58,
  "Volume": 82488300
}
```

**ğŸ“¸ Screenshot Checkpoint 24a**: Dá»¯ liá»‡u lá»‹ch sá»­ Ä‘Ã£ Ä‘Æ°á»£c crawl vÃ o HDFS

---

### Step 24b: Cháº¡y Batch Features Job

Sau khi Ä‘Ã£ cÃ³ dá»¯ liá»‡u trong HDFS, cháº¡y batch job Ä‘á»ƒ tÃ­nh toÃ¡n features:

```bash
# Run batch features computation
docker compose -f config/docker-compose.yml run --rm spark-batch-features
```

**Expected output:**

```
=== [1] ÄANG Láº¬P Káº¾ HOáº CH Äá»ŒC Dá»® LIá»†U ===
=== [2] XÃ‚Y Dá»°NG CHUá»–I TÃNH TOÃN (TRANSFORMATIONS) ===
=== [3] THá»°C THI VÃ€ Äáº¨Y Dá»® LIá»†U (ACTIONS) ===
DONE: ÄÃ£ lÆ°u káº¿t quáº£ phÃ¢n tÃ¡n vÃ o HDFS: hdfs://hadoop-namenode:9000/tmp/serving/batch_features
Äang Ä‘áº©y dá»¯ liá»‡u lÃªn Elasticsearch (Serving Layer)...
DONE: ÄÃ£ Ä‘áº©y dá»¯ liá»‡u lÃªn Elasticsearch index: batch-features.
```

#### Verify batch features trong Elasticsearch

```bash
# Kiá»ƒm tra sá»‘ lÆ°á»£ng documents
curl -s "http://localhost:9200/batch-features/_count?pretty"

# Xem sample document
curl -s "http://localhost:9200/batch-features/_search?size=1&pretty"
```

**Expected output:**

```json
{
  "count": 501,
  ...
}
```

Sample document sáº½ cÃ³ cÃ¡c features:

- **Moving Averages**: ma50, ma100, ma200
- **Trend Analysis**: trend, trend_strength
- **Returns**: daily_return, cumulative_return, return_30d, return_90d
- **Risk Metrics**: drawdown, max_drawdown
- **Volume Features**: volume_ma20, volume_ratio
- **Volatility**: monthly_volatility
- **Market Regime**: market_regime (normal/high_vol)

#### Táº¡o Kibana Index Pattern cho Batch Features

1. Má»Ÿ Kibana: http://localhost:5601
2. Go to **Stack Management** â†’ **Index Patterns**
3. Click **Create index pattern**
4. Enter: `batch-features*`
5. Select time field: `time`
6. Click **Create index pattern**
7. Go to **Discover** Ä‘á»ƒ xem dá»¯ liá»‡u batch features

**ğŸ“¸ Screenshot Checkpoint 24b**: Batch features trong Kibana

**âš ï¸ LÆ°u Ã½**: Batch job cáº§n Ã­t nháº¥t 20-30 records má»—i ticker Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘áº§y Ä‘á»§ cÃ¡c features (MA200 cáº§n 200 data points). Vá»›i Ã­t hÆ¡n, má»™t sá»‘ features sáº½ cÃ³ giÃ¡ trá»‹ null vÃ  bá»‹ dropna() loáº¡i bá».

---

### Step 25: (Optional) Schedule Batch Features CronJob

Náº¿u muá»‘n batch features tá»± Ä‘á»™ng cháº¡y Ä‘á»‹nh ká»³ (vd: má»—i ngÃ y 1 láº§n), start batch cronjob:

```bash
docker compose -f config/docker-compose.yml up -d spark-batch-features-cron
```

Default schedule trong `docker-compose.yml` lÃ :

```yaml
# Má»—i ngÃ y lÃºc 2 giá» sÃ¡ng
CRON_SCHEDULE: "0 2 * * *"
```

Verify cronjob Ä‘ang cháº¡y:

```bash
docker ps | grep batch-features-cron
```

Xem logs cá»§a cronjob:

```bash
docker logs -f spark-batch-features-cron
```

**ğŸ’¡ LÆ°u Ã½**: Cronjob chá»‰ cÃ³ Ã­ch khi HDFS liÃªn tá»¥c cÃ³ dá»¯ liá»‡u má»›i (tá»« archiver hoáº·c crawl feed Ä‘á»‹nh ká»³). Náº¿u khÃ´ng, batch job sáº½ chá»‰ process láº¡i dá»¯ liá»‡u cÅ©.

**ğŸ“¸ Screenshot Checkpoint 25**: Batch features cronjob running

---

### Step 26: Test HDFS Archiver (Manual Run)

```bash
# Run HDFS archiver manually (takes recent Kafka data and writes to HDFS)
docker compose -f config/docker-compose.yml run --rm \
  -e KAFKA_BROKER=kafka:9092 \
  -e KAFKA_TOPIC=stocks-realtime \
  -e HDFS_HOST=hadoop-namenode \
  -e HDFS_PORT=9870 \
  -e LOOKBACK_HOURS=1 \
  python-worker \
  python -m src.consumers.kafka_consumer_hdfs_archiver
```

**Expected output:**

```
[START] Archiving data from 2024-01-13 09:00:00 to 2024-01-13 10:00:00
[CONSUME] Reading from Kafka...
  Read 120 messages...
[DONE] Read 120 messages from Kafka
[HDFS] Writing 1 dates to HDFS...
  âœ“ /stock-data/2024-01-13/AAPL.json | +24 records
[COMPLETE] Wrote 5 files, 120 new records to HDFS
```

**ğŸ“¸ Screenshot Checkpoint 24**: HDFS archiver completed successfully

---

### Step 27: Verify HDFS Data

```bash
# List HDFS files
docker exec -it hadoop-namenode hdfs dfs -ls -R /stock-data

# Check file content
docker exec -it hadoop-namenode hdfs dfs -cat /stock-data/2024-01-13/AAPL.json | head -5
```

**Expected output:**

```
drwxr-xr-x   - root supergroup          0 2024-01-13 10:00 /stock-data/2024-01-13
-rw-r--r--   1 root supergroup       2456 2024-01-13 10:00 /stock-data/2024-01-13/AAPL.json
-rw-r--r--   1 root supergroup       2391 2024-01-13 10:00 /stock-data/2024-01-13/NVDA.json
```

**ğŸ“¸ Screenshot Checkpoint 25**: HDFS files created

---

## ğŸ”§ TROUBLESHOOTING

### Common Issues & Solutions

#### Issue 1: Kafka Not Starting

```bash
# Check Zookeeper is running
docker compose -f config/docker-compose.yml logs zookeeper | grep -i error

# Restart Kafka
docker compose -f config/docker-compose.yml restart kafka

# Wait and check
sleep 30
docker compose -f config/docker-compose.yml logs kafka | tail -20
```

---

#### Issue 2: Producer Not Sending Data

```bash
# Check producer logs for errors
docker compose -f config/docker-compose.yml logs stock-producer | grep -i error

# Verify network connectivity
docker exec -it stock-producer nc -zv kafka 9092

# Restart producer
docker compose -f config/docker-compose.yml restart stock-producer
```

---

#### Issue 3: Elasticsearch Yellow Status

```bash
# This is normal for single-node cluster
# Check cluster health
curl -X GET "http://localhost:9200/_cluster/health?pretty"

# Reduce replica count (optional)
curl -X PUT "http://localhost:9200/_settings" \
  -H 'Content-Type: application/json' \
  -d '{"index": {"number_of_replicas": 0}}'
```

---

#### Issue 4: Spark Streaming Errors

```bash
# Check Spark logs
docker compose -f config/docker-compose.yml logs spark-streaming-metrics | grep -i error
docker compose -f config/docker-compose.yml logs spark-streaming-alerts | grep -i error

# Check checkpoint directories
docker exec -it spark-streaming-metrics ls -la /tmp/spark-checkpoints || true
docker exec -it spark-streaming-alerts ls -la /tmp/spark-checkpoints || true

# If a checkpoint gets corrupted, remove it and restart the affected service
docker compose -f config/docker-compose.yml stop spark-streaming-metrics
docker exec -it spark-streaming-metrics rm -rf /tmp/spark-checkpoints
docker compose -f config/docker-compose.yml start spark-streaming-metrics
```

---

#### Issue 4B: Code Ä‘Ãºng nhÆ°ng mapping type sai (Kibana khÃ´ng chá»n Ä‘Æ°á»£c time field)

**Triá»‡u chá»©ng**: Kibana khÃ´ng cho chá»n `@timestamp`/`window_start` lÃ m time field, hoáº·c mapping trong Elasticsearch lÃ  `long`/`text` thay vÃ¬ `date`.

**NguyÃªn nhÃ¢n**: Elasticsearch Ä‘Ã£ **tá»± Ä‘á»™ng táº¡o mapping** tá»« dá»¯ liá»‡u cÅ© (type sai), nÃªn vá» sau dÃ¹ code Ä‘Ã£ sá»­a, mapping váº«n giá»¯ nguyÃªn.

**CÃ¡ch kháº¯c phá»¥c triá»‡t Ä‘á»ƒ** (táº¡o template + táº¡o láº¡i index):

```bash
# 1) Táº¡o index template vá»›i kiá»ƒu DATE
curl -s -X PUT http://localhost:9200/_index_template/stock-realtime-template \
  -H 'Content-Type: application/json' \
  -d '{
    "index_patterns": ["stock-realtime-1m*"],
    "template": {
      "mappings": {
        "properties": {
          "@timestamp": {"type": "date", "format": "strict_date_optional_time||epoch_millis"},
          "window_start": {"type": "date", "format": "strict_date_optional_time||epoch_millis"},
          "window_end": {"type": "date", "format": "strict_date_optional_time||epoch_millis"},
          "processed_time": {"type": "date", "format": "strict_date_optional_time||epoch_millis"},
          "source_time": {"type": "date", "format": "strict_date_optional_time||epoch_millis"}
        }
      }
    }
  }'

curl -s -X PUT http://localhost:9200/_index_template/stock-alerts-template \
  -H 'Content-Type: application/json' \
  -d '{
    "index_patterns": ["stock-alerts-1m*"],
    "template": {
      "mappings": {
        "properties": {
          "@timestamp": {"type": "date", "format": "strict_date_optional_time||epoch_millis"},
          "window_start": {"type": "date", "format": "strict_date_optional_time||epoch_millis"},
          "window_end": {"type": "date", "format": "strict_date_optional_time||epoch_millis"},
          "source_time": {"type": "date", "format": "strict_date_optional_time||epoch_millis"}
        }
      }
    }
  }'

# 2) XoÃ¡ index cÅ© Ä‘á»ƒ mapping má»›i Ä‘Æ°á»£c Ã¡p dá»¥ng
curl -s -X DELETE http://localhost:9200/stock-realtime-1m
curl -s -X DELETE http://localhost:9200/stock-alerts-1m

# 3) Chá» Spark Ä‘áº©y dá»¯ liá»‡u má»›i (>= 2 phÃºt), rá»“i kiá»ƒm tra mapping
curl -s http://localhost:9200/stock-realtime-1m/_mapping?pretty
```

**Sau Ä‘Ã³** trong Kibana:

1. VÃ o **Stack Management â†’ Index Patterns**
2. **Refresh field list** hoáº·c táº¡o láº¡i index pattern
3. Chá»n `@timestamp` lÃ m time field

**Ghi chÃº**: Chá»‰ sá»­a code lÃ  chÆ°a Ä‘á»§ náº¿u index Ä‘Ã£ tá»“n táº¡i vá»›i mapping sai. Báº¯t buá»™c pháº£i xoÃ¡ index (hoáº·c táº¡o index má»›i vá»›i suffix) Ä‘á»ƒ mapping má»›i cÃ³ hiá»‡u lá»±c.

---

#### Issue 4C: Kibana Lens bÃ¡o â€œAvailable fields: 0 / There are no available fields that contain dataâ€

**Triá»‡u chá»©ng**: Lens khÃ´ng tháº¥y field nÃ o cÃ³ dá»¯ liá»‡u, dÃ¹ index Ä‘Ã£ táº¡o vÃ  time field Ä‘Ã£ chá»n Ä‘Æ°á»£c.

**NguyÃªn nhÃ¢n thÆ°á»ng gáº·p**:

- **Time range Ä‘ang quÃ¡ háº¹p** (data bá»‹ náº±m ngoÃ i khoáº£ng thá»i gian Ä‘ang chá»n).
- **Index pattern chÆ°a refresh field list** sau khi mapping Ä‘á»•i.
- **Index cÃ³ mapping Ä‘Ãºng nhÆ°ng chÆ°a cÃ³ document nÃ o** (Spark chÆ°a Ä‘áº©y Ä‘á»§ dá»¯ liá»‡u hoáº·c watermark chÆ°a vÆ°á»£t).

**CÃ¡ch kháº¯c phá»¥c**:

1. **Má»Ÿ rá»™ng time range** trong Kibana (gÃ³c pháº£i trÃªn) â†’ chá»n **Last 24 hours** hoáº·c **Last 7 days**.

2. **Refresh field list**:

- VÃ o **Stack Management â†’ Index Patterns**
- Chá»n index pattern `stock-realtime-*`
- Click **Refresh field list**

3. **Kiá»ƒm tra cÃ³ dá»¯ liá»‡u tháº­t trong ES**:

```bash
curl -s "http://localhost:9200/stock-realtime-1m/_count?pretty"
curl -s "http://localhost:9200/stock-realtime-1m/_search?size=1&pretty"
```

4. **Náº¿u count = 0**:

- Äá»£i Ã­t nháº¥t **2â€“3 phÃºt** (watermark delay) rá»“i kiá»ƒm tra láº¡i.
- Xem log Spark Ä‘á»ƒ cháº¯c cháº¯n query Ä‘ang cháº¡y:

```bash
docker compose -f config/docker-compose.yml logs --tail=50 spark-streaming-metrics
```

**Ghi chÃº**: Vá»›i streaming cÃ³ watermark, dá»¯ liá»‡u chá»‰ â€œappendâ€ sau khi window Ä‘Ã³ng (thÆ°á»ng trá»… vÃ i phÃºt). Lens sáº½ khÃ´ng tháº¥y field náº¿u chÆ°a cÃ³ document nÃ o trong index.

---

#### Issue 4D: Spark ghi ES lá»—i â€œInvalid type: expecting [_doc] but got [doc]â€

**Triá»‡u chá»©ng**: Spark streaming crash vÃ  log bÃ¡o `Invalid type: expecting [_doc] but got [doc]`.

**NguyÃªn nhÃ¢n**: Elasticsearch 7.x chá»‰ cháº¥p nháº­n type `_doc`. Náº¿u cáº¥u hÃ¬nh `es.resource` lÃ  `{index}/doc` sáº½ bá»‹ tá»« chá»‘i.

**CÃ¡ch kháº¯c phá»¥c**:

1. Sá»­a `es.resource` sang `/_doc` trong 2 file:

- [src/streaming/spark_streaming_simple.py](src/streaming/spark_streaming_simple.py)
- [src/streaming/spark_streaming_alert.py](src/streaming/spark_streaming_alert.py)

2. Rebuild vÃ  restart cÃ¡c service:

```bash
docker compose -f config/docker-compose.yml build spark-streaming-metrics spark-streaming-alerts
docker compose -f config/docker-compose.yml up -d spark-streaming-metrics spark-streaming-alerts
```

---

#### Issue 4E: Lá»—i curl do gÃµ nháº§m `pretty~`

**Triá»‡u chá»©ng**: curl tráº£ lá»—i `unrecognized parameter: [pretty~]`.

**NguyÃªn nhÃ¢n**: GÃµ nháº§m kÃ½ tá»± `~` sau `pretty`.

**CÃ¡ch kháº¯c phá»¥c**: DÃ¹ng Ä‘Ãºng cÃº phÃ¡p:

```bash
curl -s "http://localhost:9200/stock-realtime-1m/_search?size=1&pretty"
```

---

#### Issue 5: HDFS SafeMode

```bash
# Check HDFS status
docker exec -it hadoop-namenode hdfs dfsadmin -safemode get

# If in safe mode, leave it
docker exec -it hadoop-namenode hdfs dfsadmin -safemode leave

# Verify
docker exec -it hadoop-namenode hdfs dfsadmin -report
```

---

## ğŸ—‘ï¸ CLEANUP & TEARDOWN

### Step 28: View Current Resources

```bash
# List all containers
docker compose -f config/docker-compose.yml ps

# List volumes
docker volume ls | grep big_data

# Check disk usage
docker system df
```

**ğŸ“¸ Screenshot Checkpoint 26**: Resources before cleanup

---

### Step 29: Stop All Services

```bash
# Stop all containers (keeps data)
docker compose -f config/docker-compose.yml stop

# Verify all stopped
docker compose -f config/docker-compose.yml ps
```

**Expected output:**

```
         Name                        State
-------------------------------------------
big_data_zookeeper_1         Exit 0
big_data_kafka_1             Exit 0

...
```

**ğŸ“¸ Screenshot Checkpoint 27**: All services stopped

---

### Step 30: Remove Containers (Keep Data)

```bash
# Remove containers but keep volumes
docker compose -f config/docker-compose.yml down

# Verify containers removed
docker ps -a | grep big_data
```

**ğŸ“¸ Screenshot Checkpoint 28**: Containers removed

---

### Step 31: Complete Cleanup (Remove All Data)

```bash
# âš ï¸ WARNING: This will DELETE ALL DATA!

# Remove containers and volumes
docker compose -f config/docker-compose.yml down -v

# Remove images (optional)
docker rmi bigdata-app:latest

# Clean up unused resources
docker system prune -a --volumes -f

# Verify cleanup
docker ps -a
docker volume ls
docker images
```

**Expected output:**

```
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
(empty)

VOLUME NAME
(empty)
```

**ğŸ“¸ Screenshot Checkpoint 29**: Complete cleanup done

---

### Step 32: Verify Disk Space Reclaimed

```bash
# Check disk usage after cleanup
df -h /var/lib/docker

# Check Docker system info
docker system df
```

**ğŸ“¸ Screenshot Checkpoint 30**: Disk space reclaimed

---

## ğŸ“ DEPLOYMENT SUMMARY

### âœ… Completed Steps Checklist

- [ ] Prerequisites verified
- [ ] Environment configured
- [ ] Docker images built
- [ ] Zookeeper deployed
- [ ] Kafka deployed
- [ ] HDFS deployed
- [ ] Elasticsearch deployed
- [ ] Kibana deployed
- [ ] Kafka Producer running
- [ ] Spark Streaming running
- [ ] Spark Alerts (1m) running
- [ ] Spark Batch (features) executed
- [ ] Data flow verified
- [ ] Elasticsearch indices created
- [ ] Kibana configured
- [ ] HDFS archiver tested
- [ ] All services monitored
- [ ] Screenshots captured at each checkpoint
- [ ] System tested end-to-end
- [ ] Cleanup performed

---

## ğŸ¯ Next Steps

After successful deployment:

1. **Configure Alerts**:

- Infra alerts: pod/container restarts, lag, disk full
- Data alerts: Elasticsearch index `stock-alerts-1m` receiving documents

2. **Optimize Resources**: Adjust memory/CPU based on load
3. **Schedule HDFS Archiver**: Add cron job for daily runs
4. **Schedule Spark Batch**: Run `src/batch_jobs/run_all.py` daily to build batch features and index `batch-features`
5. **Backup HDFS**: Implement backup strategy
6. **Scale Up**: Add more Kafka/Spark containers if needed

---

## ğŸ“ Support

If you encounter issues:

1. Check logs: `docker compose -f config/docker-compose.yml logs [service-name]`
2. Review this guide's troubleshooting section
3. Check container health: `docker ps`
4. Verify network: `docker network ls` then `docker network inspect <project>_bigdata-network`

---

**Last Updated**: January 13, 2026  
**Version**: 1.0  
**Environment**: Docker Compose
