# H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG PIPELINE REAL-TIME STOCK DATA

**Big Data Project - Kafka ‚Üí Spark Streaming ‚Üí Elasticsearch ‚Üí Kibana**

---

## üìã T·ªîNG QUAN H·ªÜ TH·ªêNG

Pipeline x·ª≠ l√Ω d·ªØ li·ªáu c·ªï phi·∫øu real-time v·ªõi Spark Streaming:

1. **stock_realtime** - T√≠nh to√°n metrics c∆° b·∫£n theo time window (‚úÖ ƒêang ch·∫°y)
   - Aggregation theo window 30 gi√¢y
   - Metrics: avg_price, min_price, max_price, total_volume, trade_count, price_volatility

**L∆∞u √Ω:** Technical Indicators v√† Anomaly Detection jobs kh√¥ng t∆∞∆°ng th√≠ch v·ªõi Spark Structured Streaming v√¨ y√™u c·∫ßu row-based window functions (RSI, MACD, v.v.). C√°c ch·ªâ b√°o n√†y ph√π h·ª£p h∆°n cho batch processing.

---

## üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG

### B∆∞·ªõc 1: Start t·∫•t c·∫£ services

```bash
cd /home/danz/Downloads/big_data
docker compose up -d
```

**Ch·ªù 30 gi√¢y** ƒë·ªÉ c√°c services kh·ªüi ƒë·ªông ho√†n t·∫•t.

### B∆∞·ªõc 2: Ki·ªÉm tra services ƒëang ch·∫°y

```bash
docker compose ps
```

Ph·∫£i th·∫•y c√°c containers: `elasticsearch`, `kibana`, `kafka`, `hadoop-namenode`, `spark-streaming-simple`, `python-worker`

### B∆∞·ªõc 3: Start Kafka Producer (g·ª≠i d·ªØ li·ªáu c·ªï phi·∫øu)

```bash
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"
```

Ki·ªÉm tra Producer ƒëang ch·∫°y:

```bash
docker exec python-worker tail -10 /tmp/producer.log
```

Ph·∫£i th·∫•y: `[BATCH] 2025-xx-xx xx:xx:xx: #N sent (2 messages)`

### B∆∞·ªõc 4: ƒê·ª£i d·ªØ li·ªáu ƒë∆∞·ª£c ghi v√†o Elasticsearch (2-3 ph√∫t)

```bash
# Ki·ªÉm tra sau 2 ph√∫t
sleep 120
curl -s -X GET "http://localhost:9200/_cat/indices?v" | grep stock
```

Ph·∫£i th·∫•y index `stock_realtime` v·ªõi docs.count > 0

---

## üìä XEM D·ªÆ LI·ªÜU TR√äN KIBANA

### B∆∞·ªõc 1: M·ªü Kibana

```
http://localhost:5601
```

### B∆∞·ªõc 2: T·∫°o Index Pattern

1. V√†o **Management** ‚Üí **Stack Management** ‚Üí **Index Patterns**
2. Click **"Create index pattern"**
3. Nh·∫≠p: `stock_realtime`
4. Ch·ªçn Time field: `window_start`
5. Click **"Create index pattern"**

### B∆∞·ªõc 3: Xem d·ªØ li·ªáu Real-time

1. V√†o **Analytics** ‚Üí **Discover**
2. Ch·ªçn index pattern: `stock_realtime`
3. Ch·ªçn time range: **Last 15 minutes**
4. Refresh t·ª± ƒë·ªông: **10 seconds**

### B∆∞·ªõc 4: T·∫°o Visualization (t√πy ch·ªçn)

V√†o **Analytics** ‚Üí **Visualize Library** ‚Üí **Create visualization**

**G·ª£i √Ω visualizations:**

- Line chart: `avg_price` theo th·ªùi gian
- Bar chart: `total_volume` theo ticker
- Metric: `price_volatility` hi·ªán t·∫°i

---

## üîß QU·∫¢N L√ù SPARK STREAMING JOB

### Ki·ªÉm tra logs c·ªßa Spark Streaming

```bash
# Job ch√≠nh (simple metrics) - ƒêang ch·∫°y
docker logs spark-streaming-simple --tail 50
```

**Index ƒë∆∞·ª£c t·∫°o:** `stock_realtime`

**L∆∞u √Ω v·ªÅ Technical Indicators v√† Anomaly Detection:**
Hai jobs n√†y (`spark_streaming_technical_indicators.py` v√† `spark_streaming_anomaly_detection.py`) s·ª≠ d·ª•ng row-based window functions ƒë·ªÉ t√≠nh RSI, MACD, Bollinger Bands, ATR - c√°c t√≠nh nƒÉng n√†y **kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ trong Spark Structured Streaming**.

Spark Structured Streaming ch·ªâ h·ªó tr·ª£ time-based window aggregation. ƒê·ªÉ s·ª≠ d·ª•ng c√°c ch·ªâ b√°o k·ªπ thu·∫≠t n√†y, c·∫ßn chuy·ªÉn sang **Batch Processing** ho·∫∑c d√πng c√°c th∆∞ vi·ªán b√™n ngo√†i.

---

## üõë D·ª™NG V√Ä KH·ªûI ƒê·ªòNG L·∫†I T·ª™ ƒê·∫¶U

### D·ª´ng to√†n b·ªô h·ªá th·ªëng

```bash
cd /home/danz/Downloads/big_data
docker compose down
```

### X√≥a d·ªØ li·ªáu c≈© (reset ho√†n to√†n)

```bash
# X√≥a Elasticsearch data
docker volume rm big_data_es_data 2>/dev/null || true

# X√≥a Kafka data
docker volume rm big_data_kafka_data 2>/dev/null || true

# X√≥a HDFS checkpoints
docker compose up -d hadoop-namenode
sleep 10
docker exec hadoop-namenode hdfs dfs -rm -r /user/spark_checkpoints/* 2>/dev/null || true
docker compose stop hadoop-namenode
```

### Kh·ªüi ƒë·ªông l·∫°i t·ª´ ƒë·∫ßu

```bash
docker compose up -d
sleep 30

# Start Producer
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"

# ƒê·ª£i 2-3 ph√∫t r·ªìi ki·ªÉm tra
sleep 120
curl -s -X GET "http://localhost:9200/_cat/indices?v" | grep stock
```

---

## üîç TROUBLESHOOTING

### 1. Index kh√¥ng xu·∫•t hi·ªán trong Elasticsearch

**Ki·ªÉm tra Producer:**

```bash
docker exec python-worker ps aux | grep kafka_producer
docker exec python-worker tail -20 /tmp/producer.log
```

**N·∫øu kh√¥ng ch·∫°y, restart:**

```bash
docker exec python-worker bash -c "cd /app && nohup python kafka_producer.py > /tmp/producer.log 2>&1 &"
```

### 2. Spark Streaming c√≥ l·ªói

**Xem logs:**

```bash
docker logs spark-streaming-simple --tail 100
```

**N·∫øu c√≥ l·ªói NoSuchMethodError ho·∫∑c checkpoint issues:**

```bash
# X√≥a checkpoint
docker exec hadoop-namenode hdfs dfs -rm -r /user/spark_checkpoints/stock_realtime

# Restart Spark
docker compose restart spark-streaming-simple
```

### 3. Kibana kh√¥ng hi·ªÉn th·ªã d·ªØ li·ªáu

**Ki·ªÉm tra Elasticsearch c√≥ d·ªØ li·ªáu:**

```bash
curl -X GET "http://localhost:9200/stock_realtime/_count?pretty"
```

**N·∫øu count > 0 nh∆∞ng Kibana kh√¥ng th·∫•y:**

- Refresh trang Kibana (F5)
- Ki·ªÉm tra Time Range (ph·∫£i ch·ªçn Last 15 minutes ho·∫∑c r·ªông h∆°n)
- X√≥a Index Pattern v√† t·∫°o l·∫°i

### 4. Kafka kh√¥ng nh·∫≠n messages

**Ki·ªÉm tra Kafka topic:**

```bash
docker exec python-worker python -c "
from confluent_kafka import Consumer
import time

conf = {'bootstrap.servers': 'kafka:9092', 'group.id': 'test', 'auto.offset.reset': 'latest'}
consumer = Consumer(conf)
consumer.subscribe(['stock-realtime-topic'])

print('Waiting 10 seconds...')
for _ in range(10):
    msg = consumer.poll(1.0)
    if msg and not msg.error():
        print(f'Message: {msg.value().decode()[:100]}')

consumer.close()
"
```

---

## üìà C·∫§U TR√öC D·ªÆ LI·ªÜU

### Index: stock_realtime

```json
{
  "window_start": "2025-12-21T06:00:00Z",
  "window_end": "2025-12-21T06:00:30Z",
  "ticker": "AAPL",
  "company": "Apple Inc.",
  "avg_price": 280.5,
  "min_price": 278.2,
  "max_price": 282.1,
  "total_volume": 45000000,
  "trade_count": 15,
  "price_volatility": 1.25,
  "processed_time": "2025-12-21T06:01:05Z"
}
```

**Gi·∫£i th√≠ch c√°c tr∆∞·ªùng:**

- `window_start/end`: Time window 30 gi√¢y
- `avg_price`: Gi√° trung b√¨nh trong window
- `min_price/max_price`: Gi√° th·∫•p nh·∫•t/cao nh·∫•t
- `total_volume`: T·ªïng kh·ªëi l∆∞·ª£ng giao d·ªãch
- `trade_count`: S·ªë l∆∞·ª£ng trades trong window
- `price_volatility`: ƒê·ªô bi·∫øn ƒë·ªông gi√° (standard deviation)

---

## ‚öôÔ∏è C·∫§U H√åNH

### Producer Settings (kafka_producer.py)

- `UPDATE_INTERVAL`: 30 gi√¢y (th·ªùi gian g·ª≠i batch)
- `TICKERS`: AAPL, NVDA (th√™m ticker trong docker-compose.yml)

### Spark Streaming Settings

- Window: 30 gi√¢y
- Watermark: 1 ph√∫t
- Trigger: 30 gi√¢y

### Elasticsearch Settings

- Version: 7.17.16
- No security (development mode)
- Single node

---

## üìû H·ªñ TR·ª¢

**Xem logs chi ti·∫øt:**

```bash
# All services
docker compose logs -f

# Specific service
docker compose logs -f spark-streaming-simple
docker compose logs -f kafka
docker compose logs -f elasticsearch
```

**Check Elasticsearch health:**

```bash
curl -X GET "http://localhost:9200/_cluster/health?pretty"
```

**Check Kibana status:**

```bash
curl -s "http://localhost:5601/api/status" | jq '.status.overall.state'
```

---

**üéâ Pipeline ƒë√£ s·∫µn s√†ng ho·∫°t ƒë·ªông!**
